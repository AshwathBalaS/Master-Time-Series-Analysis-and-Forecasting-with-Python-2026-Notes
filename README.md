# Master-Time-Series-Analysis-and-Forecasting-with-Python-2026-Notes
This Repository contains my "Master Time Series Analysis and Forecasting with Python 2026" Course Notes from Udemy

**I) Time Series Analysis and Forecasting with Python**

**A) Time Series Analysis and Forecasting with Python**

**B) Course Introduction**

**C) Overview of the AI Time Series Assistant**

**D) Diogo's Introduction and Background**

**E) Unlimited Updates and Enhancements 2026**

**II) Section 2: Part 1 - Time Series Analysis**

**A) Time Series Analysis Overview**

**III) Section 3: Python for Time Series Analysis**

**A) Game Plan for Python for Time Series Analysis**

**B) **Load and Explore Data****

**C) Subsetting Stores and Basic Aggregations**

**D) Working with Datetime Index and Weekday Patterns**

**E) Standardizing Sales and Comparing Weekday Patters**

**F) Analyzing Sales on a Specific Weekday**

**G) Removing Outliers and Re-Assessing Intra-Week Seasonality**

**H) Analyzing the Impact of Promotions**

**IV) Section 4: Introduction to Time Series Forecasting**

**A) Game Plan for Introduction to Time Series Forecasting**

**B) What is Time Series Data?**

**C) Python - Libraries and Data**

**D) Python - Time Series Index**

**E) Python - Exploratory Data Analysis Part 1**

**F) Python - Exploratory Data Analysis Part 2**

**G) Python - Data Visualization**

**H) Python - Data Manipulation Part 1**

**I) Python - Data Manipulation Part 2**

**J) Seasonal Decomposition**

**K) Python - Seasonal Plots**

**L) Python - Seasonal Decomposition**

**M) Auto-Correlation**

**N) Python - Auto-correlation**

**O) Partial Auto-Correlation**

**P) Python - Partial Auto-Correlation**




# **I) Time Series Analysis and Forecasting with Python**

# **A) Time Series Analysis and Forecasting with Python**

You ever feel like you're the keto vanilla ice cream of the business world?
Boring and disappointing that you're still doing forecasting the old way like a rolled ice cream when everyone now talks about a -- pistachio flavor?

Well, not anymore.

In this course, you'll be at the forefront of time series forecasting from time series analysis and diving deep into advanced forecasting models.
I'll guide you through every step.

You will cover everything from simple exponential smoothing to cutting edge models like LinkedIn, Silver Kite, Prophet, and Amazon Cronos.

Think you're just a small fish in a big pond?

Let's change that with hands on exercise, real world challenges, and capstone projects.
You won't just learn, you'll do.

Revenues.
Electricity.
People.
Temperatures.

If it moves, we predict.

By the end you'll be forecasting trends with the precision of a sniper, turning your yearly revenue into your monthly income.

And who's going to lead you through this journey?

Well.

Hi.

I'm Diogo, your guide, mentor, and now ice cream coach with years of experience in data science and a passion for turning complex concepts into actionable insights.

I practice what I preach.

Jump in and let's make this year your breakthrough.

This is your moment.

Make it count.

Enroll now and let's crush it together.

# **B) Course Introduction**

I'm very happy that you picked this course to learn time series analysis.
It's the fifth year that this course has been live, and I truly think it's the most complete out there.

And there's absolutely no video that were in the, let’s say, first two editions that are currently live now.
So it’s currently always, always updating.

What I want to show you throughout this video is every single change since the 2025 version to now, the 2026 version.
This walkthrough will help you understand exactly what has improved and why it matters.

The key changes start with much crisper videos.
I really try to make sure that all videos are shorter and more focused.

We also now have deeper explanations and better explanations overall.
The goal is not just speed, but clarity.

Coding is now done with AI in, I think, most of the videos by now.
This allows us to spend more time looking at documentation and understanding what is what and why we are doing it, without the hassle of writing all the code manually.

Of course, we are very critical about it.
In the end, the code used is the one that I like, not necessarily the one that, in our case, Jamie and I are told to use.

We also add new projects and topics based on popular demand.
So the bottom line is, if you want something, tell me.

If there are a few people that want the same thing, I’ll make it happen.
And that’s a promise.

I also introduced an AI assistant.
You now have a time series analysis assistant that you can take anywhere with you and use in your job or during interviews.

This is a print screen of the assistant.
All the materials of the course are there—honestly, everything is there.

It’s just about asking the right questions.
And the assistant responds, I think, in my tone.

So if you like the way I talk or the way I use words, you’re going to like this as well.

Now, let’s look at what the course was in 2025.
At that time, we had five parts.

These were time series analysis, modern time series models and forecasting, deep learning for time series, and advanced content for time series.
We also had a Python appendix.

The Python appendix was for those who were not very familiar with Python but still wanted to learn.
There was a crash course at the end.

These were all the sections that were available back then.
I don’t want to spend too much time listing them, but they’re all there.

The question then becomes: what did we change?
This is where the remakes come in.

One important highlight is a new part called the Time Series Graveyard.
These are techniques that existed before but have not been updated by the teams maintaining them.

That’s why I’m calling it the graveyard.
This section also exists because of your requests.

In the past, I removed some sections.
Some people later asked if they could still access them.

So now, instead of removing them completely, they live in the Time Series Graveyard.
I wouldn’t advise you to focus on them, but it’s your choice.

This section includes everything that was either remade or added.
Anything marked as new represents a true addition.

We added new practice exercises for time series analysis.
We also added new intermittent forecasting and classification for time series.

For sections that are not marked as new, their Python tutorials were fully remade.
This was necessary because new changes in the libraries required updates.

As a result, the course is now 100% up to date.
Nothing is outdated or obsolete.

This is how the 2026 version of the course is now organized.
It consists of six parts.

These are time series analysis, modern time series, deep learning, advanced content, the graveyard, and the Python appendix.

The course is also now split between sections and projects or exercises.
Depending on your goals, you can choose how deep you want to go.

If you want structured learning, you can focus on the sections.
If you want to build a portfolio, you’ll find plenty of projects and exercises.

One important thing to note is that the course is actually shorter than it was one year ago.
This is despite having a lot of new content.

The reason is simple: the videos are crisper and better edited.
Using AI for coding also makes learning much faster.

This creates a smoother and more efficient learning experience.
That’s something I’m genuinely excited about.

The 2026 version is live.
So let’s get started.

# **C) Overview of the AI Time Series Assistant**

In the previous lecture, I gave you a link to the AI time series assistant, and in this video I want to give you a brief introduction to it. I’m going to call this version one of the assistant. I focused a lot on getting the core functionalities and the architecture right so that I can build on it later. There are quite a few things that could still be improved, and that’s exactly what I’m looking for when you share your feedback.

In this video, I want to introduce what it can do, what it cannot do, and the other actions available. It’s always going to be a time series course assistant, and it has a collapsible bar where you can see its capabilities, the core topics it covers, and also what it cannot do. For example, it cannot give medical advice, it cannot run code, and it cannot browse the web. You’ll also see a “ready to predict the future” message and some initial information explaining what it can and cannot do.

Let’s say we enter a message, for instance, “explain time series analysis,” and then click on send. You’ll notice that it shows when it’s thinking, both in the chat itself and at the top, where it indicates that it’s running. This assistant is going to complement the AI assistant from Udemy. There are a few things they have in common, especially when it comes to the videos themselves, because both have access to the transcripts. In fact, everything from the Udemy AI assistant comes from those transcripts, so if you want something specific to a given lecture, that’s where it’s useful.

For example, if you want a summary of a lecture, the Udemy assistant is very good because everything is directly linked to your current video. However, this time series assistant is not linked to your current video experience, so that’s one area where Udemy’s assistant is better. On the other hand, this assistant has some clear advantages. First, it’s always going to be up to date. For instance, we’re currently using GPT-5 already, whereas the Udemy assistant, based on things like its use of words such as “delve,” still feels very much like GPT-4.

At the same time, this assistant has all the course material loaded into it. That includes all the Python code, all the PDFs, and everything else from the course. So if you want something really tailored to the course—especially things you want to use, reuse, and take into your professional life—this is the one to use. It’s also not really tied to Udemy, which means you can keep it open and ask questions while you’re working on your own projects.

This tool is meant for you to use and reuse. Of course, there are many things that can still be improved. There’s an option to give feedback, and while I’d obviously love it if you love it, what I really care about is that you share what you like and what you would improve. This could be things like the colors, the layout, or any other aspect. Again, this is version one.

If you’ve been with the course for a long time—especially since the course originally dates back to 2021—you know that I’m always improving it. There will be a version two, version three, and so on, because I’m very focused on making this the best course possible.

While you’re here, you can also ask things like, “Give me the code for exponential smoothing,” press control-enter, and the assistant will think and respond. Alongside this, I want to emphasize that the feedback section is really important. Sometimes people ask for personal help through the feedback, but I can’t help you that way. If you need my help directly, you should always use the Q&A section. I answer questions there almost every day—there’s maybe one day a month when I don’t check it.

The Q&A is always the best way to reach out to me. I do gather feedback from this tool, but I don’t check feedback every day. I usually review it weekly or monthly, make sure everything is working, and then act on it after a couple of months to improve the tool. One example of feedback is that the Udemy assistant allows you to add images, which is really cool, whereas this one currently cannot. That’s something to keep in mind for future improvements.

You’ll also find all the code used in the course here. This is the same code we use throughout the course, and it should always be up to date because it reflects the latest course materials. If you ever find any issues, please let me know, because sometimes it can produce hallucinations, which I can’t fully control.

If at any point you don’t want to continue a conversation and you want to start fresh, you can reset the conversation. That will start a new session. Everything here is session-based only—there’s no database in the backend, and nothing is stored.

If this is something you love and use a lot, there’s an option to donate. This is completely optional, but I want to be transparent that this tool does cost money to run. The cost for one to ten people is basically nothing, but when you get to thousands of users, it becomes more significant. So if it genuinely makes a difference in your life, you can support it, but again, it’s absolutely optional.

At the end of the day, this is yours. Even if you hate me, you can bookmark it. There’s no login, and it’s an open tool that you can use and reuse. It’s here for you to keep. I personally find it very cool when people use my work, so if you do use it, let me know.

# **D) Diogo's Introduction and Background**

Hi there, I’m Diogo, and I’m thrilled to welcome you to this course. Before we get started, let me share a bit about myself so you know who’s guiding you through this journey.

I hold a Master of Science in Management from ESMT Berlin, one of the top business schools in Europe, where I specialized in business and analytics. My professional career has been centered around using data to solve complex business challenges. I’ve worked on projects ranging from sales planning involving billions of euros to A/B tests where companies had to invest hundreds of thousands of euros. I’ve truly been around the block and have had a lot of hands-on experience.

Beyond teaching, I’m also a startup founder. My company, which you can find at Join Betacom, aims to leverage the power of data to help restaurants around the world. We analyze vast amounts of data to provide actionable insights, such as optimizing menus or determining the best pricing strategies for their products. And if you’re interested in my startup, feel free to reach out and drop me a line.

I’m here not just to teach, but to genuinely support your learning journey. If at any point you feel that this course isn’t the right fit for you, don’t hesitate to reach out. I’m more than happy to guide you toward a learning path that suits you best, even if that means recommending other resources or courses that better align with your learning style.

Before we wrap up, I’d like to extend a personal invitation for you to connect with me on LinkedIn. Simply search for Diogo Alves de Rezende, and let’s keep the conversation going. I’m always eager to engage with my students and grow our professional networks together.

Once you complete this course and earn your certificate, I encourage you to share it on LinkedIn. I make it a point to repost these achievements and celebrate your success, just as I’ve done for many of my students. It’s a great way to gain visibility and start building your professional reputation in the business and analytics community.

My goal is for you to succeed and feel empowered to make a real impact. Whether you’re here to advance your career, pivot into a new industry, or innovate within your own business or company, I’m excited to help you achieve those dreams.

# **E) Unlimited Updates and Enhancements 2026**

Hey, before we start, let me talk about the current status of this course and what’s coming next.

All the content you’re about to dive into has been pre-checked and updated to ensure it’s relevant and accurate for 2026. I know how fast technology evolves, and that’s why I’m committed to keeping this course up to date with the latest developments and insights, so you can stay ahead of the curve.

That said, in a world that’s constantly changing, there’s always a chance that something might slip through the cracks. If you notice any outdated information or anything that doesn’t feel quite right, please don’t hesitate to let me know. Your input is invaluable in maintaining the quality and relevance of this course.

In addition, if there’s any specific content or material that you’d like to see added, I’d love to hear from you. Your suggestions help shape the future of this course.

In the next lecture, I’ll share a form where you can report anything that’s incorrect, offer suggestions, or request additional resources. This course is a collaborative journey, and your feedback plays a key role in making it the best it can be.

Thank you for being an active part of this learning community. I’ll see you in the next video.

# **II) Section 2: Part 1 - Time Series Analysis**

# **A) Time Series Analysis Overview**

Welcome to the time series analysis part of the course. This is where you take your data skills to the next level, learning how to make your data work for you and predict future trends with accuracy. Let’s get started.

Have you ever wondered how companies like Amazon and Netflix always seem to know what’s coming next? It all comes down to mastering time series forecasting. By the end of this section, you’ll be applying the same techniques to your own business and career, turning raw data into accurate, actionable predictions.

We’ll begin by understanding what time series data is and why it plays such a critical role in forecasting. You’ll work with real-world data in Python and set up all the essential tools needed for time series analysis. This foundation will allow you to build robust models and generate precise predictions.

Before forecasting effectively, it’s essential to know your data inside out. You’ll perform exploratory data analysis to uncover hidden patterns and structures in your time series. Through visualization and data manipulation techniques, you’ll learn how to interpret your data and prepare it properly for forecasting.

You may notice that certain trends repeat over time—this is known as seasonality. You’ll learn how to decompose your data into seasonal, trend, and residual components. This breakdown will help you model your data more accurately and significantly improve your predictions.

Understanding how past data points influence future values is another key concept. You’ll explore autocorrelation and partial autocorrelation to measure these relationships. This knowledge is essential for selecting, tuning, and validating your forecasting models.

Next, we’ll dive into a range of forecasting techniques. You’ll start with simple exponential smoothing, which works well for short-term predictions. From there, you’ll move on to the Holt-Winters method to handle seasonality. You’ll also work with more advanced models such as ARIMA and SARIMAX to capture complex patterns in time series data.

Imagine being able to predict stock prices or customer demand with confidence. You’ll develop these skills through hands-on exercises and real-world challenges. This section is not just about theory—it’s about applying these techniques to real data and understanding their results.

You’ll also explore what happens when forecasting goes wrong. By studying real case studies where predictions missed the mark, you’ll learn to recognize common pitfalls and improve your forecasting approach.

Finally, you’ll bring everything together in a capstone project focused on forecasting airline miles. You’ll prepare the data, build forecasting models, and evaluate their performance. By the end of this project, you’ll have a strong, practical example of your forecasting skills to showcase professionally.

You’re not just learning how to forecast—you’re mastering a powerful skill that can transform your career. See you in the next video.

# **III) Section 3: Python for Time Series Analysis**

# **A) Game Plan for Python for Time Series Analysis**

In this section, you are going to use Python as a tool to understand time series. This is not really an academic exercise. We’re going to work with real data, real questions, and we’re going to aim for real decisions.

We’re going to move fast, stay focused, and build intuition step by step. The goal is to truly understand time series, not just in theory, but in practice.

I can tell you already that you don’t need to be a master of Python. You only need enough to slice data, spot patterns, test ideas, and eventually explain what’s actually going on in the business. That’s really the core objective here.

If you already feel comfortable with Python, that’s great—you’ll be sharpening your instincts. If, however, you’re rusty or starting fresh, don’t worry. There’s a full Python crash course at the end of this course where you can practice more or even start from zero at your own pace.

# **B) **Load and Explore Data****

You are going to find this inside Python for Time Series, which itself is inside Time Series Analysis, and then Python for Time Series Analysis. There, you will find a file called lab starter one.

And here we go.

So we are here. I need to reconnect. This is basically taking our script and saying, “Hey, this is live.” Then I would mount the drive here. Of course, if you’re not using Google Colab, you don’t need to do this step. This is just for those who are using it. Whichever workspace you’re using, just go for it.

In case you’re using Colab, let me show you. You go to Drive → My Drive. This is so that you get the path. Inside My Drive, you’ll find Python Time Series Forecasting. Then inside Time Series Analysis, you will find Python for Time Series Analysis. I will copy the path here. Here we go. Control. Of course, nothing changes, and here we go.

So this is the part where I do this with you. Of course, it’s very specific for Colab. And now we can go.

Task number one is to import the pandas library. All right, let’s do this. Import the pandas library. It’s very simple:
import pandas as pd.

So pandas is the library, and pd is the short form that we are going to call. Cool. Shift + Enter. Task number one is done.

Then task number two is to load the train.csv file into a DataFrame called df, and preview the first rows. You’re going to find the dataset in your materials. It’s a very cool dataset. It has a lot of stores, the day of the week, and it’s a very rich dataset for us to explore and practice some Python.

And here we go. DataFrame. So I’m going to call it dataframe = pandas.read_csv("train.csv"). There was an extra dot here, so let me remove it.

Then I do dataframe.head(). In case you’re using the Jupyter functions of Colab or whichever it is, the comments are going to give quite a few things away. I would always encourage you to double-check and make sure that whatever you’re doing makes sense. In the end, or in case you can, just try to do it on your own. That’s always the best way to practice.

Of course, when you’re actually doing the work, go for it. Go fast. As for practicing, doing it yourself is a tiny bit better.

Now we look at the dataset. We have store, day of week, date, sales, customers, whether it was open, whether there was a promo, whether we have a state holiday, and a school holiday.

The dataset is from a very well-known store, Rossmann. It’s all in German, but it’s basically something that sells things like cleaning products, beauty products, and so on. And of course, at least in Germany, there’s something very specific: some stores are closed on Sunday. That’s something we need to consider.

But if we look at it here, let’s look at something. We see a dtype warning for column seven. Remember, Python indexing starts at zero, so we have columns 0, 1, 2, 3, 4, 5, 6, 7. So the state holiday column has something weird going on.

We can specify the dtype option on import or set low_memory=False. This is clearly something we need to investigate. So I’d say the first step is to look at information about the DataFrame.

To do that, we use dataframe.info(). This provides some information. We can see that, for instance, state_holiday is currently an object. And when we look at it, it has zeros, but it’s an object type, which means it has characters or letters inside. That’s something we definitely need to investigate.

This leads us to task number four. We’re going to print the unique values in state_holiday and their counts.

The way we do this is dataframe["state_holiday"].value_counts(). And we should get something. If you’re not using a notebook style environment, you’d have to use print. I personally have a big preference for notebook styles because of the input-output flow—it’s much easier to explore data this way.

So let’s remove the print because this looks way nicer. We can see that we have a zero, and then we have a different type of zero, and then we have A, B, and C. There is something odd here.

I’m going to assume that A, B, and C are types of holidays, but these two different zeros suggest that something went wrong during data entry. That’s how I’d interpret this right now.

This leads us to task number five. We are going to binarize this set. Instead of having zero, zero, A, B, and C, we’re going to have just zero and one. The A, B, and C values will become ones, and both types of zero will become zeros.

Let’s do this. I’ve found that the simplest way is to use the following format. We take dataframe["state_holiday"] and use .isin(["A", "B", "C"]). Then we convert this to integers using .astype(int). This makes sure that whatever matches A, B, or C becomes a one.

I find this approach intuitive because we’re simply saying whether the value is in A, B, or C. The astype part may not be intuitive at first, but it’s a very simple way to convert booleans into zeros and ones.

Cool. Then we run this and replace the column in the same variable. There was one extra quote there, so we remove it.

Now let’s check again by running value_counts() on state_holiday, just like before. Control + Enter, and here we go. We now only have zeros and ones.

# **C) Subsetting Stores and Basic Aggregations**

When we look at the data, we can see that we have stores, and because there are a lot of entries, it’s not immediately clear how many stores we actually have. It’s usually very helpful to understand the structure of the data you’re analyzing.

So we’re going to count this. Let’s make it nice and readable by printing it using an f-string. We print something like: “There are” and then the length of dataframe.store.unique(), followed by “stores”.

When we run this, we see that there are 1,115 stores. That’s quite a lot for the sake of our analysis.

As we start exploring the data or even doing data visualization, visualizing 1,115 stores is simply not reasonable. So there are two things we’re going to do. First, we’re going to randomize a subset to see how things work. Second, we’re going to simplify our analysis from here on out.

What we’re going to do is subset the data to ten random stores, using a random seed of 1502, and then store the result. Let’s do this.

We start with the original DataFrame and sample from the store column. Step by step, we first sample ten stores. When we run this, you’ll notice that every time we run it, the results are the same. That’s because we set a random state.

The random state makes sure that the results I get and the results you get are the same. It doesn’t matter how many times we run it. If you change the random state, the stores will change, but as long as we keep it at 1502, the output remains consistent.

So now that we have these ten stores, we take the DataFrame and filter it. We go to the store column and use isin() to check whether each row belongs to one of these selected stores. When we run this, we get a series of booleans—True and False.

The next step is to use this boolean mask to retrieve only the rows where the value is True. This tells the DataFrame to keep only the rows corresponding to those ten stores. When we do this, we can see stores like 24, 47, and others appearing in the filtered data.

At this point, we want to store this result. So we save it as dataframe_ten. We can quickly preview it to confirm everything looks correct.

One important best practice here is to use .copy(). This ensures that we’re working with a separate copy of the data and that any modifications we make won’t unintentionally affect the original DataFrame. This is just good hygiene when working with pandas.

Now that dataframe_ten is properly set up, we can move on.

Task number three is to compute the mean sales for each of the ten stores and sort them in descending order. Let’s do this.

We take dataframe_ten, group it by store, then select the sales column and compute the mean. Once we have the result, we sort the values in descending order.

When we look at the output, it’s not immediately obvious which store is the biggest just by scanning the numbers, so sorting helps. After sorting, we can clearly see that store number 4 has the highest average sales, while store 794 has the lowest.

There’s quite a big difference here—roughly a threefold difference in average sales. And remember, this is just within our subset of ten stores.

Now, one last thing that I find particularly interesting is the following: for each store, we want to find the row corresponding to the day with the all-time highest sales.

In other words, we want to know which specific day each store achieved its maximum sales.

To do this, we take dataframe_ten, group by store, and focus on the sales column. Instead of computing the mean, we now use idxmax(), which gives us the index of the row where sales were highest for each store. We could also look at idxmin() if we wanted the worst day.

Once we have this, we can optionally sort the results in descending order to see the biggest peaks first.

When we inspect the output, we see something remarkable. For example, store 864, which averages around 3.5K in sales, had one day where sales were close to one million. That’s roughly 300 times its average daily sales, which is absolutely insane.

# **D) Working with Datetime Index and Weekday Patterns**

Task number one is to set the date column as the index, and then preview the first rows.

To give a bit of insight here, when we work with time series data, we care about time. Setting time as the index enables us to manipulate the data in a much easier and more natural way. This is especially important for visualization, because Python then understands that we are dealing with data evolving over time. Without this, Python doesn’t really know that the data is temporal, and I’ll show you that in a moment.

For now, we set the date column as the index. The way we do this is by using dataframe_ten.set_index("date", inplace=True).

Let’s preview a few rows to see what we get. We can see that the date is now shown in a consistent format. That looks good.

But now let’s look at what Python actually thinks about our date column. If we go back and inspect the information, we see that the date is still an object. An object means characters or strings, not a real date.

So we inspect dataframe_ten.index. When we look at it, we see that it has a start, but it’s still considered an object. The name is “date”, but that name itself is meaningless—it’s just a label.

What we need to do is convert this into a proper datetime object. To do that, we use pandas and apply to_datetime on the index. Once we run this, Python now understands that this index represents dates.

At this point, Python knows that we’re working with time, and this allows us to properly explore and manipulate the data from a time series perspective.

Cool.

Now let’s look at something interesting. Even though we’re not heavily using the index yet, a useful exercise is to compute the average sales per day and per store.

Let’s do this.

We take dataframe_ten and use groupby. At this point, you can probably already see how relevant grouping is. We group by day of week and by store. Then we select the sales column and compute the mean.

Here we go.

Now, I know—I know—it’s not very easy to interpret this output just by looking at the table. That’s fine. That’s going to be addressed in task number four.

Before moving on, I want to highlight something important. Instead of using the existing day_of_week column, we could have used dataframe_ten.index.dayofweek, and the result would be exactly the same. In our case, the day-of-week variable already exists, which makes things easier.

But when you work with time series data, you can create all sorts of time-based variables directly from the index—day of week, month, year, and so on. This makes feature engineering very powerful and very simple once your index is properly set.

All right.

So now we move on to visualization.

We want to visualize the average sales per weekday and per store using a line plot.

The simplest way to do this is to reuse the code we already have. I’m a big fan of reusing code whenever possible.

The first thing we need to do is unstack the grouped result. Once we unstack, we can already see that this becomes easier to compare across weekdays—0, 1, 2, 3, and so on. Day 6 corresponds to Sunday.

Since we’re dealing with Germany, this makes sense because many stores are closed on Sundays. Still, even with this structure, it’s not very easy to visualize just by looking at numbers.

So the next step is to call .plot().

When we do that, the visualization becomes much clearer. Now we can actually see the patterns.

Looking at the plot, one thing immediately stands out. All stores have zero sales on Sunday except for one. This is something interesting and consistent with what we saw in the table.

However, there’s another issue. The magnitude of sales differs a lot between stores. Some stores reach sales around 10,000, while others are much lower. This makes comparisons tricky, because the scale dominates the visualization.

And this is where the exercise really starts.

I want you to start thinking about this: when we want to understand how sales evolve during the week, we’re essentially looking at a seasonal pattern. To truly compare these patterns across stores, we should also start thinking about standardizing the data.

# **E) Standardizing Sales and Comparing Weekday Patters**

In order to properly compare behavior across different stores, we need to standardize the data. Standardization ensures that there is a common denominator among the stores, so that we are truly comparing apples with apples.

The standardization formula itself is very straightforward. It works as follows: we take the value, subtract the average, and divide by the standard deviation. This is the classic z-score formulation.

So this is step one. The main challenges here are twofold. First, we need to build a function. This is a very standard Python task and a great way to practice writing functions. Second, we need to apply this function correctly to our data, making sure that we zoom in on each store individually. In other words, we want to compute the mean and standard deviation per store and standardize sales within each store.

This brings us to task number one.

We need to define a function that standardizes a numeric series using z-scores, meaning value minus mean divided by standard deviation. Let’s do this.

We start by defining a function called standardize, and we pass a series as the input. Inside the function, we return the series minus the series mean, divided by the series standard deviation. This is very straightforward.

Now that the function is defined, we need to apply it to our data.

What we need to do is take our DataFrame and group it by store. This ensures that we are focusing on one store at a time. Then we focus on the KPI we care about, which is sales.

When we do this, we group by store, select sales, and apply a transformation using our standardize function. At first, it may show as a generic grouped series because we haven’t applied the transformation yet. But once we apply transform(standardize), the values are standardized per store.

After running this, we can see that the values are now standardized. This allows us to really work with and compare patterns across stores much more easily.

Recall that standardized values are centered around zero, and most values typically lie between -2 and 2. This makes interpretation and comparison much more intuitive.

This leads us to task number three. But before we do that, we need to store the standardized values.

We assign the standardized sales back into our DataFrame using .loc across all rows, and we create a new column called sales_standardized. The name “STD” here refers both to standard deviation and to the normalization itself.

Once we run this, the new column is created. We can quickly preview a few rows using .head() to confirm that everything looks correct.

Now we move on to computing the mean of the standardized sales.

We take dataframe_ten and group by day of week and store. Then we select the sales_standardized column and compute the mean. This gives us the average standardized sales per weekday and per store.

This naturally leads us to task number four.

We take the result, unstack it, and then plot it. This allows us to visualize the standardized weekday patterns much more clearly.

However, there are a few issues here. Nine out of ten stores have no sales on day seven, which is Sunday. This completely skews the visualization. From an analysis perspective, this is not ideal because zeros don’t really carry meaningful information in this context.

One thing that clearly stands out is the brown series, store 353. It shows extremely strong seasonality on Sundays. That makes sense, because if most other stores are closed, this store benefits disproportionately.

This is something we can infer from the data.

If we look at the other series, we see a different pattern. Sales tend to decrease day by day until Saturday, showing a clear weekday seasonality. That’s one conclusion we can draw.

At the same time, Sunday completely distorts the picture and makes it difficult to analyze the remaining patterns properly. We clearly need to clean this up further.

Still, let’s continue exploring.

Let’s take a closer look at Sundays specifically. Are stores always closed? Is there even a single Sunday where some of them are open? The same question applies to the other stores as well.

These are things we can visualize, explore, and then use to draw our own conclusions.

# **F) Analyzing Sales on a Specific Weekday**

I have to say, I only listed one task here, but this could have been done in multiple ways. What we need to do is filter the data for day seven and then visualize it. So this will be step number one and step number two at the same time.

First, let’s import what we need for visualization. We import matplotlib.pyplot as plt. That’s step number one.

Next, we create a new DataFrame called dataframe_d7. We start from dataframe_ten. There are many ways to do this. One way is to filter where the day of the week is seven and then make a copy. That’s one valid approach.

Another option would have been to use the index directly, since we’ve already set it to datetime. That’s another possibility. In this case, we’ll stick with using the existing variable and filtering based on the day of the week.

Now we move on to plotting.

We start by creating a figure using plt.figure(). We set the figure size to 12 by 6, which is a fairly standard size.

Next, we loop through the data. For each store and its corresponding data in dataframe_d7 grouped by store, we plot a line. We plot the date, which is stored in the DataFrame index, on the x-axis, and sales on the y-axis. For each line, we set the label using an f-string so that it shows the store number.

This loop allows us to plot each store’s Sunday sales on the same chart.

We close the parentheses and then display the plot. This is already enough to generate the visualization. It’s also a nice introduction to matplotlib if you’ve never used it before, because it shows the basic structure of creating a figure, plotting data, and labeling it.

Now, here’s a cool thing that you might or might not have noticed. Let’s quickly look at dataframe_d7 itself. You’ll see that the earliest dates are from 2015, and the later dates are from 2013. In other words, the data is reversed.

However, because Python knows that we’re dealing with dates, this doesn’t matter at all. When we plot the data, everything is automatically ordered correctly along the time axis.

That was just a quick side note.

Now let’s clean up the plot a bit. We add a title, “Sales on Sunday”. We label the axes with date and sales. We also add a legend so that we can identify each store.

I also like to add plt.tight_layout(). This usually tidies things up a bit and makes the plot easier to read, which I personally appreciate.

When we look at the plot, I feel it’s a bit too tall, so we reduce the figure height from 6 to 5. That small adjustment makes it look better.

Now, what do we see?

We see that every single store except one is always closed on Sundays. There are no exceptions at all. This tells us that by including Sundays in our analysis, we’re potentially introducing noise—especially when we standardize the data.

For store 353, there is one Sunday where the store was closed, which explains a drop. But aside from that, this store has recorded sales on every single Sunday, and those sales are growing over time. That’s definitely interesting to observe.

And that’s it.

This is how we do a quick visualization to explore a specific pattern in the data. I would say this exercise is on the more difficult side, because it includes a for-loop and multiple steps. But if you understand this, you’ll be able to handle virtually every visualization throughout this course.

# **G) Removing Outliers and Re-Assessing Intra-Week Seasonality**

Our conclusion so far is that we clearly have store 353, which behaves very differently from the others. There’s really no point in comparing it directly with the rest. At the same time, we also have day seven, which is Sunday, and this day is kind of “poisoning” our data because it introduces a lot of noise.

Because of this, task number one is to remove all observations for day of week seven and for store 353. Doing this will give us a much cleaner DataFrame, allowing us to continue exploring seasonality in a more meaningful way.

Cool, let’s do this.

We start from dataframe_ten. We want to keep only the rows where the day of week is not seven and the store is not 353. Since we have two conditions, we need to combine them.

First, we filter dataframe_ten where the day of week is not equal to seven. Then, we add a second condition where the store is not equal to 353. Finally, we make a copy of the result to avoid any unwanted side effects.

We store the result in a new DataFrame called dataframe_clean.

Once that’s done, we can take a quick look using .head(). This preview doesn’t show anything dramatic, because we’ve removed rows rather than added new ones. The main purpose here is simply to confirm that we still have data and that the operation worked as expected.

Now that we have a clean DataFrame, the next step is to re-standardize the sales column within each store, but this time using dataframe_clean.

We essentially repeat the same process as before. We take the sales column, group by store, and apply the standardization transformation. The result is stored again in the sales_standardized column.

If we preview a few rows using .head(), we can immediately see that the standardized values have changed. For example, values that were previously around 1.5 and 1.1 are now closer to 1.65 and 1.16. This makes sense, because we’ve changed the underlying data by removing Sundays and the outlier store.

This brings us to task number three.

We now take dataframe_clean and group it by day of week and store. Then we select the sales_standardized column and compute the mean. This gives us the average standardized sales per weekday and per store, using the cleaned data.

As mentioned earlier, we can store this result. Let’s call it intra_week.

Now we move on to task number four, which is visualization.

To visualize the intra-week seasonality, we take intra_week, unstack it, and then plot it. Compared to before, the view is much clearer. Earlier, everything almost looked like a straight line. Now we can actually see meaningful differences and patterns.

We can also customize the plot directly within pandas. For example, we can set the figure size to 12 by 6, add a title such as “Intra-week Seasonality of Standardized Sales”, and adjust the layout. If we want to remove or fine-tune certain axes, we can do that as well.

Reducing the figure height slightly—from 6 to 5—makes the visualization a bit cleaner and easier to read.

Now let’s interpret what we see.

There’s one store, store 267, that shows very strong Saturday performance, which is quite unique. In general, Thursday to Saturday seem to be the strongest days for most stores. From Monday to Tuesday to Wednesday, sales tend to decrease, and then there appears to be some stabilization.

Overall, Monday is often the strongest day, Saturday the weakest—except for that one store. We also see a general decrease until Wednesday, followed by a flatter pattern for the rest of the week.

One important takeaway here goes beyond just seasonality. This exercise demonstrates the process of time series analysis. We take a sample of data, analyze it, and identify problems. Then we ask questions like: “What’s causing this distortion?” In this case, Sunday was the issue, so we removed it.

Next, we notice a store that behaves completely differently. That store clearly belongs to a different category and should be analyzed separately. Ideally, we would later compare stores that are open on Sundays with those that are not, because they may not be directly comparable.

This is how we move step by step toward insight: analyze, detect anomalies, clean the data, reanalyze, and visualize the results. And ideally, at the end of this process, we present clear visualizations that support our conclusions.

# **H) Analyzing the Impact of Promotions**

One of the most interesting questions—especially in revenue planning or demand planning—is understanding the impact of promotions on revenue. This is something we are going to explore slowly here, just a little bit, from a more data-exploratory perspective.

Let’s start with the first task. We want to compare the average sales on promotion days versus non-promotion days. To do this, we use a function you’re probably already tired of by now: groupby. We group by the promo variable and then compute the mean. This immediately gives us something concrete to compare—average sales when promotions are active versus when they are not.

At this point, you might notice something odd happening, especially if you’re working in Colab. You may realize you’re doing something wrong, or Colab might not be cooperating properly. When that happens, it’s totally fine—and often necessary—to restart the runtime and rerun everything from scratch. That’s exactly what we do here.

After restarting and rerunning the pipeline, we see the results clearly. Average sales on promotion days are around 7991, while on non-promotion days they are around 4.4k. That’s already interesting. Here, I’m intentionally switching back to using the full data frame instead of one of the cleaned or filtered versions from before. For this particular analysis, working with the complete data frame feels more natural and helps illustrate a few additional concepts.

Now we move to task number two. Instead of looking at promotions globally, let’s break this down per store. We want to compare promotion versus non-promotion sales at the store level. To do this, we group by both store and promo, select sales, compute the mean, and then unstack the result. This gives us a table where, for each store, we can directly compare average sales with and without promotions.

We store this result in a new object called promo_uplift. This data structure now contains the mean sales per store, split by promotion status. We can inspect it using .head() or similar methods to confirm everything looks correct.

Task number three is where things get more interesting. For each store, we want to compute the relative uplift in sales during promotion days compared to non-promotion days. The idea is simple: we take the difference between promotional and non-promotional sales and divide it by the non-promotional baseline. In other words, (promo − non_promo) / non_promo.

This relative format is very useful because it puts all stores on a common scale, similar to standardization. Even if stores have very different absolute sales levels, relative uplift allows us to compare them fairly. We compute this relative difference and add it back to our promo_uplift structure. After that, we can again inspect the results to make sure everything looks reasonable.

At this stage, a very natural and insightful question arises: in which stores is promotion actually making a meaningful difference? To answer this, we look for the stores with the largest relative uplift. We take the computed uplift values and use nlargest(5) to identify the top five stores in terms of promotional elasticity.

This reveals some very interesting results. For example, stores like 198 and 607 stand out, and one store—store 108—shows an uplift of around 2.25x. That’s a very significant increase and clearly indicates that promotions are extremely effective in that store.

Task number five is simply the flip side of this analysis. If we can find the highest promotional elasticity, we should also look for the lowest. Using nsmallest(5), we identify the stores where promotions have little to no impact on sales. This immediately raises important business questions: what’s happening in those stores? Why don’t promotions work there?

Of course, there are many possible explanations. There could be other factors we’re not accounting for, such as how often promotions are run, when they are run, or how large they are. Promotions aren’t just a binary yes-or-no variable. They have magnitudes—depth of discount, breadth of products included, and timing over time. These are what we call confounding factors, and they can strongly influence the observed effect of promotions.

Overall, this exercise shows how we can start uncovering meaningful insights step by step using relatively simple exploratory techniques. If you’d like to see more exercises like this—especially ones that dig deeper into real business questions—just let me know, and I’d be very happy to create them for you.

# **IV) Section 4: Introduction to Time Series Forecasting**

# **A) Game Plan for Introduction to Time Series Forecasting**

Welcome to this video where I’ll make this game plan for our introduction to time series forecasting.

The thing is that we are going to talk about this new type of data. Well, not really new, but it could be new for you, which is time series data. Have you ever heard of it?

It’s all about looking at how things like stock prices or the weather change over time. And guess what? We are going to tackle it with Python. So don’t worry if you are new to this, I’ll guide you every step of the way, and I promise you it will be fun.

First off, let’s discuss what time series data is and what this really means. Imagine that you’re keeping track of your daily coffee spending. That’s time series data. Anything that records how things change day by day or month over month—that is time series data.

Now the cool part: we are going to use Python for all of this. We’ll start with the basics, and you’ll be amazed at how much you can do with just a few lines of code. I think you’ll be quite happy with how much we can achieve just in this section, which is an introduction.

We’ll get our hands dirty by sorting and playing with data. It’s really like putting a puzzle together, figuring out how to do it. We’ll look at patterns and trends. For instance, can we understand why Bitcoin’s price skyrocketed last week? Or why do sales dip every July?

You’ll learn how to spot these patterns, and of course, we’ll draw some graphs. Not just any graphs, but ones that really tell a story. You’ll learn how to turn numbers and dates into cool visual stories that anyone can understand.

Ever wonder if you can predict stuff like stock prices? We’ll talk about that too. It’s a bit like trying to guess the end of a movie, but with data and trends.

And to wrap it up, we’ll look at real-world examples where forecasting didn’t go as planned. It’s really about looking at someone else’s mistakes—and trust me, there’s really a lot to learn there.

# **B) What is Time Series Data?**

In this video, we are going to dive deep into time series data, and I’m also excited to introduce you to a particularly intriguing data set: the Bitcoin price data.

This data set will be the central focus of our tutorials. It tracks the daily price of Bitcoin spanning nearly a decade, from 2014 to 2023.

Now, why Bitcoin price data? Bitcoin, as a pioneer of cryptocurrencies, has a rich and dynamic data set. Its market is known for volatility, rapid price changes, and significant trends, making it an excellent subject for time series analysis.

By studying this data set, you’ll gain insights not just into Bitcoin’s price movements, but also into broader financial market dynamics and investment behavior.

Now let’s understand the essence of time series data. Time series data is unique, and I really want you to understand why. It’s like a chronological story, where each data point is a moment in time, neatly lined up from the oldest to the newest.

You’ll often find this data captured at consistent intervals—think daily, weekly, or monthly snapshots of data.

If we explore its wide-ranging applications, time series data isn’t just about finance. Imagine it being used in weather forecasting, where it helps predict rainfall or temperature trends. In economics, it’s crucial for analyzing GDP growth, and in healthcare, it’s used for monitoring patient heart rates over time.

Now let’s go deeper into the unique statistical tools used in time series analysis. Time series analysis introduces some fascinating statistical concepts. Together, we’ll look at autocorrelation—understanding how a data point is related to its past.

In fact, time series data is very unique because we use data from the past to predict the future. Moreover, we’ll discuss seasonality, identifying patterns that repeat over time. These concepts are key to accurate forecasting and trend analysis.

As we step into this journey through time series data, I encourage you to think about how this knowledge could enhance your own projects. What questions do you have? Do you see any practical applications for what you are learning?

Feel free to reach out in the Q&A or the student communities. I’m here to help. Until the next video—have fun!

# **C) Python - Libraries and Data**

Welcome to our first Python tutorial.

In this video, we are going to set up everything when it comes to our Python script. I have stored my materials here in my drive because we are going to use Google Colaboratory. You are welcome to use anything else, but I would strongly recommend that you do this with me.

That said, the materials are there for you, and of course, your IDE is your IDE. If you are new to Python, especially, I strongly encourage you to follow along, but I’m always here to help in case you have any issues.

If you decide to pick something else, you’ll find in the materials that there are several folders. We’re going to start with the time series analysis, and then with the introduction to time series analysis. We’re going to click on New, then More, and then Google Colaboratory—either create and share or just create.

If you have not shared it with anyone, it will just be “Create.” In case you’re having issues finding all the materials, they are in lecture number three or four of this course. You’ll be able to find everything there. There’s a link to my website, and on my website, there’s a big download button.

Now I’m going to change the title to Introduction to Time Series Analysis. You’ll notice that there are several things here. This is the table of contents, and we’ll build on this by creating headings and subheadings.

There’s also something here called Secrets, which we’re not going to use in this course. Then there’s Files—this is how we connect to Google Drive, and we’re going to use this extensively.

Let me mount the drive already. Connecting to Google Drive is called mounting Google Drive. You click on the drive icon, connect to Google Drive, and sometimes you’ll get a piece of code that you need to run.

If you’re wondering what that piece of code might be, it’s:

from google.colab import drive
drive.mount('/content/drive')

This is the code used to mount the drive. It doesn’t happen every time that you get this piece of code, which is why I’m sharing it—especially since this is the first time.

Once the drive is mounted, you’ll see something called drive. You click on drive, then go to My Drive. I have personally stored the folder on the homepage of the drive—the main part of the drive.

You’ll see Python Time Series Forecasting, then Time Series Analysis, and then Introduction to Time Series Analysis. You go to the three dots, copy the path, and then click back on code.

Now we’re going to set the directory. This is how we always connect to that specific directory, where in this case we have a couple of CSVs that are going to be our datasets. To do that, you use %cd to change the directory, and then you paste the path using Ctrl + V.

Here we go.

Let me also add a section here—this is going to be the Setup. To see this, we go to the table of contents, and you’ll notice that Setup now appears there.

Next, I definitely want us to import libraries and data. For importing libraries, we’ll import pandas as pd, numpy as np, and matplotlib. For now, this is going to be it. Each time we require a specific function or library, I’ll import those later. This way, we’re building everything step by step.

Then we’re going to load the dataset using pandas.read_csv. We’ll start with the Bitcoin price dataset. This is a good dataset for daily data—bitcoin_price.csv.

Then we use DataFrame.head(). This allows us to preview the first five rows of the data.

Here we go. We have seven KPIs. We have the date, starting from 2014, and then we have open, high, and low. Open is the price at the start of the day, high is the maximum, and low is the minimum.

Then we have the close, which is the value at the end of the day, and the adjusted close. The adjusted close accounts for things like dividends or splits. In this case, there’s nothing like that, but the adjusted close is the value we typically use because it’s the most relevant indicator of the value at the end of the day.

Finally, we have the volume, which is the trading volume for that specific day.

Now, what I want us to do is to close this video. In the next one, we’re going to return and explore something called the time series index. We’ll see how to do it, and that’s going to be it for now.

# **D) Python - Time Series Index**

In this video, we’re going to focus on the Time Series Index. This is a very important topic because when you’re dealing with the index—and you actually make the index out of dates—it makes your life much easier when it comes to time series analysis. Whether it’s plotting, forecasting, or the analysis itself, everything becomes quite straightforward.

Not all libraries that we are going to work with require this step, and I’ll show you which libraries do and which ones do not. However, for the sake of doing some pre-analysis, the main libraries that we work with—such as pandas, matplotlib, and later on, Statsmodels—use the date as the index. This is why it’s very important that you master this part of time series analysis.

What we’re going to do is convert the date column into a datetime object and then set the date as the index. These are two different steps, so let’s give it a go.

The first thing we need to focus on is the date format. In our case, the date is already in the correct format, which is year-year-year-year, month-month, day-day. This format is expressed using %Y-%m-%d. The capital Y means we are using four digits for the year, followed by the month and the day.

We can even see that tools like Google Gemini suggest using pandas.to_datetime. We set the column as date and specify the format. For our dataset, the date is already in the correct format, but it’s still a good practice to check.

Let me check this one. Okay, this one is fine as well. But in the next section, you’ll see that the date is not in the correct format. The bottom line is that you can always handle this by specifying the current format, and pandas will transform it into the correct one.

If we run DataFrame.head(), nothing is really going to change visually. The important step is setting the date as the index. To do that, we use set_index, specify the date column, and use inplace=True. Inplace means that the change is applied directly to the DataFrame, and the date column itself is removed.

Let me run this. Here we go—now the date is our index.

There are a few different ways to do this, and I’ll show you another one in a moment. But first, I want you to see what this allows us to do. For example, if we want to select a specific day from our DataFrame, we can use .loc and reference the index directly.

We specify a date, such as 2021-11-09, and when we run it, we get all the values for that specific row.

Now, another thing I want to show you is that we can also set the index at the time of importing the data. Let me call this DataFrame one. We again use pandas.read_csv with the same Bitcoin price CSV file. This time, we specify index_col='date' and parse_dates=True. This ensures that the date is parsed correctly and set as the index right away.

This approach gives us exactly the same result.

Before we close this video, I want to show you one last thing. You can always resample time series data to a different time granularity.

What does this mean? Imagine we have daily data, but daily values might be too noisy. Maybe weekly data would make more sense for our analysis. This is completely possible.

To do this, we take the DataFrame, use .resample(), specify 'W' for weekly, and then apply an aggregation such as .mean(). As you can see, this gives us weekly data.

You could also use the median, the maximum, or any other aggregation you can think of. If you can think of it, it probably exists—it’s just a matter of knowing what to apply.

Instead of weekly, you could also resample monthly, quarterly, or yearly. It’s really about deciding how to transform daily data into another time granularity.

This always depends on the business requirement. Do you care about daily data? If yes, then keep it. If weekly data is more relevant to your business problem, then go with weekly. It’s always about defining the business problem first, and then letting the analytics follow.

# **E) Python - Exploratory Data Analysis Part 1**

In this video, we’re going to talk about exploratory data analysis. Here we go.

It’s very important that we always, always understand the data that we are working with. In this case, we want to understand its cycles, its growth, and how it is developing. This is one of the fundamental things that really distinguishes a good analyst, scientist, or engineer from a mediocre one.

It all comes down to purpose. What are we doing? What is this for?

I’m going to cover a couple of things that are extremely simple, but also very common in time series analysis—one of them being the rolling average. This is particularly important when we start doing feature engineering, such as combining variables or averaging them. In this case, rolling averages create smooth curves, which are very useful for analysis.

Let’s do it.

Our first step is to generate a seven-day rolling average for the closing price. This is very straightforward. The way we do it is by taking the DataFrame, selecting the close column, and then applying the rolling method.

We specify a window of seven days, and then we decide how we want to aggregate—here, we use the mean. Once we do that, we get the rolling average. Of course, the first few values are missing because we don’t yet have a full seven-day window, but after that, the values appear.

What I think is particularly relevant is plotting two things together. Before doing that, I want to store this rolling average in a variable. I’ll call it the seven-day closing average. Now this value is stored and ready to use.

Next, I want to plot both the closing price and the seven-day rolling average together. Once we do that, we can clearly see the difference between the raw data and the smoothed curve.

Let me make sure we actually show the plot. Sometimes, especially in Jupyter environments, plots don’t display correctly unless we explicitly call plt.show(). This helps avoid issues with axis labels and rendering.

You can see quite a lot of data here, so let’s zoom in. A very easy way to do this is by using .loc and specifying a year, for example 2023.

Now we can clearly see what’s happening. The orange line represents the seven-day closing average. You’ll notice that it’s always slightly delayed compared to the actual closing price. This makes sense—it takes time for new values to influence the average.

That’s exactly how rolling averages work. You can shift values if you want, and I’ll show shifting later, but that’s not the nature of a rolling average. The more days you include in the window, the more delayed the curve becomes. Seven days is a relatively small window, so the delay is manageable, but it’s still there.

The next thing I want to show you is also related to rolling, but it’s more about aggregation. Let’s say we want to find the month with the highest average closing price.

To do this, we resample the data on a monthly basis and calculate the mean. Then we focus on the close column and look for the index of the maximum value. This gives us the month with the highest average closing price.

You might see a warning that the letter M is deprecated and that you should use ME for month-end, but the logic remains the same. Once we run this, we get our result.

This is one way to explore the data. You can also preview different parts of the dataset. For example, if you want to see the last five rows, you can use DataFrame.tail(). This is essentially the opposite of DataFrame.head().

And that’s it for this video.

You might notice that I mention things like data manipulation, seasonality, autocorrelation, and partial autocorrelation. Realistically, this entire section is dedicated to giving you techniques to get to know your time series better.

The goal is that, by the end, you have a toolbox of methods you can use to understand any time series you’re working with.

# **F) Python - Exploratory Data Analysis Part 2**

In this video, we are going to continue with the exploratory data analysis.

One thing that’s very interesting in time series analysis is looking at day-over-day or period-over-period comparisons. For example, you take one day and then look at the percentage change compared to the previous one. This is exactly what we’re going to do here.

We’re going to compute the percentage change for the closing price variable. There is a built-in function in pandas that allows us to do this very easily.

We always start by selecting our variable, so we take DataFrame['Close']. Then we apply the percentage change function, which is already available. When we do this, we immediately get the percentage change for each day.

The first day is, of course, not available because we don’t have a previous day to compare it to. Essentially, the calculation works by dividing the current day’s value by the previous day’s value. For example, one day divided by the day before it. This type of calculation is extremely common in financial data and is very helpful because it connects the dots between consecutive periods.

Personally, I like to multiply this value by 100 so that it’s easier to visualize as a percentage. I find that having the values expressed this way makes interpretation much more intuitive.

We then store this under a new column in the DataFrame, something like daily_returns_100pct, so it’s always clear that the values have been multiplied by 100.

Once we have this, we can start exploring the data. For instance, we might want to check which days had more than a 10% change in price.

To do that, we take the DataFrame and filter it by selecting the daily returns that are greater than 10%. This gives us all the days where the price increased by more than 10%.

But then you might think—what about days where the price dropped by more than 10%? Those are not included in this filter.

The solution is very simple. We take the absolute value of the daily returns and then check which values are greater than 10%. This way, we capture both large positive and large negative changes.

Now we’re looking at all the days that experienced more than a 10% change in either direction. For this dataset, you can see that there are around 97 days with more than a 10% change.

That’s massive and clearly shows how extremely volatile this data is.

That’s it for this video. In the next one, we’re going to focus on data visualization and explore this dataset visually.

# **G) Python - Data Visualization**

In this video, we are going to explore data visualization. Of course, we have already done some visualization earlier, but now I want to explore a few additional parameters that you can add to make your visualizations cleaner and more visually appealing.

Let’s take a look.

We will start with the very basics, as always. For example, plotting the daily closing price. We simply go to our DataFrame, select the close column, and call .plot().

We’ve done this before, but let’s improve it a little. For instance, we can add a title. We just specify something like “Daily Closing Price”, and now the plot looks more informative and polished. This is one simple way to enhance your visualization.

Next, let’s explore something slightly different. We’ve already looked at rolling averages earlier, but now let’s apply it to a different variable.

We’ll create a 30-day rolling average for volume. To do this, we start with our DataFrame, select the volume column, apply a rolling window of 30, and then compute the mean. This gives us a new variable representing the 30-day rolling volume.

Now let’s try to plot both the closing price and the 30-day rolling volume together.

If we try to plot them directly on the same chart, it doesn’t really make sense because the magnitudes are completely different. The volume values are much larger, which causes the closing price to appear almost like a flat horizontal line.

So how do we fix this?

The solution is to use two different y-axes.

We start by plotting the 30-day rolling volume and enabling the legend. Then we add plt.show() to display the plot properly.

Next, we plot the closing price on a secondary y-axis. We specify secondary_y=True and also enable the legend. It’s important to remember that True must be capitalized so that Python recognizes it as a boolean value.

Finally, we label the y-axis for the closing price. This is how we correctly visualize two variables with very different magnitudes on the same plot.

Now we can actually start connecting the dots.

By looking at the visualization, we can begin to see potential relationships between volume and price. While it’s sometimes difficult to interpret visually, in general, when volume goes up, the price also tends to go up. This kind of relationship becomes much clearer when both variables are plotted together with separate axes.

As a final step, let’s briefly look at correlation.

We can compute the correlation between the closing price and the 30-day rolling volume. This gives us a numerical measure of how strongly these two variables are related.

If we print the results properly and use the .corr() function, we can see that the values are the same, just formatted slightly differently depending on how we display them.

It’s important to note that this correlation is the Pearson correlation, which is the default in pandas. Pearson correlation is often not ideal for time series data, but for our purposes, it still gives us a good intuition.

Just as an FYI, you might want to explore Spearman correlation, which is more commonly used for time series. However, I don’t want to go down the rabbit hole of statistics here.

Even with Pearson correlation, we’re still about 90% correct, and the overall conclusion does not change: the two variables are strongly connected. In fact, we don’t even strictly need the correlation value here, because the visualization already tells us the story.

That’s it for data visualization.

In the next video, I want us to focus on data manipulation. This is a specific topic that I’d like us to dive into next.

# **H) Python - Data Manipulation Part 1**

The first step is to identify missing values. The way we do this is by using our DataFrame, calling isnull(), and then applying sum(). This allows us to count the number of missing values per column.

Once we do this, we can clearly see that some of the columns we are working with contain null values. At that point, the real challenge becomes: how do we actually fill these missing values?

There is no single correct solution here. The approach you choose very much depends on the context of the problem. What I want to do is show you a few common techniques, and then based on your own use case, you can decide which one makes the most sense.

The first technique is to fill missing values using the next available observation.

For example, let’s say Day 29 is missing, but Day 30 has a value. In this case, Day 29 would take the value of Day 30. This approach is very useful when you want to maintain continuity in the data, or when you believe that something went wrong on a specific day and the next value is a reasonable replacement.

Again, this is just one option, and whether it makes sense or not depends entirely on your situation.

Let’s take a concrete example: the 30-day rolling volume.

We take the 30-day rolling volume series and apply fillna() with the method set to backward fill (bfill), and we try to do it in place.

When we do this, we may encounter an error or warning. This is actually an important topic to address.

The warning says that a value is trying to be set on a copy of a DataFrame or Series through chained assignment using an in-place method. It also mentions that this behavior will change in pandas 3.0.

This is one of the reasons I’m explicitly recording and explaining this, because these kinds of issues can easily show up in real projects. At the moment, we’re using pandas version 2.2, but future versions will be stricter.

The warning essentially tells us that instead of modifying a column in place using chained assignment, we should reassign the result back to the column explicitly.

So rather than using an in-place operation, we should do something like:
assign the filled result back to the DataFrame column itself.

When we try to follow this approach, we might run into another issue. For example, using method= inside fillna() on a Series may result in a deprecation warning. The message tells us that using method is deprecated and that we should instead use ffill() or bfill() directly.

This is another example of pandas evolving over time and why it’s important to keep your code up to date.

So instead of using fillna(method="bfill"), we directly use bfill().

Once we do this correctly, everything works as expected. This is a good example of how you can take a warning message, read it carefully, and fix your code step by step.

At this point, it’s worth clarifying the difference between backward fill (bfill) and forward fill (ffill).

Backward fill means that a missing value is filled using the next available value. This matches the earlier example where Day 29 takes the value of Day 30.

Forward fill does the opposite: it fills the missing value using the previous available value. In that case, Day 29 would take the value of Day 28, assuming it exists.

In practice, I personally tend to use backward fill more often, because in many time-series scenarios it makes more intuitive sense. But again, this depends entirely on your data and your assumptions.

Another common technique for handling missing values is interpolation.

Let’s take a different example: the 7-day closing average. We already know that rolling averages naturally introduce missing values at the beginning of the series.

Instead of forward or backward filling, we can interpolate these values.

To do this, we take the 7-day closing average series and call .interpolate() on it. We no longer use inplace=True, and the operation completes successfully.

This is how interpolation works in practice.

Whenever you use interpolation, it’s very important to check the documentation. I’m a big fan of going through the pandas docs, especially when working with something new or nuanced. It does take time to get used to, but it’s absolutely worth it.

In this case, we are working with a Series, which is one-dimensional data. A DataFrame would be two-dimensional, but that distinction doesn’t change much for interpolation in this scenario.

Looking at the pandas Series interpolate() documentation, we see that the default method is linear interpolation.

Linear interpolation assumes that the data points are equally spaced, which is true for daily time-series data. In most cases, linear interpolation will be sufficient.

The key difference between interpolation and forward or backward fill is that interpolation looks at both sides of the missing value. It considers the value before and the value after, and then computes a reasonable estimate in between.

Forward fill and backward fill, on the other hand, only look at one side of the equation.

To summarize:

Forward fill (ffill) uses the previous value

Backward fill (bfill) uses the next value

Interpolation uses both before and after values

Each approach has its place, depending on the problem you’re trying to solve.

That’s it for now. I spent some extra time here because missing values are a critical part of time-series data manipulation.

We’re going to come back to data manipulation and explore it further in the next video.

# **I) Python - Data Manipulation Part 2**

In this video, we’re going to focus on the index and also on feature engineering. Let’s get started.

The first thing we’re going to do is focus on the index. If we fetch the index from our DataFrame, this is as simple as calling dataframe.index. When we do that, we get all of the dates associated with our time series.

You may notice that the frequency is currently set to None. We’ll talk about frequency later, but for now, that’s perfectly fine.

What’s important here is that the index already contains a lot of valuable information. Because these are dates, we can actually create multiple features directly from the index. For example, we might want to know whether a given observation falls on a Monday, whether it’s a weekend, which month it belongs to, or which year it comes from.

All of this information already exists inside the index.

For instance, if we access the day of the week, we’ll see values like 0, 1, 2, 3, 4, 5, and 6. These correspond to Monday through Sunday.

Our goal here is to extract time variables and store them as new features.

Let’s start by extracting the year.
We create a new column called year, and assign it the value dataframe.index.year.

Next, we can extract the month by creating a month column and assigning dataframe.index.month.

We can do the same for the day of the month by creating a day column and assigning dataframe.index.day.

We can continue with more granular features. For example, we may want to extract the day of the week numerically. We do this by creating a column and assigning dataframe.index.dayofweek.

Sometimes, instead of numeric values like 0, 1, or 2, we want the actual day names, such as Monday, Tuesday, and so on. To do that, we create another column and assign dataframe.index.day_name(). This one requires parentheses.

There’s also another numeric representation we can use, which is weekday. This is essentially the same as day of week, but it’s still useful to store explicitly. We create a column and assign dataframe.index.weekday.

At this point, if we take a quick look at the DataFrame using head(), we can see that at the end we now have several new columns: year, month, day, day of week, weekday name, and weekday numeric.

As a final time-based feature, let’s create a weekend indicator.

We define a new column called is_weekend. This column will be a boolean that tells us whether a date falls on a weekend or not. The result is True for weekends and False otherwise.

This is actually very useful, because booleans are often easier to work with in models.

If you prefer to have this feature as a binary variable instead of True and False, you can convert it to integers using astype(int). This gives you 0 and 1, which is often ideal for machine learning models.

You can even add a comment to remind yourself that this conversion turns the feature into a binary representation.

All of this work is done using the index, but it also falls under feature engineering, which is what I want to focus on next.

There are many ways to perform feature engineering, and we’ve already covered some of them. For example, rolling averages—like a 30-day rolling average—are also a form of feature engineering. They smooth the data and help capture trends.

However, one of the most common and most important feature engineering techniques in time series is the creation of lagged variables.

So what are lagged variables?

Think about a real-world example. You’re browsing the web and you see an ad. You think it looks good, but you don’t buy the product that day. Instead, you purchase it the next day.

From a modeling perspective, the influence of the ad doesn’t happen immediately—it happens with a delay.

If we don’t include lagged variables in our model, we fail to capture this delayed effect. Lagged variables allow us to model situations where the impact of a regressor occurs in the future, rather than at the same time.

In other words, the value today may influence outcomes tomorrow.

Creating lagged variables is extremely simple.

For example, if we take the close column and apply shift(1), we create a lag-1 variable. This means that today’s row now contains yesterday’s closing price.

If we look at the values, we can see that what originally appeared on one date now appears on the next date.

If we want to go further, we can create a lag-2 variable by using shift(2). This shifts the values back by two periods.

As you can see, building lagged variables is extremely easy and very powerful.

We can store these as new columns, such as close_lag_1 and close_lag_2. If we preview the DataFrame again using head(), we’ll see these new lagged features added at the end.

All of this opens up a wide range of possibilities when it comes to modeling. As you start to understand your problem better, you begin to think about which variables influence outcomes and how those influences unfold over time.

This is where time series analysis and forecasting really begin to take shape.

# **J) Seasonal Decomposition**

In this video, I’m going to introduce you to seasonal decomposition.

The general idea behind seasonal decomposition is that we separate time series data into three distinct components: the trend, the seasonality, and the error term (also called the residual).

Let’s look at each of these components individually, starting with the trend.

The trend represents the general direction of the time series over time. You can imagine this as a smooth curve showing whether the data is generally increasing, decreasing, or remaining stable.

One important thing to keep in mind is that a trend can change over time, but it does not change every single day. If it did, it would no longer be considered a trend.

Next, we have seasonality.

Seasonality refers to recurring, cyclical patterns that repeat at regular intervals. A classic example is a time series that tends to be higher during the summer months and lower during the winter months.

You can imagine a chart where the data follows a repeating seasonal curve. This curve is cyclical, remains fairly consistent over time, and has a certain amplitude between its peaks and troughs.

Finally, we have the error term, also known as the residual.

The error term represents everything that is not explained by the trend or the seasonality. Ideally, this component behaves like random noise and does not exhibit any clear pattern. You can think of it as a random walk with no structure.

Now let’s zoom in specifically on seasonality.

One important thing to understand is that there are two main types of seasonality.

The first type is additive seasonality.

Additive seasonality is characterized by constant seasonal fluctuations. For example, this could mean that we always add 10 units in July or subtract 50 units in December.

In this case, the size of the seasonal effect stays the same regardless of whether the trend is low, medium, or high. If you were to plot this, you would see the same seasonal pattern repeating with the same amplitude over time.

The second type is multiplicative seasonality.

Multiplicative seasonality occurs when the seasonal cycles are proportional to the trend. Instead of thinking in terms of absolute units, we think in percentages.

For example, the data might increase by 10% in July or decrease by 50% in December. In a chart, you would observe that the size of the seasonal fluctuations increases as the trend increases. Even though the pattern repeats, the amplitude grows over time.

Now you might ask: why does this distinction matter?

By understanding whether your data exhibits additive or multiplicative seasonality, you can make better forecasts and better-informed decisions. The type of seasonality directly influences how models interpret patterns and predict future values.

Another important question is: how do we identify which type of seasonality our time series has?

Unfortunately, there is no statistical test that can definitively tell us whether the seasonality is additive or multiplicative. However, we do have two practical options.

The first option is data visualization.

By plotting the time series, we can visually inspect whether the seasonal fluctuations remain constant over time or whether they grow and shrink proportionally with the trend. This is something we’ve already started doing in earlier videos.

The second option—and this one is very powerful—is to focus on model performance.

This means building two different models: one assuming additive seasonality and another assuming multiplicative seasonality. We then compare their performance and see which one better fits the data.

Ideally, we should use both approaches every time.

Visualization helps us form an initial intuition before modeling, while model performance helps us validate that intuition afterward. In practice, the second approach is often preferred because it is results-driven.

In other words, the first option is about making an assessment before modeling, and the second option is about evaluating the results after modeling.

Of course, we’ll try both approaches.

But for now, let’s move on and see how to check for seasonality and plot it in Python.

# **K) Python - Seasonal Plots**

One of the most important topics in time series analysis is seasonality, which refers to the cyclical patterns that occur over time. These recurring curves appear again and again across specific time intervals, such as months or quarters.

There are many different ways to identify, analyze, and model seasonality, and throughout this course, I’ll show you several approaches. For now, however, we are going to focus purely on visualizing seasonality, which is often the first and most intuitive step.

There is one specific set of functions that I want to introduce for this purpose. To keep everything organized, we’ll start by importing them.

From statsmodels.graphics.tsa.plots, we are going to import two functions: month_plot and quarter_plot. These are specifically designed to visualize seasonal patterns.

In addition, although we won’t use it in this video, we will also import seasonal_decompose from statsmodels.tsa.seasonal, as it will be important later in the course.

To begin, let’s focus on monthly seasonality.

One of the simplest ways to analyze monthly seasonality is to take the monthly values of a variable and observe how they behave over time. This approach provides a clear visual indication of whether seasonal patterns exist.

We start by selecting the closing price from our data frame. Then, we resample the data to a monthly frequency using month-end ("M") and calculate the mean for each month.

Once this is done, we apply the month_plot function to the resampled data.

The resulting visualization contains two important components.

The black line represents the actual observed values for each month over time. In the case of Bitcoin, this line trends upward overall, but with large fluctuations due to volatility. Each point on this line corresponds to an actual monthly value.

The red line represents the average of the averages. In other words, for each month (January, February, and so on), it shows the average value across all years. This red line is what we use to identify seasonal patterns.

In this particular case, we do not observe strong seasonality. The red line does not show clear peaks or troughs across months, indicating that Bitcoin does not have a pronounced monthly seasonal pattern.

To clean up the output and ensure that only a single plot is displayed, we use plt.show().

We can also improve readability by adding a y-axis label, such as “Closing Price”, which makes the visualization easier to interpret.

Next, we can explore quarterly seasonality using a similar approach.

Once again, we start with the closing price, but this time we resample the data using quarter-end ("Q") instead of month-end. After calculating the mean, we apply the quarter_plot function.

This visualization makes seasonal differences more apparent. With the exception of Q3, which appears unusually flat or lower, the quarterly seasonality becomes clearer. This kind of insight can signal that further investigation is worthwhile.

At this point, it’s useful to look at a different data set that exhibits clearer seasonality.

For this purpose, we import a second data set containing monthly revenue for a chocolate company. This data set is already aggregated at a monthly level, which makes it ideal for seasonal analysis.

We read the CSV file using pandas.read_csv, specifying the date column as the index and enabling parse_dates=True to ensure the dates are handled correctly.

Because this data is already monthly, we do not need to resample it. We can directly apply the month_plot function to the revenue column.

This time, the seasonality is much more apparent. We can clearly see seasonal bottoms occurring during certain months and strong peaks during others.

For example, November stands out as a peak month, which aligns well with real-world events such as Black Friday, Singles’ Day, and holiday shopping seasons.

If we wanted to visualize quarterly seasonality for this data, we could easily do so by resampling using quarter-end frequency and applying the quarter_plot function.

However, we’ll stop here for now. There is much more to explore when it comes to seasonality, and we will continue building on this foundation in the next video.

# **L) Python - Seasonal Decomposition**

Now let’s take a look at one of the most important techniques in time series analysis: seasonal decomposition.

The general idea behind seasonal decomposition is that we take a time series and split it into three separate components. These components are the trend, the seasonal cycles, and the noise (or residuals), which represents whatever remains unexplained after removing trend and seasonality.

To start, we apply seasonal decomposition to our time series data. In this case, we are working with the adjusted close price from our data frame. We use the seasonal_decompose function and then call .plot() to visualize the result.

When we run this, we obtain a plot that shows four panels: the original series, the trend, the seasonal component, and the residuals. This gives us a structured way to understand what is driving the time series.

To make the visualization cleaner and easier to work with, we store the decomposition output in a variable called decomposition. From there, we generate the plot and explicitly set the figure size. A size of 18 by 10 generally works well and makes the components easy to read.

When we reduce the figure size significantly, for example to 5 by 5, we notice that the seasonal cycles are very difficult to see. Everything appears compressed, and the seasonal structure is effectively hidden. This highlights how important visualization choices are when working with decomposition.

At this point, we need to talk about an important parameter in seasonal decomposition: the period.

The seasonal_decompose function needs to know the length of the seasonal cycle. If the data index does not have an explicit frequency, or if we want to override the default behavior, we must specify the period manually.

This is critical because seasonal decomposition only supports one seasonality at a time.

This limitation is important to understand. For example, with daily data, we may have weekly seasonality (7 days), yearly seasonality (365 days), and potentially even monthly effects. However, classical seasonal decomposition forces us to choose only one of these.

This is a known limitation of early time series techniques. More modern models can handle multiple seasonalities, but here we must be explicit about which one we want to analyze.

Since we are working with daily data, one reasonable choice is to set the period to 365, which captures yearly seasonality.

When we do this, we start to see a seasonal pattern emerge. The seasonal component is not perfectly smooth, but it is consistent across time. The trend component shows longer-term movements up and down, and the original series reflects the raw adjusted close values.

If instead we set the period to 7, which corresponds to weekly seasonality, the decomposition becomes much less insightful. The seasonal and trend components do not reveal meaningful structure, and the output is not very informative.

This is expected, especially for stock market data. Stock prices are widely known to behave like a random walk, meaning they do not exhibit strong or reliable trends or seasonality. Because of this, predicting stock prices consistently is extremely difficult, if not impossible.

As a general rule, if someone claims they can reliably predict stock prices, they are likely trying to sell you something.

Now let’s contrast this with a different data set: the chocolate company revenue data.

This data is monthly and contains a single variable: revenue. We apply seasonal decomposition again, this time using a period of 12, since the data is monthly.

We start with an additive model, and the results immediately make more sense. The trend clearly increases over time, the seasonal component shows a repeating annual pattern, and the residuals are relatively small.

However, when we look closely at the revenue data itself, we notice something important. The seasonal spikes become larger over time. Early in the series, the seasonal fluctuations are small, but as revenue grows, the seasonal peaks and troughs grow as well.

This behavior suggests that an additive model may not be ideal.

In cases where seasonal fluctuations increase proportionally with the trend, a multiplicative seasonality model is more appropriate.

When we apply a multiplicative decomposition, the seasonal component is expressed as percentages rather than absolute values. A value of 1.0 represents a neutral effect. Values below 1 indicate a negative seasonal impact, and values above 1 indicate a positive seasonal impact.

For example, a value of 0.8 means revenue is about 20% lower than average due to seasonality, while a value of 1.4 indicates roughly a 40% increase. In this data set, months like November typically show strong positive seasonal effects, which aligns well with real-world business patterns.

This interpretation makes multiplicative seasonality far more intuitive for growing time series like revenue.

With that, we’ll stop here for now. There is still much more to explore when it comes to seasonal decomposition, and we’ll continue building on this in the next video.

# **M) Auto-Correlation**



# **N) Python - Auto-correlation**

# **O) Partial Auto-Correlation**

# **P) Python - Partial Auto-Correlation**
