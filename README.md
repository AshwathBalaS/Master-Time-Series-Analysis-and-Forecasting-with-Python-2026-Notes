# Master-Time-Series-Analysis-and-Forecasting-with-Python-2026-Notes
This Repository contains my "Master Time Series Analysis and Forecasting with Python 2026" Course Notes from Udemy

**I) Time Series Analysis and Forecasting with Python**

**A) Time Series Analysis and Forecasting with Python**

**B) Course Introduction**

**C) Overview of the AI Time Series Assistant**

**D) Diogo's Introduction and Background**

**E) Unlimited Updates and Enhancements 2026**

**II) Section 2: Part 1 - Time Series Analysis**

**A) Time Series Analysis Overview**

**III) Section 3: Python for Time Series Analysis**

**A) Game Plan for Python for Time Series Analysis**

**B) **Load and Explore Data****

**C) Subsetting Stores and Basic Aggregations**

**D) Working with Datetime Index and Weekday Patterns**

**E) Standardizing Sales and Comparing Weekday Patters**

**F) Analyzing Sales on a Specific Weekday**

**G) Removing Outliers and Re-Assessing Intra-Week Seasonality**

**H) Analyzing the Impact of Promotions**

**IV) Section 4: Introduction to Time Series Forecasting**

**A) Game Plan for Introduction to Time Series Forecasting**

**B) What is Time Series Data?**

**C) Python - Libraries and Data**

**D) Python - Time Series Index**

**E) Python - Exploratory Data Analysis Part 1**

**F) Python - Exploratory Data Analysis Part 2**

**G) Python - Data Visualization**

**H) Python - Data Manipulation Part 1**

**I) Python - Data Manipulation Part 2**

**J) Seasonal Decomposition**

**K) Python - Seasonal Plots**

**L) Python - Seasonal Decomposition**

**M) Auto-Correlation**

**N) Python - Auto-correlation**

**O) Partial Auto-Correlation**

**P) Python - Partial Auto-Correlation**

**Q) Python - Building a Useful Function Script**

**R) Can you predict stock prices?**

**S) What did we learn in this section?**

**T) CASE STUDY: Forecasting Gone Wrong**

**V) Section 5: Time Series Analysis Practice**

**A) Data Loading and Index**

**B) Data Visualization for Time Series**

**C) Exploratory Data Analysis for Time Series**

**VI) Section 6: Exponential Smoothing & Holt-Winters**

**A) Game Plan For Exponential Smoothing and Holt-Winters**

**B) CASE STUDY BRIEFING: Customer Complaints**

**C) Python - Exponential Smoothing Set Up**

**D) Python - Exploratory Data Analysis**

**E) Training and Test Set in Time Series**

**F) Python - Training and Test Set**

**G) Simple Exponential Smoothing**

**H) Python - Simple Exponential Smoothing**

**I) Double Exponential Smoothing**

**J) Python - Double Exponential Smoothing**

**K) Triple Exponential Smoothing aka Holt-Winters**

**L) Python - Triple Exponential Smoothing aka Holt-Winters**

**M) Measuring Errors for Time Series Forecasting**

**N) Python - MAE, RMSE, MAPE**

**O) Python - Predicting The Future**

**P) Python - Daily Data**

**Q) Python - Working on the Useful Code Script**

**R) Holt-Winter Pros and Cons**

**VII) Section 7: HOLT-WINTERS CAPSTONE PROJECT: Air miles**

**A) Capstone Project Presentation**

**B) Python Solutions: Task 1 and Task 2**

**C) Python Solutions: Task 3 and 4**

**D) Python Solutions: Task 5 and 6**

**VIII) Section 8: ARIMA, SARIMA and SARIMAX**

**A) Game Plan for ARIMA, SARIMA and SARIMAX**

**B) CASE STUDY BRIEFING: Predicting Daily Revenues**

**C) Python - Setting Up ARIMA**

**D) ARIMA**

**E) Auto-Regressive**

**F) Integrated**

**G) Python - Stationarity**

**H) Moving Average**

**I) Python - ARIMA**

**J) ARIMA in Action**

**K) SARIMA**

**L) Python - SARIMA**

**M) SARIMA in Action**

**N) SARIMAX**

**O) Python - SARIMAX**

**P) SARIMAX in Action**

**Q) Cross-Validation for Time Series**

**R) Python - Cross-Validation**

**S) Parameter Tuning**

**T) Python - Setting the Parameters**

**U) Python - Parameter Tuning**

**V) Python - Parameter Tuning Results**

**W) Q&A Highlight: Handling Future Data in Forecasting**

**X) Python - Predicting The Future Set Up**

**Y) Python - Predicting The Future**

**Z) SARIMAX Pros and Cons**

**IX) Section 9: PART 2: MODERN TIME SERIES FORECASTING**

**A) Modern Time Series Forecasting Overview**

**X) Section 10: (Facebook) Prophet**

**A) Game Plan for Facebook Prophet**

**B) Structural Time Series and Prophet**

**C) CASE STUDY BRIEFING: Bike Sharing**

**D) Python - Directory and Libraries**

**E) Python - Preparing Data**

**F) Python - Exploratory Data Analysis**

**G) Dynamic Holidays**

**H) Python - Holidays**

**I) Prophet Model Parameters**

**J) Python - Prophet Model**

**K) Python - Regressor Coefficients with ChatGPT**

**L) Python - Cross-Validation**

**M) Python - Performance Metrics**

**N) Python - Fixing 2012-10-29 with ChatGPT**

**O) Python - Feature Engineering**

**P) Python - Parameter Tuning Set Up**

**Q) Python - Parameter Tuning**

**R) Python - Parameter Tuning Outcome**

**S) Python - Predicting The Future Set Up**

**T) Python - Tuned Prophet Model**

**U) Python - Forecasting**

**V) Python - Prophet Data Visualization with ChatGPT**

**W) Prophet Pros and Cons**

**XI) Section 11: Capstone Project: Prophet**

**A) Project Introduction**

**B) Python - Challenge Solutions Part 1**

**C) Python - Challenge Solutions Part 2**

**D) Python - Challenge Solutions Part 3**

**XII) Section 12: Intermittent Time Series**

**A) Game Plan for Intermittent Forecasting**

**B) Python - Intermittent Time Series Setup**

**C) Python - Data Prep**

**D) Python - Feature Engineering**

**E) Python - Time Series EDA**

**F) Python - Darts Data Preparation**

**G) Python - AutoArima**

**H) AIC and BIC**

**I) Python - Cross-Validation**

**J) Python - Cross-Validation Results and the Zero issue**

**K) Python - Plotting Cross Validation**

**L) Predict the Future + Challenge**

**XIII) Section 13: Mid-Course Feedback**

**A) Will you help me?**

**XIV) Section 14: PART 3 - DEEP LEARNING FOR TIME SERIES FORECASTING**

**A) Deep Learning for Time Series Forecasting Overview**

**XV) Section 15: RNN - LSTM**

**A) Game Plan for LSTM**

**B) Simple Neural Network**

**C) Recurrent Neural Networks (RNN)**

**D) LSTM**

**E) Python - LSTM Setup**

**F) Python - Time Variables**

**G) Python - Scaling**

**H) LSTM parameters**

**I) Activation Functions**

**J) Python - LSTM Model**

**K) Python - Cross-Validation**

**L) Python - Cross-Validation Performance**

**M) Python - Parameter Grid**

**N) Python - Parameter Tuning Round 1**

**O) Python - Parameter Tuning Round 1 Part 2**

**P) Python - Parameter Tuning Round 2**

**Q) Python - Parameter Tuning Final Results**

**R) Python - Tuned LSTM Model and Predicting the Future**

**S) LSTM Pros and Cons**

**XVI) Section 16: LSTM - Multiple Time Series Forecasting**

**A) Multiple Time Series Forecasting**

**B) M4 Data set**

**C) Python - Preparing Script and Loading Data**

**D) Python - Preprocessing Time**

**E) Python - Visualization + Hourly Seasonality**

**F) Python - Daily Seasonality**

**G) Python - Time Covariates, Scaling and Time Steps**

**H) Python - LSTM Model and Cross-Validation with Multiple Series**

**I) Python - CV Results**

**J) Python - CV Visualization**

**K) Python - Parameter Tuning**

**L) Python - LSTM Parameter Tuning Results**

**M) Python - Predicting the future**

**N) Python - LSTM Debugging**

**XVII) Section 17: Temporal Fusion Transformers (TFT)**

**A) Game Plan for TFT**

**B) Introduction to Temporal Fusion Transformers**

**C) Case Study Briefing - Electricity Pricing**

**D) Python - TFT Starter File**

**E) TFT Model Architecture**

**F) Covariates: Past, Future and Static**

**G) Python - Series, Time and Static Covariates**

**H) Python - Past Covariates**

**I) Python - Future Covariates**

**J) Python - Scaling**

**K) TFT Model Parameters**

**L) Python - TFT Model**

**M) Python - Cross-Validation**

**N) Python - Cross-Validation Results**

**O) Python - Visualizing the Cross-Validation Results**

**P) Random vs Complete Parameter Tuning**

**Q) Python - Parameter Grid**

**R) Python - Parameter Tuning**

**S) Python - Parameter Tuning Results and Storing**

**T) Python - Tuned TFT Model**

**U) Python - Interpretability**

**V) TFT - Pros and Cons**

**W) TFT Key Takeaways**

**XVIII) Section 18: CAPSTONE PROJECT: Multiple Series with TFT**

**A) Project Presentation: Multiple Series with TFT**

**B) Python Solutions - TFT Model**

**C) Python Solutions - Cross-Validation**

**D) Python Solutions - Parameter Tuning**

**E) Python Solutions - Predict The Future**



# **I) Time Series Analysis and Forecasting with Python**

# **A) Time Series Analysis and Forecasting with Python**

You ever feel like you're the keto vanilla ice cream of the business world?
Boring and disappointing that you're still doing forecasting the old way like a rolled ice cream when everyone now talks about a -- pistachio flavor?

Well, not anymore.

In this course, you'll be at the forefront of time series forecasting from time series analysis and diving deep into advanced forecasting models.
I'll guide you through every step.

You will cover everything from simple exponential smoothing to cutting edge models like LinkedIn, Silver Kite, Prophet, and Amazon Cronos.

Think you're just a small fish in a big pond?

Let's change that with hands on exercise, real world challenges, and capstone projects.
You won't just learn, you'll do.

Revenues.
Electricity.
People.
Temperatures.

If it moves, we predict.

By the end you'll be forecasting trends with the precision of a sniper, turning your yearly revenue into your monthly income.

And who's going to lead you through this journey?

Well.

Hi.

I'm Diogo, your guide, mentor, and now ice cream coach with years of experience in data science and a passion for turning complex concepts into actionable insights.

I practice what I preach.

Jump in and let's make this year your breakthrough.

This is your moment.

Make it count.

Enroll now and let's crush it together.

# **B) Course Introduction**

I'm very happy that you picked this course to learn time series analysis.
It's the fifth year that this course has been live, and I truly think it's the most complete out there.

And there's absolutely no video that were in the, let’s say, first two editions that are currently live now.
So it’s currently always, always updating.

What I want to show you throughout this video is every single change since the 2025 version to now, the 2026 version.
This walkthrough will help you understand exactly what has improved and why it matters.

The key changes start with much crisper videos.
I really try to make sure that all videos are shorter and more focused.

We also now have deeper explanations and better explanations overall.
The goal is not just speed, but clarity.

Coding is now done with AI in, I think, most of the videos by now.
This allows us to spend more time looking at documentation and understanding what is what and why we are doing it, without the hassle of writing all the code manually.

Of course, we are very critical about it.
In the end, the code used is the one that I like, not necessarily the one that, in our case, Jamie and I are told to use.

We also add new projects and topics based on popular demand.
So the bottom line is, if you want something, tell me.

If there are a few people that want the same thing, I’ll make it happen.
And that’s a promise.

I also introduced an AI assistant.
You now have a time series analysis assistant that you can take anywhere with you and use in your job or during interviews.

This is a print screen of the assistant.
All the materials of the course are there—honestly, everything is there.

It’s just about asking the right questions.
And the assistant responds, I think, in my tone.

So if you like the way I talk or the way I use words, you’re going to like this as well.

Now, let’s look at what the course was in 2025.
At that time, we had five parts.

These were time series analysis, modern time series models and forecasting, deep learning for time series, and advanced content for time series.
We also had a Python appendix.

The Python appendix was for those who were not very familiar with Python but still wanted to learn.
There was a crash course at the end.

These were all the sections that were available back then.
I don’t want to spend too much time listing them, but they’re all there.

The question then becomes: what did we change?
This is where the remakes come in.

One important highlight is a new part called the Time Series Graveyard.
These are techniques that existed before but have not been updated by the teams maintaining them.

That’s why I’m calling it the graveyard.
This section also exists because of your requests.

In the past, I removed some sections.
Some people later asked if they could still access them.

So now, instead of removing them completely, they live in the Time Series Graveyard.
I wouldn’t advise you to focus on them, but it’s your choice.

This section includes everything that was either remade or added.
Anything marked as new represents a true addition.

We added new practice exercises for time series analysis.
We also added new intermittent forecasting and classification for time series.

For sections that are not marked as new, their Python tutorials were fully remade.
This was necessary because new changes in the libraries required updates.

As a result, the course is now 100% up to date.
Nothing is outdated or obsolete.

This is how the 2026 version of the course is now organized.
It consists of six parts.

These are time series analysis, modern time series, deep learning, advanced content, the graveyard, and the Python appendix.

The course is also now split between sections and projects or exercises.
Depending on your goals, you can choose how deep you want to go.

If you want structured learning, you can focus on the sections.
If you want to build a portfolio, you’ll find plenty of projects and exercises.

One important thing to note is that the course is actually shorter than it was one year ago.
This is despite having a lot of new content.

The reason is simple: the videos are crisper and better edited.
Using AI for coding also makes learning much faster.

This creates a smoother and more efficient learning experience.
That’s something I’m genuinely excited about.

The 2026 version is live.
So let’s get started.

# **C) Overview of the AI Time Series Assistant**

In the previous lecture, I gave you a link to the AI time series assistant, and in this video I want to give you a brief introduction to it. I’m going to call this version one of the assistant. I focused a lot on getting the core functionalities and the architecture right so that I can build on it later. There are quite a few things that could still be improved, and that’s exactly what I’m looking for when you share your feedback.

In this video, I want to introduce what it can do, what it cannot do, and the other actions available. It’s always going to be a time series course assistant, and it has a collapsible bar where you can see its capabilities, the core topics it covers, and also what it cannot do. For example, it cannot give medical advice, it cannot run code, and it cannot browse the web. You’ll also see a “ready to predict the future” message and some initial information explaining what it can and cannot do.

Let’s say we enter a message, for instance, “explain time series analysis,” and then click on send. You’ll notice that it shows when it’s thinking, both in the chat itself and at the top, where it indicates that it’s running. This assistant is going to complement the AI assistant from Udemy. There are a few things they have in common, especially when it comes to the videos themselves, because both have access to the transcripts. In fact, everything from the Udemy AI assistant comes from those transcripts, so if you want something specific to a given lecture, that’s where it’s useful.

For example, if you want a summary of a lecture, the Udemy assistant is very good because everything is directly linked to your current video. However, this time series assistant is not linked to your current video experience, so that’s one area where Udemy’s assistant is better. On the other hand, this assistant has some clear advantages. First, it’s always going to be up to date. For instance, we’re currently using GPT-5 already, whereas the Udemy assistant, based on things like its use of words such as “delve,” still feels very much like GPT-4.

At the same time, this assistant has all the course material loaded into it. That includes all the Python code, all the PDFs, and everything else from the course. So if you want something really tailored to the course—especially things you want to use, reuse, and take into your professional life—this is the one to use. It’s also not really tied to Udemy, which means you can keep it open and ask questions while you’re working on your own projects.

This tool is meant for you to use and reuse. Of course, there are many things that can still be improved. There’s an option to give feedback, and while I’d obviously love it if you love it, what I really care about is that you share what you like and what you would improve. This could be things like the colors, the layout, or any other aspect. Again, this is version one.

If you’ve been with the course for a long time—especially since the course originally dates back to 2021—you know that I’m always improving it. There will be a version two, version three, and so on, because I’m very focused on making this the best course possible.

While you’re here, you can also ask things like, “Give me the code for exponential smoothing,” press control-enter, and the assistant will think and respond. Alongside this, I want to emphasize that the feedback section is really important. Sometimes people ask for personal help through the feedback, but I can’t help you that way. If you need my help directly, you should always use the Q&A section. I answer questions there almost every day—there’s maybe one day a month when I don’t check it.

The Q&A is always the best way to reach out to me. I do gather feedback from this tool, but I don’t check feedback every day. I usually review it weekly or monthly, make sure everything is working, and then act on it after a couple of months to improve the tool. One example of feedback is that the Udemy assistant allows you to add images, which is really cool, whereas this one currently cannot. That’s something to keep in mind for future improvements.

You’ll also find all the code used in the course here. This is the same code we use throughout the course, and it should always be up to date because it reflects the latest course materials. If you ever find any issues, please let me know, because sometimes it can produce hallucinations, which I can’t fully control.

If at any point you don’t want to continue a conversation and you want to start fresh, you can reset the conversation. That will start a new session. Everything here is session-based only—there’s no database in the backend, and nothing is stored.

If this is something you love and use a lot, there’s an option to donate. This is completely optional, but I want to be transparent that this tool does cost money to run. The cost for one to ten people is basically nothing, but when you get to thousands of users, it becomes more significant. So if it genuinely makes a difference in your life, you can support it, but again, it’s absolutely optional.

At the end of the day, this is yours. Even if you hate me, you can bookmark it. There’s no login, and it’s an open tool that you can use and reuse. It’s here for you to keep. I personally find it very cool when people use my work, so if you do use it, let me know.

# **D) Diogo's Introduction and Background**

Hi there, I’m Diogo, and I’m thrilled to welcome you to this course. Before we get started, let me share a bit about myself so you know who’s guiding you through this journey.

I hold a Master of Science in Management from ESMT Berlin, one of the top business schools in Europe, where I specialized in business and analytics. My professional career has been centered around using data to solve complex business challenges. I’ve worked on projects ranging from sales planning involving billions of euros to A/B tests where companies had to invest hundreds of thousands of euros. I’ve truly been around the block and have had a lot of hands-on experience.

Beyond teaching, I’m also a startup founder. My company, which you can find at Join Betacom, aims to leverage the power of data to help restaurants around the world. We analyze vast amounts of data to provide actionable insights, such as optimizing menus or determining the best pricing strategies for their products. And if you’re interested in my startup, feel free to reach out and drop me a line.

I’m here not just to teach, but to genuinely support your learning journey. If at any point you feel that this course isn’t the right fit for you, don’t hesitate to reach out. I’m more than happy to guide you toward a learning path that suits you best, even if that means recommending other resources or courses that better align with your learning style.

Before we wrap up, I’d like to extend a personal invitation for you to connect with me on LinkedIn. Simply search for Diogo Alves de Rezende, and let’s keep the conversation going. I’m always eager to engage with my students and grow our professional networks together.

Once you complete this course and earn your certificate, I encourage you to share it on LinkedIn. I make it a point to repost these achievements and celebrate your success, just as I’ve done for many of my students. It’s a great way to gain visibility and start building your professional reputation in the business and analytics community.

My goal is for you to succeed and feel empowered to make a real impact. Whether you’re here to advance your career, pivot into a new industry, or innovate within your own business or company, I’m excited to help you achieve those dreams.

# **E) Unlimited Updates and Enhancements 2026**

Hey, before we start, let me talk about the current status of this course and what’s coming next.

All the content you’re about to dive into has been pre-checked and updated to ensure it’s relevant and accurate for 2026. I know how fast technology evolves, and that’s why I’m committed to keeping this course up to date with the latest developments and insights, so you can stay ahead of the curve.

That said, in a world that’s constantly changing, there’s always a chance that something might slip through the cracks. If you notice any outdated information or anything that doesn’t feel quite right, please don’t hesitate to let me know. Your input is invaluable in maintaining the quality and relevance of this course.

In addition, if there’s any specific content or material that you’d like to see added, I’d love to hear from you. Your suggestions help shape the future of this course.

In the next lecture, I’ll share a form where you can report anything that’s incorrect, offer suggestions, or request additional resources. This course is a collaborative journey, and your feedback plays a key role in making it the best it can be.

Thank you for being an active part of this learning community. I’ll see you in the next video.

# **II) Section 2: Part 1 - Time Series Analysis**

# **A) Time Series Analysis Overview**

Welcome to the time series analysis part of the course. This is where you take your data skills to the next level, learning how to make your data work for you and predict future trends with accuracy. Let’s get started.

Have you ever wondered how companies like Amazon and Netflix always seem to know what’s coming next? It all comes down to mastering time series forecasting. By the end of this section, you’ll be applying the same techniques to your own business and career, turning raw data into accurate, actionable predictions.

We’ll begin by understanding what time series data is and why it plays such a critical role in forecasting. You’ll work with real-world data in Python and set up all the essential tools needed for time series analysis. This foundation will allow you to build robust models and generate precise predictions.

Before forecasting effectively, it’s essential to know your data inside out. You’ll perform exploratory data analysis to uncover hidden patterns and structures in your time series. Through visualization and data manipulation techniques, you’ll learn how to interpret your data and prepare it properly for forecasting.

You may notice that certain trends repeat over time—this is known as seasonality. You’ll learn how to decompose your data into seasonal, trend, and residual components. This breakdown will help you model your data more accurately and significantly improve your predictions.

Understanding how past data points influence future values is another key concept. You’ll explore autocorrelation and partial autocorrelation to measure these relationships. This knowledge is essential for selecting, tuning, and validating your forecasting models.

Next, we’ll dive into a range of forecasting techniques. You’ll start with simple exponential smoothing, which works well for short-term predictions. From there, you’ll move on to the Holt-Winters method to handle seasonality. You’ll also work with more advanced models such as ARIMA and SARIMAX to capture complex patterns in time series data.

Imagine being able to predict stock prices or customer demand with confidence. You’ll develop these skills through hands-on exercises and real-world challenges. This section is not just about theory—it’s about applying these techniques to real data and understanding their results.

You’ll also explore what happens when forecasting goes wrong. By studying real case studies where predictions missed the mark, you’ll learn to recognize common pitfalls and improve your forecasting approach.

Finally, you’ll bring everything together in a capstone project focused on forecasting airline miles. You’ll prepare the data, build forecasting models, and evaluate their performance. By the end of this project, you’ll have a strong, practical example of your forecasting skills to showcase professionally.

You’re not just learning how to forecast—you’re mastering a powerful skill that can transform your career. See you in the next video.

# **III) Section 3: Python for Time Series Analysis**

# **A) Game Plan for Python for Time Series Analysis**

In this section, you are going to use Python as a tool to understand time series. This is not really an academic exercise. We’re going to work with real data, real questions, and we’re going to aim for real decisions.

We’re going to move fast, stay focused, and build intuition step by step. The goal is to truly understand time series, not just in theory, but in practice.

I can tell you already that you don’t need to be a master of Python. You only need enough to slice data, spot patterns, test ideas, and eventually explain what’s actually going on in the business. That’s really the core objective here.

If you already feel comfortable with Python, that’s great—you’ll be sharpening your instincts. If, however, you’re rusty or starting fresh, don’t worry. There’s a full Python crash course at the end of this course where you can practice more or even start from zero at your own pace.

# **B) **Load and Explore Data****

You are going to find this inside Python for Time Series, which itself is inside Time Series Analysis, and then Python for Time Series Analysis. There, you will find a file called lab starter one.

And here we go.

So we are here. I need to reconnect. This is basically taking our script and saying, “Hey, this is live.” Then I would mount the drive here. Of course, if you’re not using Google Colab, you don’t need to do this step. This is just for those who are using it. Whichever workspace you’re using, just go for it.

In case you’re using Colab, let me show you. You go to Drive → My Drive. This is so that you get the path. Inside My Drive, you’ll find Python Time Series Forecasting. Then inside Time Series Analysis, you will find Python for Time Series Analysis. I will copy the path here. Here we go. Control. Of course, nothing changes, and here we go.

So this is the part where I do this with you. Of course, it’s very specific for Colab. And now we can go.

Task number one is to import the pandas library. All right, let’s do this. Import the pandas library. It’s very simple:
import pandas as pd.

So pandas is the library, and pd is the short form that we are going to call. Cool. Shift + Enter. Task number one is done.

Then task number two is to load the train.csv file into a DataFrame called df, and preview the first rows. You’re going to find the dataset in your materials. It’s a very cool dataset. It has a lot of stores, the day of the week, and it’s a very rich dataset for us to explore and practice some Python.

And here we go. DataFrame. So I’m going to call it dataframe = pandas.read_csv("train.csv"). There was an extra dot here, so let me remove it.

Then I do dataframe.head(). In case you’re using the Jupyter functions of Colab or whichever it is, the comments are going to give quite a few things away. I would always encourage you to double-check and make sure that whatever you’re doing makes sense. In the end, or in case you can, just try to do it on your own. That’s always the best way to practice.

Of course, when you’re actually doing the work, go for it. Go fast. As for practicing, doing it yourself is a tiny bit better.

Now we look at the dataset. We have store, day of week, date, sales, customers, whether it was open, whether there was a promo, whether we have a state holiday, and a school holiday.

The dataset is from a very well-known store, Rossmann. It’s all in German, but it’s basically something that sells things like cleaning products, beauty products, and so on. And of course, at least in Germany, there’s something very specific: some stores are closed on Sunday. That’s something we need to consider.

But if we look at it here, let’s look at something. We see a dtype warning for column seven. Remember, Python indexing starts at zero, so we have columns 0, 1, 2, 3, 4, 5, 6, 7. So the state holiday column has something weird going on.

We can specify the dtype option on import or set low_memory=False. This is clearly something we need to investigate. So I’d say the first step is to look at information about the DataFrame.

To do that, we use dataframe.info(). This provides some information. We can see that, for instance, state_holiday is currently an object. And when we look at it, it has zeros, but it’s an object type, which means it has characters or letters inside. That’s something we definitely need to investigate.

This leads us to task number four. We’re going to print the unique values in state_holiday and their counts.

The way we do this is dataframe["state_holiday"].value_counts(). And we should get something. If you’re not using a notebook style environment, you’d have to use print. I personally have a big preference for notebook styles because of the input-output flow—it’s much easier to explore data this way.

So let’s remove the print because this looks way nicer. We can see that we have a zero, and then we have a different type of zero, and then we have A, B, and C. There is something odd here.

I’m going to assume that A, B, and C are types of holidays, but these two different zeros suggest that something went wrong during data entry. That’s how I’d interpret this right now.

This leads us to task number five. We are going to binarize this set. Instead of having zero, zero, A, B, and C, we’re going to have just zero and one. The A, B, and C values will become ones, and both types of zero will become zeros.

Let’s do this. I’ve found that the simplest way is to use the following format. We take dataframe["state_holiday"] and use .isin(["A", "B", "C"]). Then we convert this to integers using .astype(int). This makes sure that whatever matches A, B, or C becomes a one.

I find this approach intuitive because we’re simply saying whether the value is in A, B, or C. The astype part may not be intuitive at first, but it’s a very simple way to convert booleans into zeros and ones.

Cool. Then we run this and replace the column in the same variable. There was one extra quote there, so we remove it.

Now let’s check again by running value_counts() on state_holiday, just like before. Control + Enter, and here we go. We now only have zeros and ones.

# **C) Subsetting Stores and Basic Aggregations**

When we look at the data, we can see that we have stores, and because there are a lot of entries, it’s not immediately clear how many stores we actually have. It’s usually very helpful to understand the structure of the data you’re analyzing.

So we’re going to count this. Let’s make it nice and readable by printing it using an f-string. We print something like: “There are” and then the length of dataframe.store.unique(), followed by “stores”.

When we run this, we see that there are 1,115 stores. That’s quite a lot for the sake of our analysis.

As we start exploring the data or even doing data visualization, visualizing 1,115 stores is simply not reasonable. So there are two things we’re going to do. First, we’re going to randomize a subset to see how things work. Second, we’re going to simplify our analysis from here on out.

What we’re going to do is subset the data to ten random stores, using a random seed of 1502, and then store the result. Let’s do this.

We start with the original DataFrame and sample from the store column. Step by step, we first sample ten stores. When we run this, you’ll notice that every time we run it, the results are the same. That’s because we set a random state.

The random state makes sure that the results I get and the results you get are the same. It doesn’t matter how many times we run it. If you change the random state, the stores will change, but as long as we keep it at 1502, the output remains consistent.

So now that we have these ten stores, we take the DataFrame and filter it. We go to the store column and use isin() to check whether each row belongs to one of these selected stores. When we run this, we get a series of booleans—True and False.

The next step is to use this boolean mask to retrieve only the rows where the value is True. This tells the DataFrame to keep only the rows corresponding to those ten stores. When we do this, we can see stores like 24, 47, and others appearing in the filtered data.

At this point, we want to store this result. So we save it as dataframe_ten. We can quickly preview it to confirm everything looks correct.

One important best practice here is to use .copy(). This ensures that we’re working with a separate copy of the data and that any modifications we make won’t unintentionally affect the original DataFrame. This is just good hygiene when working with pandas.

Now that dataframe_ten is properly set up, we can move on.

Task number three is to compute the mean sales for each of the ten stores and sort them in descending order. Let’s do this.

We take dataframe_ten, group it by store, then select the sales column and compute the mean. Once we have the result, we sort the values in descending order.

When we look at the output, it’s not immediately obvious which store is the biggest just by scanning the numbers, so sorting helps. After sorting, we can clearly see that store number 4 has the highest average sales, while store 794 has the lowest.

There’s quite a big difference here—roughly a threefold difference in average sales. And remember, this is just within our subset of ten stores.

Now, one last thing that I find particularly interesting is the following: for each store, we want to find the row corresponding to the day with the all-time highest sales.

In other words, we want to know which specific day each store achieved its maximum sales.

To do this, we take dataframe_ten, group by store, and focus on the sales column. Instead of computing the mean, we now use idxmax(), which gives us the index of the row where sales were highest for each store. We could also look at idxmin() if we wanted the worst day.

Once we have this, we can optionally sort the results in descending order to see the biggest peaks first.

When we inspect the output, we see something remarkable. For example, store 864, which averages around 3.5K in sales, had one day where sales were close to one million. That’s roughly 300 times its average daily sales, which is absolutely insane.

# **D) Working with Datetime Index and Weekday Patterns**

Task number one is to set the date column as the index, and then preview the first rows.

To give a bit of insight here, when we work with time series data, we care about time. Setting time as the index enables us to manipulate the data in a much easier and more natural way. This is especially important for visualization, because Python then understands that we are dealing with data evolving over time. Without this, Python doesn’t really know that the data is temporal, and I’ll show you that in a moment.

For now, we set the date column as the index. The way we do this is by using dataframe_ten.set_index("date", inplace=True).

Let’s preview a few rows to see what we get. We can see that the date is now shown in a consistent format. That looks good.

But now let’s look at what Python actually thinks about our date column. If we go back and inspect the information, we see that the date is still an object. An object means characters or strings, not a real date.

So we inspect dataframe_ten.index. When we look at it, we see that it has a start, but it’s still considered an object. The name is “date”, but that name itself is meaningless—it’s just a label.

What we need to do is convert this into a proper datetime object. To do that, we use pandas and apply to_datetime on the index. Once we run this, Python now understands that this index represents dates.

At this point, Python knows that we’re working with time, and this allows us to properly explore and manipulate the data from a time series perspective.

Cool.

Now let’s look at something interesting. Even though we’re not heavily using the index yet, a useful exercise is to compute the average sales per day and per store.

Let’s do this.

We take dataframe_ten and use groupby. At this point, you can probably already see how relevant grouping is. We group by day of week and by store. Then we select the sales column and compute the mean.

Here we go.

Now, I know—I know—it’s not very easy to interpret this output just by looking at the table. That’s fine. That’s going to be addressed in task number four.

Before moving on, I want to highlight something important. Instead of using the existing day_of_week column, we could have used dataframe_ten.index.dayofweek, and the result would be exactly the same. In our case, the day-of-week variable already exists, which makes things easier.

But when you work with time series data, you can create all sorts of time-based variables directly from the index—day of week, month, year, and so on. This makes feature engineering very powerful and very simple once your index is properly set.

All right.

So now we move on to visualization.

We want to visualize the average sales per weekday and per store using a line plot.

The simplest way to do this is to reuse the code we already have. I’m a big fan of reusing code whenever possible.

The first thing we need to do is unstack the grouped result. Once we unstack, we can already see that this becomes easier to compare across weekdays—0, 1, 2, 3, and so on. Day 6 corresponds to Sunday.

Since we’re dealing with Germany, this makes sense because many stores are closed on Sundays. Still, even with this structure, it’s not very easy to visualize just by looking at numbers.

So the next step is to call .plot().

When we do that, the visualization becomes much clearer. Now we can actually see the patterns.

Looking at the plot, one thing immediately stands out. All stores have zero sales on Sunday except for one. This is something interesting and consistent with what we saw in the table.

However, there’s another issue. The magnitude of sales differs a lot between stores. Some stores reach sales around 10,000, while others are much lower. This makes comparisons tricky, because the scale dominates the visualization.

And this is where the exercise really starts.

I want you to start thinking about this: when we want to understand how sales evolve during the week, we’re essentially looking at a seasonal pattern. To truly compare these patterns across stores, we should also start thinking about standardizing the data.

# **E) Standardizing Sales and Comparing Weekday Patters**

In order to properly compare behavior across different stores, we need to standardize the data. Standardization ensures that there is a common denominator among the stores, so that we are truly comparing apples with apples.

The standardization formula itself is very straightforward. It works as follows: we take the value, subtract the average, and divide by the standard deviation. This is the classic z-score formulation.

So this is step one. The main challenges here are twofold. First, we need to build a function. This is a very standard Python task and a great way to practice writing functions. Second, we need to apply this function correctly to our data, making sure that we zoom in on each store individually. In other words, we want to compute the mean and standard deviation per store and standardize sales within each store.

This brings us to task number one.

We need to define a function that standardizes a numeric series using z-scores, meaning value minus mean divided by standard deviation. Let’s do this.

We start by defining a function called standardize, and we pass a series as the input. Inside the function, we return the series minus the series mean, divided by the series standard deviation. This is very straightforward.

Now that the function is defined, we need to apply it to our data.

What we need to do is take our DataFrame and group it by store. This ensures that we are focusing on one store at a time. Then we focus on the KPI we care about, which is sales.

When we do this, we group by store, select sales, and apply a transformation using our standardize function. At first, it may show as a generic grouped series because we haven’t applied the transformation yet. But once we apply transform(standardize), the values are standardized per store.

After running this, we can see that the values are now standardized. This allows us to really work with and compare patterns across stores much more easily.

Recall that standardized values are centered around zero, and most values typically lie between -2 and 2. This makes interpretation and comparison much more intuitive.

This leads us to task number three. But before we do that, we need to store the standardized values.

We assign the standardized sales back into our DataFrame using .loc across all rows, and we create a new column called sales_standardized. The name “STD” here refers both to standard deviation and to the normalization itself.

Once we run this, the new column is created. We can quickly preview a few rows using .head() to confirm that everything looks correct.

Now we move on to computing the mean of the standardized sales.

We take dataframe_ten and group by day of week and store. Then we select the sales_standardized column and compute the mean. This gives us the average standardized sales per weekday and per store.

This naturally leads us to task number four.

We take the result, unstack it, and then plot it. This allows us to visualize the standardized weekday patterns much more clearly.

However, there are a few issues here. Nine out of ten stores have no sales on day seven, which is Sunday. This completely skews the visualization. From an analysis perspective, this is not ideal because zeros don’t really carry meaningful information in this context.

One thing that clearly stands out is the brown series, store 353. It shows extremely strong seasonality on Sundays. That makes sense, because if most other stores are closed, this store benefits disproportionately.

This is something we can infer from the data.

If we look at the other series, we see a different pattern. Sales tend to decrease day by day until Saturday, showing a clear weekday seasonality. That’s one conclusion we can draw.

At the same time, Sunday completely distorts the picture and makes it difficult to analyze the remaining patterns properly. We clearly need to clean this up further.

Still, let’s continue exploring.

Let’s take a closer look at Sundays specifically. Are stores always closed? Is there even a single Sunday where some of them are open? The same question applies to the other stores as well.

These are things we can visualize, explore, and then use to draw our own conclusions.

# **F) Analyzing Sales on a Specific Weekday**

I have to say, I only listed one task here, but this could have been done in multiple ways. What we need to do is filter the data for day seven and then visualize it. So this will be step number one and step number two at the same time.

First, let’s import what we need for visualization. We import matplotlib.pyplot as plt. That’s step number one.

Next, we create a new DataFrame called dataframe_d7. We start from dataframe_ten. There are many ways to do this. One way is to filter where the day of the week is seven and then make a copy. That’s one valid approach.

Another option would have been to use the index directly, since we’ve already set it to datetime. That’s another possibility. In this case, we’ll stick with using the existing variable and filtering based on the day of the week.

Now we move on to plotting.

We start by creating a figure using plt.figure(). We set the figure size to 12 by 6, which is a fairly standard size.

Next, we loop through the data. For each store and its corresponding data in dataframe_d7 grouped by store, we plot a line. We plot the date, which is stored in the DataFrame index, on the x-axis, and sales on the y-axis. For each line, we set the label using an f-string so that it shows the store number.

This loop allows us to plot each store’s Sunday sales on the same chart.

We close the parentheses and then display the plot. This is already enough to generate the visualization. It’s also a nice introduction to matplotlib if you’ve never used it before, because it shows the basic structure of creating a figure, plotting data, and labeling it.

Now, here’s a cool thing that you might or might not have noticed. Let’s quickly look at dataframe_d7 itself. You’ll see that the earliest dates are from 2015, and the later dates are from 2013. In other words, the data is reversed.

However, because Python knows that we’re dealing with dates, this doesn’t matter at all. When we plot the data, everything is automatically ordered correctly along the time axis.

That was just a quick side note.

Now let’s clean up the plot a bit. We add a title, “Sales on Sunday”. We label the axes with date and sales. We also add a legend so that we can identify each store.

I also like to add plt.tight_layout(). This usually tidies things up a bit and makes the plot easier to read, which I personally appreciate.

When we look at the plot, I feel it’s a bit too tall, so we reduce the figure height from 6 to 5. That small adjustment makes it look better.

Now, what do we see?

We see that every single store except one is always closed on Sundays. There are no exceptions at all. This tells us that by including Sundays in our analysis, we’re potentially introducing noise—especially when we standardize the data.

For store 353, there is one Sunday where the store was closed, which explains a drop. But aside from that, this store has recorded sales on every single Sunday, and those sales are growing over time. That’s definitely interesting to observe.

And that’s it.

This is how we do a quick visualization to explore a specific pattern in the data. I would say this exercise is on the more difficult side, because it includes a for-loop and multiple steps. But if you understand this, you’ll be able to handle virtually every visualization throughout this course.

# **G) Removing Outliers and Re-Assessing Intra-Week Seasonality**

Our conclusion so far is that we clearly have store 353, which behaves very differently from the others. There’s really no point in comparing it directly with the rest. At the same time, we also have day seven, which is Sunday, and this day is kind of “poisoning” our data because it introduces a lot of noise.

Because of this, task number one is to remove all observations for day of week seven and for store 353. Doing this will give us a much cleaner DataFrame, allowing us to continue exploring seasonality in a more meaningful way.

Cool, let’s do this.

We start from dataframe_ten. We want to keep only the rows where the day of week is not seven and the store is not 353. Since we have two conditions, we need to combine them.

First, we filter dataframe_ten where the day of week is not equal to seven. Then, we add a second condition where the store is not equal to 353. Finally, we make a copy of the result to avoid any unwanted side effects.

We store the result in a new DataFrame called dataframe_clean.

Once that’s done, we can take a quick look using .head(). This preview doesn’t show anything dramatic, because we’ve removed rows rather than added new ones. The main purpose here is simply to confirm that we still have data and that the operation worked as expected.

Now that we have a clean DataFrame, the next step is to re-standardize the sales column within each store, but this time using dataframe_clean.

We essentially repeat the same process as before. We take the sales column, group by store, and apply the standardization transformation. The result is stored again in the sales_standardized column.

If we preview a few rows using .head(), we can immediately see that the standardized values have changed. For example, values that were previously around 1.5 and 1.1 are now closer to 1.65 and 1.16. This makes sense, because we’ve changed the underlying data by removing Sundays and the outlier store.

This brings us to task number three.

We now take dataframe_clean and group it by day of week and store. Then we select the sales_standardized column and compute the mean. This gives us the average standardized sales per weekday and per store, using the cleaned data.

As mentioned earlier, we can store this result. Let’s call it intra_week.

Now we move on to task number four, which is visualization.

To visualize the intra-week seasonality, we take intra_week, unstack it, and then plot it. Compared to before, the view is much clearer. Earlier, everything almost looked like a straight line. Now we can actually see meaningful differences and patterns.

We can also customize the plot directly within pandas. For example, we can set the figure size to 12 by 6, add a title such as “Intra-week Seasonality of Standardized Sales”, and adjust the layout. If we want to remove or fine-tune certain axes, we can do that as well.

Reducing the figure height slightly—from 6 to 5—makes the visualization a bit cleaner and easier to read.

Now let’s interpret what we see.

There’s one store, store 267, that shows very strong Saturday performance, which is quite unique. In general, Thursday to Saturday seem to be the strongest days for most stores. From Monday to Tuesday to Wednesday, sales tend to decrease, and then there appears to be some stabilization.

Overall, Monday is often the strongest day, Saturday the weakest—except for that one store. We also see a general decrease until Wednesday, followed by a flatter pattern for the rest of the week.

One important takeaway here goes beyond just seasonality. This exercise demonstrates the process of time series analysis. We take a sample of data, analyze it, and identify problems. Then we ask questions like: “What’s causing this distortion?” In this case, Sunday was the issue, so we removed it.

Next, we notice a store that behaves completely differently. That store clearly belongs to a different category and should be analyzed separately. Ideally, we would later compare stores that are open on Sundays with those that are not, because they may not be directly comparable.

This is how we move step by step toward insight: analyze, detect anomalies, clean the data, reanalyze, and visualize the results. And ideally, at the end of this process, we present clear visualizations that support our conclusions.

# **H) Analyzing the Impact of Promotions**

One of the most interesting questions—especially in revenue planning or demand planning—is understanding the impact of promotions on revenue. This is something we are going to explore slowly here, just a little bit, from a more data-exploratory perspective.

Let’s start with the first task. We want to compare the average sales on promotion days versus non-promotion days. To do this, we use a function you’re probably already tired of by now: groupby. We group by the promo variable and then compute the mean. This immediately gives us something concrete to compare—average sales when promotions are active versus when they are not.

At this point, you might notice something odd happening, especially if you’re working in Colab. You may realize you’re doing something wrong, or Colab might not be cooperating properly. When that happens, it’s totally fine—and often necessary—to restart the runtime and rerun everything from scratch. That’s exactly what we do here.

After restarting and rerunning the pipeline, we see the results clearly. Average sales on promotion days are around 7991, while on non-promotion days they are around 4.4k. That’s already interesting. Here, I’m intentionally switching back to using the full data frame instead of one of the cleaned or filtered versions from before. For this particular analysis, working with the complete data frame feels more natural and helps illustrate a few additional concepts.

Now we move to task number two. Instead of looking at promotions globally, let’s break this down per store. We want to compare promotion versus non-promotion sales at the store level. To do this, we group by both store and promo, select sales, compute the mean, and then unstack the result. This gives us a table where, for each store, we can directly compare average sales with and without promotions.

We store this result in a new object called promo_uplift. This data structure now contains the mean sales per store, split by promotion status. We can inspect it using .head() or similar methods to confirm everything looks correct.

Task number three is where things get more interesting. For each store, we want to compute the relative uplift in sales during promotion days compared to non-promotion days. The idea is simple: we take the difference between promotional and non-promotional sales and divide it by the non-promotional baseline. In other words, (promo − non_promo) / non_promo.

This relative format is very useful because it puts all stores on a common scale, similar to standardization. Even if stores have very different absolute sales levels, relative uplift allows us to compare them fairly. We compute this relative difference and add it back to our promo_uplift structure. After that, we can again inspect the results to make sure everything looks reasonable.

At this stage, a very natural and insightful question arises: in which stores is promotion actually making a meaningful difference? To answer this, we look for the stores with the largest relative uplift. We take the computed uplift values and use nlargest(5) to identify the top five stores in terms of promotional elasticity.

This reveals some very interesting results. For example, stores like 198 and 607 stand out, and one store—store 108—shows an uplift of around 2.25x. That’s a very significant increase and clearly indicates that promotions are extremely effective in that store.

Task number five is simply the flip side of this analysis. If we can find the highest promotional elasticity, we should also look for the lowest. Using nsmallest(5), we identify the stores where promotions have little to no impact on sales. This immediately raises important business questions: what’s happening in those stores? Why don’t promotions work there?

Of course, there are many possible explanations. There could be other factors we’re not accounting for, such as how often promotions are run, when they are run, or how large they are. Promotions aren’t just a binary yes-or-no variable. They have magnitudes—depth of discount, breadth of products included, and timing over time. These are what we call confounding factors, and they can strongly influence the observed effect of promotions.

Overall, this exercise shows how we can start uncovering meaningful insights step by step using relatively simple exploratory techniques. If you’d like to see more exercises like this—especially ones that dig deeper into real business questions—just let me know, and I’d be very happy to create them for you.

# **IV) Section 4: Introduction to Time Series Forecasting**

# **A) Game Plan for Introduction to Time Series Forecasting**

Welcome to this video where I’ll make this game plan for our introduction to time series forecasting.

The thing is that we are going to talk about this new type of data. Well, not really new, but it could be new for you, which is time series data. Have you ever heard of it?

It’s all about looking at how things like stock prices or the weather change over time. And guess what? We are going to tackle it with Python. So don’t worry if you are new to this, I’ll guide you every step of the way, and I promise you it will be fun.

First off, let’s discuss what time series data is and what this really means. Imagine that you’re keeping track of your daily coffee spending. That’s time series data. Anything that records how things change day by day or month over month—that is time series data.

Now the cool part: we are going to use Python for all of this. We’ll start with the basics, and you’ll be amazed at how much you can do with just a few lines of code. I think you’ll be quite happy with how much we can achieve just in this section, which is an introduction.

We’ll get our hands dirty by sorting and playing with data. It’s really like putting a puzzle together, figuring out how to do it. We’ll look at patterns and trends. For instance, can we understand why Bitcoin’s price skyrocketed last week? Or why do sales dip every July?

You’ll learn how to spot these patterns, and of course, we’ll draw some graphs. Not just any graphs, but ones that really tell a story. You’ll learn how to turn numbers and dates into cool visual stories that anyone can understand.

Ever wonder if you can predict stuff like stock prices? We’ll talk about that too. It’s a bit like trying to guess the end of a movie, but with data and trends.

And to wrap it up, we’ll look at real-world examples where forecasting didn’t go as planned. It’s really about looking at someone else’s mistakes—and trust me, there’s really a lot to learn there.

# **B) What is Time Series Data?**

In this video, we are going to dive deep into time series data, and I’m also excited to introduce you to a particularly intriguing data set: the Bitcoin price data.

This data set will be the central focus of our tutorials. It tracks the daily price of Bitcoin spanning nearly a decade, from 2014 to 2023.

Now, why Bitcoin price data? Bitcoin, as a pioneer of cryptocurrencies, has a rich and dynamic data set. Its market is known for volatility, rapid price changes, and significant trends, making it an excellent subject for time series analysis.

By studying this data set, you’ll gain insights not just into Bitcoin’s price movements, but also into broader financial market dynamics and investment behavior.

Now let’s understand the essence of time series data. Time series data is unique, and I really want you to understand why. It’s like a chronological story, where each data point is a moment in time, neatly lined up from the oldest to the newest.

You’ll often find this data captured at consistent intervals—think daily, weekly, or monthly snapshots of data.

If we explore its wide-ranging applications, time series data isn’t just about finance. Imagine it being used in weather forecasting, where it helps predict rainfall or temperature trends. In economics, it’s crucial for analyzing GDP growth, and in healthcare, it’s used for monitoring patient heart rates over time.

Now let’s go deeper into the unique statistical tools used in time series analysis. Time series analysis introduces some fascinating statistical concepts. Together, we’ll look at autocorrelation—understanding how a data point is related to its past.

In fact, time series data is very unique because we use data from the past to predict the future. Moreover, we’ll discuss seasonality, identifying patterns that repeat over time. These concepts are key to accurate forecasting and trend analysis.

As we step into this journey through time series data, I encourage you to think about how this knowledge could enhance your own projects. What questions do you have? Do you see any practical applications for what you are learning?

Feel free to reach out in the Q&A or the student communities. I’m here to help. Until the next video—have fun!

# **C) Python - Libraries and Data**

Welcome to our first Python tutorial.

In this video, we are going to set up everything when it comes to our Python script. I have stored my materials here in my drive because we are going to use Google Colaboratory. You are welcome to use anything else, but I would strongly recommend that you do this with me.

That said, the materials are there for you, and of course, your IDE is your IDE. If you are new to Python, especially, I strongly encourage you to follow along, but I’m always here to help in case you have any issues.

If you decide to pick something else, you’ll find in the materials that there are several folders. We’re going to start with the time series analysis, and then with the introduction to time series analysis. We’re going to click on New, then More, and then Google Colaboratory—either create and share or just create.

If you have not shared it with anyone, it will just be “Create.” In case you’re having issues finding all the materials, they are in lecture number three or four of this course. You’ll be able to find everything there. There’s a link to my website, and on my website, there’s a big download button.

Now I’m going to change the title to Introduction to Time Series Analysis. You’ll notice that there are several things here. This is the table of contents, and we’ll build on this by creating headings and subheadings.

There’s also something here called Secrets, which we’re not going to use in this course. Then there’s Files—this is how we connect to Google Drive, and we’re going to use this extensively.

Let me mount the drive already. Connecting to Google Drive is called mounting Google Drive. You click on the drive icon, connect to Google Drive, and sometimes you’ll get a piece of code that you need to run.

If you’re wondering what that piece of code might be, it’s:

from google.colab import drive
drive.mount('/content/drive')

This is the code used to mount the drive. It doesn’t happen every time that you get this piece of code, which is why I’m sharing it—especially since this is the first time.

Once the drive is mounted, you’ll see something called drive. You click on drive, then go to My Drive. I have personally stored the folder on the homepage of the drive—the main part of the drive.

You’ll see Python Time Series Forecasting, then Time Series Analysis, and then Introduction to Time Series Analysis. You go to the three dots, copy the path, and then click back on code.

Now we’re going to set the directory. This is how we always connect to that specific directory, where in this case we have a couple of CSVs that are going to be our datasets. To do that, you use %cd to change the directory, and then you paste the path using Ctrl + V.

Here we go.

Let me also add a section here—this is going to be the Setup. To see this, we go to the table of contents, and you’ll notice that Setup now appears there.

Next, I definitely want us to import libraries and data. For importing libraries, we’ll import pandas as pd, numpy as np, and matplotlib. For now, this is going to be it. Each time we require a specific function or library, I’ll import those later. This way, we’re building everything step by step.

Then we’re going to load the dataset using pandas.read_csv. We’ll start with the Bitcoin price dataset. This is a good dataset for daily data—bitcoin_price.csv.

Then we use DataFrame.head(). This allows us to preview the first five rows of the data.

Here we go. We have seven KPIs. We have the date, starting from 2014, and then we have open, high, and low. Open is the price at the start of the day, high is the maximum, and low is the minimum.

Then we have the close, which is the value at the end of the day, and the adjusted close. The adjusted close accounts for things like dividends or splits. In this case, there’s nothing like that, but the adjusted close is the value we typically use because it’s the most relevant indicator of the value at the end of the day.

Finally, we have the volume, which is the trading volume for that specific day.

Now, what I want us to do is to close this video. In the next one, we’re going to return and explore something called the time series index. We’ll see how to do it, and that’s going to be it for now.

# **D) Python - Time Series Index**

In this video, we’re going to focus on the Time Series Index. This is a very important topic because when you’re dealing with the index—and you actually make the index out of dates—it makes your life much easier when it comes to time series analysis. Whether it’s plotting, forecasting, or the analysis itself, everything becomes quite straightforward.

Not all libraries that we are going to work with require this step, and I’ll show you which libraries do and which ones do not. However, for the sake of doing some pre-analysis, the main libraries that we work with—such as pandas, matplotlib, and later on, Statsmodels—use the date as the index. This is why it’s very important that you master this part of time series analysis.

What we’re going to do is convert the date column into a datetime object and then set the date as the index. These are two different steps, so let’s give it a go.

The first thing we need to focus on is the date format. In our case, the date is already in the correct format, which is year-year-year-year, month-month, day-day. This format is expressed using %Y-%m-%d. The capital Y means we are using four digits for the year, followed by the month and the day.

We can even see that tools like Google Gemini suggest using pandas.to_datetime. We set the column as date and specify the format. For our dataset, the date is already in the correct format, but it’s still a good practice to check.

Let me check this one. Okay, this one is fine as well. But in the next section, you’ll see that the date is not in the correct format. The bottom line is that you can always handle this by specifying the current format, and pandas will transform it into the correct one.

If we run DataFrame.head(), nothing is really going to change visually. The important step is setting the date as the index. To do that, we use set_index, specify the date column, and use inplace=True. Inplace means that the change is applied directly to the DataFrame, and the date column itself is removed.

Let me run this. Here we go—now the date is our index.

There are a few different ways to do this, and I’ll show you another one in a moment. But first, I want you to see what this allows us to do. For example, if we want to select a specific day from our DataFrame, we can use .loc and reference the index directly.

We specify a date, such as 2021-11-09, and when we run it, we get all the values for that specific row.

Now, another thing I want to show you is that we can also set the index at the time of importing the data. Let me call this DataFrame one. We again use pandas.read_csv with the same Bitcoin price CSV file. This time, we specify index_col='date' and parse_dates=True. This ensures that the date is parsed correctly and set as the index right away.

This approach gives us exactly the same result.

Before we close this video, I want to show you one last thing. You can always resample time series data to a different time granularity.

What does this mean? Imagine we have daily data, but daily values might be too noisy. Maybe weekly data would make more sense for our analysis. This is completely possible.

To do this, we take the DataFrame, use .resample(), specify 'W' for weekly, and then apply an aggregation such as .mean(). As you can see, this gives us weekly data.

You could also use the median, the maximum, or any other aggregation you can think of. If you can think of it, it probably exists—it’s just a matter of knowing what to apply.

Instead of weekly, you could also resample monthly, quarterly, or yearly. It’s really about deciding how to transform daily data into another time granularity.

This always depends on the business requirement. Do you care about daily data? If yes, then keep it. If weekly data is more relevant to your business problem, then go with weekly. It’s always about defining the business problem first, and then letting the analytics follow.

# **E) Python - Exploratory Data Analysis Part 1**

In this video, we’re going to talk about exploratory data analysis. Here we go.

It’s very important that we always, always understand the data that we are working with. In this case, we want to understand its cycles, its growth, and how it is developing. This is one of the fundamental things that really distinguishes a good analyst, scientist, or engineer from a mediocre one.

It all comes down to purpose. What are we doing? What is this for?

I’m going to cover a couple of things that are extremely simple, but also very common in time series analysis—one of them being the rolling average. This is particularly important when we start doing feature engineering, such as combining variables or averaging them. In this case, rolling averages create smooth curves, which are very useful for analysis.

Let’s do it.

Our first step is to generate a seven-day rolling average for the closing price. This is very straightforward. The way we do it is by taking the DataFrame, selecting the close column, and then applying the rolling method.

We specify a window of seven days, and then we decide how we want to aggregate—here, we use the mean. Once we do that, we get the rolling average. Of course, the first few values are missing because we don’t yet have a full seven-day window, but after that, the values appear.

What I think is particularly relevant is plotting two things together. Before doing that, I want to store this rolling average in a variable. I’ll call it the seven-day closing average. Now this value is stored and ready to use.

Next, I want to plot both the closing price and the seven-day rolling average together. Once we do that, we can clearly see the difference between the raw data and the smoothed curve.

Let me make sure we actually show the plot. Sometimes, especially in Jupyter environments, plots don’t display correctly unless we explicitly call plt.show(). This helps avoid issues with axis labels and rendering.

You can see quite a lot of data here, so let’s zoom in. A very easy way to do this is by using .loc and specifying a year, for example 2023.

Now we can clearly see what’s happening. The orange line represents the seven-day closing average. You’ll notice that it’s always slightly delayed compared to the actual closing price. This makes sense—it takes time for new values to influence the average.

That’s exactly how rolling averages work. You can shift values if you want, and I’ll show shifting later, but that’s not the nature of a rolling average. The more days you include in the window, the more delayed the curve becomes. Seven days is a relatively small window, so the delay is manageable, but it’s still there.

The next thing I want to show you is also related to rolling, but it’s more about aggregation. Let’s say we want to find the month with the highest average closing price.

To do this, we resample the data on a monthly basis and calculate the mean. Then we focus on the close column and look for the index of the maximum value. This gives us the month with the highest average closing price.

You might see a warning that the letter M is deprecated and that you should use ME for month-end, but the logic remains the same. Once we run this, we get our result.

This is one way to explore the data. You can also preview different parts of the dataset. For example, if you want to see the last five rows, you can use DataFrame.tail(). This is essentially the opposite of DataFrame.head().

And that’s it for this video.

You might notice that I mention things like data manipulation, seasonality, autocorrelation, and partial autocorrelation. Realistically, this entire section is dedicated to giving you techniques to get to know your time series better.

The goal is that, by the end, you have a toolbox of methods you can use to understand any time series you’re working with.

# **F) Python - Exploratory Data Analysis Part 2**

In this video, we are going to continue with the exploratory data analysis.

One thing that’s very interesting in time series analysis is looking at day-over-day or period-over-period comparisons. For example, you take one day and then look at the percentage change compared to the previous one. This is exactly what we’re going to do here.

We’re going to compute the percentage change for the closing price variable. There is a built-in function in pandas that allows us to do this very easily.

We always start by selecting our variable, so we take DataFrame['Close']. Then we apply the percentage change function, which is already available. When we do this, we immediately get the percentage change for each day.

The first day is, of course, not available because we don’t have a previous day to compare it to. Essentially, the calculation works by dividing the current day’s value by the previous day’s value. For example, one day divided by the day before it. This type of calculation is extremely common in financial data and is very helpful because it connects the dots between consecutive periods.

Personally, I like to multiply this value by 100 so that it’s easier to visualize as a percentage. I find that having the values expressed this way makes interpretation much more intuitive.

We then store this under a new column in the DataFrame, something like daily_returns_100pct, so it’s always clear that the values have been multiplied by 100.

Once we have this, we can start exploring the data. For instance, we might want to check which days had more than a 10% change in price.

To do that, we take the DataFrame and filter it by selecting the daily returns that are greater than 10%. This gives us all the days where the price increased by more than 10%.

But then you might think—what about days where the price dropped by more than 10%? Those are not included in this filter.

The solution is very simple. We take the absolute value of the daily returns and then check which values are greater than 10%. This way, we capture both large positive and large negative changes.

Now we’re looking at all the days that experienced more than a 10% change in either direction. For this dataset, you can see that there are around 97 days with more than a 10% change.

That’s massive and clearly shows how extremely volatile this data is.

That’s it for this video. In the next one, we’re going to focus on data visualization and explore this dataset visually.

# **G) Python - Data Visualization**

In this video, we are going to explore data visualization. Of course, we have already done some visualization earlier, but now I want to explore a few additional parameters that you can add to make your visualizations cleaner and more visually appealing.

Let’s take a look.

We will start with the very basics, as always. For example, plotting the daily closing price. We simply go to our DataFrame, select the close column, and call .plot().

We’ve done this before, but let’s improve it a little. For instance, we can add a title. We just specify something like “Daily Closing Price”, and now the plot looks more informative and polished. This is one simple way to enhance your visualization.

Next, let’s explore something slightly different. We’ve already looked at rolling averages earlier, but now let’s apply it to a different variable.

We’ll create a 30-day rolling average for volume. To do this, we start with our DataFrame, select the volume column, apply a rolling window of 30, and then compute the mean. This gives us a new variable representing the 30-day rolling volume.

Now let’s try to plot both the closing price and the 30-day rolling volume together.

If we try to plot them directly on the same chart, it doesn’t really make sense because the magnitudes are completely different. The volume values are much larger, which causes the closing price to appear almost like a flat horizontal line.

So how do we fix this?

The solution is to use two different y-axes.

We start by plotting the 30-day rolling volume and enabling the legend. Then we add plt.show() to display the plot properly.

Next, we plot the closing price on a secondary y-axis. We specify secondary_y=True and also enable the legend. It’s important to remember that True must be capitalized so that Python recognizes it as a boolean value.

Finally, we label the y-axis for the closing price. This is how we correctly visualize two variables with very different magnitudes on the same plot.

Now we can actually start connecting the dots.

By looking at the visualization, we can begin to see potential relationships between volume and price. While it’s sometimes difficult to interpret visually, in general, when volume goes up, the price also tends to go up. This kind of relationship becomes much clearer when both variables are plotted together with separate axes.

As a final step, let’s briefly look at correlation.

We can compute the correlation between the closing price and the 30-day rolling volume. This gives us a numerical measure of how strongly these two variables are related.

If we print the results properly and use the .corr() function, we can see that the values are the same, just formatted slightly differently depending on how we display them.

It’s important to note that this correlation is the Pearson correlation, which is the default in pandas. Pearson correlation is often not ideal for time series data, but for our purposes, it still gives us a good intuition.

Just as an FYI, you might want to explore Spearman correlation, which is more commonly used for time series. However, I don’t want to go down the rabbit hole of statistics here.

Even with Pearson correlation, we’re still about 90% correct, and the overall conclusion does not change: the two variables are strongly connected. In fact, we don’t even strictly need the correlation value here, because the visualization already tells us the story.

That’s it for data visualization.

In the next video, I want us to focus on data manipulation. This is a specific topic that I’d like us to dive into next.

# **H) Python - Data Manipulation Part 1**

The first step is to identify missing values. The way we do this is by using our DataFrame, calling isnull(), and then applying sum(). This allows us to count the number of missing values per column.

Once we do this, we can clearly see that some of the columns we are working with contain null values. At that point, the real challenge becomes: how do we actually fill these missing values?

There is no single correct solution here. The approach you choose very much depends on the context of the problem. What I want to do is show you a few common techniques, and then based on your own use case, you can decide which one makes the most sense.

The first technique is to fill missing values using the next available observation.

For example, let’s say Day 29 is missing, but Day 30 has a value. In this case, Day 29 would take the value of Day 30. This approach is very useful when you want to maintain continuity in the data, or when you believe that something went wrong on a specific day and the next value is a reasonable replacement.

Again, this is just one option, and whether it makes sense or not depends entirely on your situation.

Let’s take a concrete example: the 30-day rolling volume.

We take the 30-day rolling volume series and apply fillna() with the method set to backward fill (bfill), and we try to do it in place.

When we do this, we may encounter an error or warning. This is actually an important topic to address.

The warning says that a value is trying to be set on a copy of a DataFrame or Series through chained assignment using an in-place method. It also mentions that this behavior will change in pandas 3.0.

This is one of the reasons I’m explicitly recording and explaining this, because these kinds of issues can easily show up in real projects. At the moment, we’re using pandas version 2.2, but future versions will be stricter.

The warning essentially tells us that instead of modifying a column in place using chained assignment, we should reassign the result back to the column explicitly.

So rather than using an in-place operation, we should do something like:
assign the filled result back to the DataFrame column itself.

When we try to follow this approach, we might run into another issue. For example, using method= inside fillna() on a Series may result in a deprecation warning. The message tells us that using method is deprecated and that we should instead use ffill() or bfill() directly.

This is another example of pandas evolving over time and why it’s important to keep your code up to date.

So instead of using fillna(method="bfill"), we directly use bfill().

Once we do this correctly, everything works as expected. This is a good example of how you can take a warning message, read it carefully, and fix your code step by step.

At this point, it’s worth clarifying the difference between backward fill (bfill) and forward fill (ffill).

Backward fill means that a missing value is filled using the next available value. This matches the earlier example where Day 29 takes the value of Day 30.

Forward fill does the opposite: it fills the missing value using the previous available value. In that case, Day 29 would take the value of Day 28, assuming it exists.

In practice, I personally tend to use backward fill more often, because in many time-series scenarios it makes more intuitive sense. But again, this depends entirely on your data and your assumptions.

Another common technique for handling missing values is interpolation.

Let’s take a different example: the 7-day closing average. We already know that rolling averages naturally introduce missing values at the beginning of the series.

Instead of forward or backward filling, we can interpolate these values.

To do this, we take the 7-day closing average series and call .interpolate() on it. We no longer use inplace=True, and the operation completes successfully.

This is how interpolation works in practice.

Whenever you use interpolation, it’s very important to check the documentation. I’m a big fan of going through the pandas docs, especially when working with something new or nuanced. It does take time to get used to, but it’s absolutely worth it.

In this case, we are working with a Series, which is one-dimensional data. A DataFrame would be two-dimensional, but that distinction doesn’t change much for interpolation in this scenario.

Looking at the pandas Series interpolate() documentation, we see that the default method is linear interpolation.

Linear interpolation assumes that the data points are equally spaced, which is true for daily time-series data. In most cases, linear interpolation will be sufficient.

The key difference between interpolation and forward or backward fill is that interpolation looks at both sides of the missing value. It considers the value before and the value after, and then computes a reasonable estimate in between.

Forward fill and backward fill, on the other hand, only look at one side of the equation.

To summarize:

Forward fill (ffill) uses the previous value

Backward fill (bfill) uses the next value

Interpolation uses both before and after values

Each approach has its place, depending on the problem you’re trying to solve.

That’s it for now. I spent some extra time here because missing values are a critical part of time-series data manipulation.

We’re going to come back to data manipulation and explore it further in the next video.

# **I) Python - Data Manipulation Part 2**

In this video, we’re going to focus on the index and also on feature engineering. Let’s get started.

The first thing we’re going to do is focus on the index. If we fetch the index from our DataFrame, this is as simple as calling dataframe.index. When we do that, we get all of the dates associated with our time series.

You may notice that the frequency is currently set to None. We’ll talk about frequency later, but for now, that’s perfectly fine.

What’s important here is that the index already contains a lot of valuable information. Because these are dates, we can actually create multiple features directly from the index. For example, we might want to know whether a given observation falls on a Monday, whether it’s a weekend, which month it belongs to, or which year it comes from.

All of this information already exists inside the index.

For instance, if we access the day of the week, we’ll see values like 0, 1, 2, 3, 4, 5, and 6. These correspond to Monday through Sunday.

Our goal here is to extract time variables and store them as new features.

Let’s start by extracting the year.
We create a new column called year, and assign it the value dataframe.index.year.

Next, we can extract the month by creating a month column and assigning dataframe.index.month.

We can do the same for the day of the month by creating a day column and assigning dataframe.index.day.

We can continue with more granular features. For example, we may want to extract the day of the week numerically. We do this by creating a column and assigning dataframe.index.dayofweek.

Sometimes, instead of numeric values like 0, 1, or 2, we want the actual day names, such as Monday, Tuesday, and so on. To do that, we create another column and assign dataframe.index.day_name(). This one requires parentheses.

There’s also another numeric representation we can use, which is weekday. This is essentially the same as day of week, but it’s still useful to store explicitly. We create a column and assign dataframe.index.weekday.

At this point, if we take a quick look at the DataFrame using head(), we can see that at the end we now have several new columns: year, month, day, day of week, weekday name, and weekday numeric.

As a final time-based feature, let’s create a weekend indicator.

We define a new column called is_weekend. This column will be a boolean that tells us whether a date falls on a weekend or not. The result is True for weekends and False otherwise.

This is actually very useful, because booleans are often easier to work with in models.

If you prefer to have this feature as a binary variable instead of True and False, you can convert it to integers using astype(int). This gives you 0 and 1, which is often ideal for machine learning models.

You can even add a comment to remind yourself that this conversion turns the feature into a binary representation.

All of this work is done using the index, but it also falls under feature engineering, which is what I want to focus on next.

There are many ways to perform feature engineering, and we’ve already covered some of them. For example, rolling averages—like a 30-day rolling average—are also a form of feature engineering. They smooth the data and help capture trends.

However, one of the most common and most important feature engineering techniques in time series is the creation of lagged variables.

So what are lagged variables?

Think about a real-world example. You’re browsing the web and you see an ad. You think it looks good, but you don’t buy the product that day. Instead, you purchase it the next day.

From a modeling perspective, the influence of the ad doesn’t happen immediately—it happens with a delay.

If we don’t include lagged variables in our model, we fail to capture this delayed effect. Lagged variables allow us to model situations where the impact of a regressor occurs in the future, rather than at the same time.

In other words, the value today may influence outcomes tomorrow.

Creating lagged variables is extremely simple.

For example, if we take the close column and apply shift(1), we create a lag-1 variable. This means that today’s row now contains yesterday’s closing price.

If we look at the values, we can see that what originally appeared on one date now appears on the next date.

If we want to go further, we can create a lag-2 variable by using shift(2). This shifts the values back by two periods.

As you can see, building lagged variables is extremely easy and very powerful.

We can store these as new columns, such as close_lag_1 and close_lag_2. If we preview the DataFrame again using head(), we’ll see these new lagged features added at the end.

All of this opens up a wide range of possibilities when it comes to modeling. As you start to understand your problem better, you begin to think about which variables influence outcomes and how those influences unfold over time.

This is where time series analysis and forecasting really begin to take shape.

# **J) Seasonal Decomposition**

In this video, I’m going to introduce you to seasonal decomposition.

The general idea behind seasonal decomposition is that we separate time series data into three distinct components: the trend, the seasonality, and the error term (also called the residual).

Let’s look at each of these components individually, starting with the trend.

The trend represents the general direction of the time series over time. You can imagine this as a smooth curve showing whether the data is generally increasing, decreasing, or remaining stable.

One important thing to keep in mind is that a trend can change over time, but it does not change every single day. If it did, it would no longer be considered a trend.

Next, we have seasonality.

Seasonality refers to recurring, cyclical patterns that repeat at regular intervals. A classic example is a time series that tends to be higher during the summer months and lower during the winter months.

You can imagine a chart where the data follows a repeating seasonal curve. This curve is cyclical, remains fairly consistent over time, and has a certain amplitude between its peaks and troughs.

Finally, we have the error term, also known as the residual.

The error term represents everything that is not explained by the trend or the seasonality. Ideally, this component behaves like random noise and does not exhibit any clear pattern. You can think of it as a random walk with no structure.

Now let’s zoom in specifically on seasonality.

One important thing to understand is that there are two main types of seasonality.

The first type is additive seasonality.

Additive seasonality is characterized by constant seasonal fluctuations. For example, this could mean that we always add 10 units in July or subtract 50 units in December.

In this case, the size of the seasonal effect stays the same regardless of whether the trend is low, medium, or high. If you were to plot this, you would see the same seasonal pattern repeating with the same amplitude over time.

The second type is multiplicative seasonality.

Multiplicative seasonality occurs when the seasonal cycles are proportional to the trend. Instead of thinking in terms of absolute units, we think in percentages.

For example, the data might increase by 10% in July or decrease by 50% in December. In a chart, you would observe that the size of the seasonal fluctuations increases as the trend increases. Even though the pattern repeats, the amplitude grows over time.

Now you might ask: why does this distinction matter?

By understanding whether your data exhibits additive or multiplicative seasonality, you can make better forecasts and better-informed decisions. The type of seasonality directly influences how models interpret patterns and predict future values.

Another important question is: how do we identify which type of seasonality our time series has?

Unfortunately, there is no statistical test that can definitively tell us whether the seasonality is additive or multiplicative. However, we do have two practical options.

The first option is data visualization.

By plotting the time series, we can visually inspect whether the seasonal fluctuations remain constant over time or whether they grow and shrink proportionally with the trend. This is something we’ve already started doing in earlier videos.

The second option—and this one is very powerful—is to focus on model performance.

This means building two different models: one assuming additive seasonality and another assuming multiplicative seasonality. We then compare their performance and see which one better fits the data.

Ideally, we should use both approaches every time.

Visualization helps us form an initial intuition before modeling, while model performance helps us validate that intuition afterward. In practice, the second approach is often preferred because it is results-driven.

In other words, the first option is about making an assessment before modeling, and the second option is about evaluating the results after modeling.

Of course, we’ll try both approaches.

But for now, let’s move on and see how to check for seasonality and plot it in Python.

# **K) Python - Seasonal Plots**

One of the most important topics in time series analysis is seasonality, which refers to the cyclical patterns that occur over time. These recurring curves appear again and again across specific time intervals, such as months or quarters.

There are many different ways to identify, analyze, and model seasonality, and throughout this course, I’ll show you several approaches. For now, however, we are going to focus purely on visualizing seasonality, which is often the first and most intuitive step.

There is one specific set of functions that I want to introduce for this purpose. To keep everything organized, we’ll start by importing them.

From statsmodels.graphics.tsa.plots, we are going to import two functions: month_plot and quarter_plot. These are specifically designed to visualize seasonal patterns.

In addition, although we won’t use it in this video, we will also import seasonal_decompose from statsmodels.tsa.seasonal, as it will be important later in the course.

To begin, let’s focus on monthly seasonality.

One of the simplest ways to analyze monthly seasonality is to take the monthly values of a variable and observe how they behave over time. This approach provides a clear visual indication of whether seasonal patterns exist.

We start by selecting the closing price from our data frame. Then, we resample the data to a monthly frequency using month-end ("M") and calculate the mean for each month.

Once this is done, we apply the month_plot function to the resampled data.

The resulting visualization contains two important components.

The black line represents the actual observed values for each month over time. In the case of Bitcoin, this line trends upward overall, but with large fluctuations due to volatility. Each point on this line corresponds to an actual monthly value.

The red line represents the average of the averages. In other words, for each month (January, February, and so on), it shows the average value across all years. This red line is what we use to identify seasonal patterns.

In this particular case, we do not observe strong seasonality. The red line does not show clear peaks or troughs across months, indicating that Bitcoin does not have a pronounced monthly seasonal pattern.

To clean up the output and ensure that only a single plot is displayed, we use plt.show().

We can also improve readability by adding a y-axis label, such as “Closing Price”, which makes the visualization easier to interpret.

Next, we can explore quarterly seasonality using a similar approach.

Once again, we start with the closing price, but this time we resample the data using quarter-end ("Q") instead of month-end. After calculating the mean, we apply the quarter_plot function.

This visualization makes seasonal differences more apparent. With the exception of Q3, which appears unusually flat or lower, the quarterly seasonality becomes clearer. This kind of insight can signal that further investigation is worthwhile.

At this point, it’s useful to look at a different data set that exhibits clearer seasonality.

For this purpose, we import a second data set containing monthly revenue for a chocolate company. This data set is already aggregated at a monthly level, which makes it ideal for seasonal analysis.

We read the CSV file using pandas.read_csv, specifying the date column as the index and enabling parse_dates=True to ensure the dates are handled correctly.

Because this data is already monthly, we do not need to resample it. We can directly apply the month_plot function to the revenue column.

This time, the seasonality is much more apparent. We can clearly see seasonal bottoms occurring during certain months and strong peaks during others.

For example, November stands out as a peak month, which aligns well with real-world events such as Black Friday, Singles’ Day, and holiday shopping seasons.

If we wanted to visualize quarterly seasonality for this data, we could easily do so by resampling using quarter-end frequency and applying the quarter_plot function.

However, we’ll stop here for now. There is much more to explore when it comes to seasonality, and we will continue building on this foundation in the next video.

# **L) Python - Seasonal Decomposition**

Now let’s take a look at one of the most important techniques in time series analysis: seasonal decomposition.

The general idea behind seasonal decomposition is that we take a time series and split it into three separate components. These components are the trend, the seasonal cycles, and the noise (or residuals), which represents whatever remains unexplained after removing trend and seasonality.

To start, we apply seasonal decomposition to our time series data. In this case, we are working with the adjusted close price from our data frame. We use the seasonal_decompose function and then call .plot() to visualize the result.

When we run this, we obtain a plot that shows four panels: the original series, the trend, the seasonal component, and the residuals. This gives us a structured way to understand what is driving the time series.

To make the visualization cleaner and easier to work with, we store the decomposition output in a variable called decomposition. From there, we generate the plot and explicitly set the figure size. A size of 18 by 10 generally works well and makes the components easy to read.

When we reduce the figure size significantly, for example to 5 by 5, we notice that the seasonal cycles are very difficult to see. Everything appears compressed, and the seasonal structure is effectively hidden. This highlights how important visualization choices are when working with decomposition.

At this point, we need to talk about an important parameter in seasonal decomposition: the period.

The seasonal_decompose function needs to know the length of the seasonal cycle. If the data index does not have an explicit frequency, or if we want to override the default behavior, we must specify the period manually.

This is critical because seasonal decomposition only supports one seasonality at a time.

This limitation is important to understand. For example, with daily data, we may have weekly seasonality (7 days), yearly seasonality (365 days), and potentially even monthly effects. However, classical seasonal decomposition forces us to choose only one of these.

This is a known limitation of early time series techniques. More modern models can handle multiple seasonalities, but here we must be explicit about which one we want to analyze.

Since we are working with daily data, one reasonable choice is to set the period to 365, which captures yearly seasonality.

When we do this, we start to see a seasonal pattern emerge. The seasonal component is not perfectly smooth, but it is consistent across time. The trend component shows longer-term movements up and down, and the original series reflects the raw adjusted close values.

If instead we set the period to 7, which corresponds to weekly seasonality, the decomposition becomes much less insightful. The seasonal and trend components do not reveal meaningful structure, and the output is not very informative.

This is expected, especially for stock market data. Stock prices are widely known to behave like a random walk, meaning they do not exhibit strong or reliable trends or seasonality. Because of this, predicting stock prices consistently is extremely difficult, if not impossible.

As a general rule, if someone claims they can reliably predict stock prices, they are likely trying to sell you something.

Now let’s contrast this with a different data set: the chocolate company revenue data.

This data is monthly and contains a single variable: revenue. We apply seasonal decomposition again, this time using a period of 12, since the data is monthly.

We start with an additive model, and the results immediately make more sense. The trend clearly increases over time, the seasonal component shows a repeating annual pattern, and the residuals are relatively small.

However, when we look closely at the revenue data itself, we notice something important. The seasonal spikes become larger over time. Early in the series, the seasonal fluctuations are small, but as revenue grows, the seasonal peaks and troughs grow as well.

This behavior suggests that an additive model may not be ideal.

In cases where seasonal fluctuations increase proportionally with the trend, a multiplicative seasonality model is more appropriate.

When we apply a multiplicative decomposition, the seasonal component is expressed as percentages rather than absolute values. A value of 1.0 represents a neutral effect. Values below 1 indicate a negative seasonal impact, and values above 1 indicate a positive seasonal impact.

For example, a value of 0.8 means revenue is about 20% lower than average due to seasonality, while a value of 1.4 indicates roughly a 40% increase. In this data set, months like November typically show strong positive seasonal effects, which aligns well with real-world business patterns.

This interpretation makes multiplicative seasonality far more intuitive for growing time series like revenue.

With that, we’ll stop here for now. There is still much more to explore when it comes to seasonal decomposition, and we’ll continue building on this in the next video.

# **M) Auto-Correlation**

In this video, I’ll introduce a very important and interesting concept called autocorrelation.

The basic idea behind autocorrelation is to understand whether information from the past can help us predict the future. To do this, we correlate the values of a time series with its own past, or lagged, values and observe how strongly they are related.

Imagine two axes where we plot the observation 
𝑦
y at time 
𝑡
t against 
𝑦
y at time 
𝑡
−
1
t−1. This represents the relationship between the current value and the value from the immediately previous period. If the data points show a clear upward trend, this indicates a positive correlation. Suppose we compute this correlation and get a value of 0.8.

Now we create a main graph where the correlation value is shown on the y-axis and the number of lags is shown on the x-axis. The point corresponding to lag 1 would be plotted at 0.8 on this graph.

Next, we move to lag 2. Here, we correlate the time series with values lagged by two time units. Let’s imagine the correlation value for lag 2 is 0.6. We plot this value on the same graph as well.

One key idea to understand is that as the lag increases, the correlation usually decreases. This is because the information comes from further in the past and is therefore less relevant for predicting the present. As a result, the correlation values tend to get smaller as the lag increases.

If we continue this process for higher lags, the correlation values may become very small or even negative, but they generally move closer to zero. By analyzing this pattern, we can understand how far back in time we can go and still find useful information for prediction.

Of course, this does not give us the complete picture by itself, but it is a very important piece of information when working with time series data.

To summarize, an autocorrelation plot helps us determine whether there is useful information in the past values of a time series and how long that information remains relevant.

Now let’s apply this concept and see how it works in practice. Until the next video.

# **N) Python - Auto-correlation**

We’re now going to work with autocorrelation using the statsmodels library, which is great because it not only computes autocorrelation but also plots it for us. Here, I’m following a common suggestion, which is to plot both the autocorrelation and the partial autocorrelation, although we still need to properly learn what partial autocorrelation is.

The good thing is that this is very easy to do. In general, we don’t care much about the exact numerical values of the correlations, because we’re not going to directly use those numbers anywhere. What really matters is how the plot looks, what it tells us, and the intuition behind it. That’s what we’re going to focus on.

Let me activate the environment first. I actually need to activate everything else as well. Alright, everything is activated now. I jumped to the bottom of the script. If you’re wondering why I needed to activate everything again, it’s because I’m re-recording this video. Sometimes recordings don’t go as planned, and I need to redo them to keep the content and the Python code as up to date as possible. Anyway, let’s move on.

The first step is to plot the autocorrelation of the Bitcoin adjusted close price. I have a suggested snippet here, but I’m not going to use it immediately because I want to control the figure size. So I first define the figure using plt.figure and set the size. I’ll go with a width of 12 and a height of 6. Then I plot the autocorrelation for the adjusted close values.

One important thing we still need to specify is the number of lags. Let’s set it to 100 and see what this gives us. Once we run it, we get the plot.

What we see here is quite striking. The level of correlation in Bitcoin is absolutely massive. There is a very strong correlation across many lags, which is honestly quite uncanny. Now, if we want to be a bit more critical and properly assess this, it almost looks like a linear curve. That immediately raises an important question: is this actually relevant?

In other words, if we look at lag 100, what information does lag 100 really contain that was not already present in lag 99? That’s the key question. What this plot is indicating is that all of these lags contain information, and possibly even relevant information. You could technically take lag 100 and still observe a strong correlation. But the real issue is uniqueness. What unique information does lag 100 bring that was not already captured by lag 99, lag 98, and so on?

This is exactly where partial autocorrelation comes into play. I’m spoiling it a bit here, but that’s essentially what partial autocorrelation answers. It tells us the correlation at a specific lag after removing the effects of the previous lags. In other words, it helps us understand whether the correlation at lag 100 is truly new information, or whether it only exists because the correlation was already present at lag 99, which itself depended on lag 98, and so forth.

In many cases, what we see at higher lags is not really new or relevant information. It’s just inherited from earlier lags. We’ll confirm this more clearly once we look at partial autocorrelation.

Now let’s shift our focus to a different dataset. If we inspect the df_choco dataframe, we can see that it has only 60 entries. Because of that, it wouldn’t make sense to use 100 lags here. Instead, something like 20 lags is much more reasonable.

So we now plot the autocorrelation function for the revenue KPI in df_choco, using 20 lags. We follow the same approach as before, just changing the data and the number of lags. Once we plot it, the result looks very different.

Notice that we don’t see the same kind of linear decay here. Instead, the pattern goes down, up, down, up. Another very important thing to notice is the blue shaded area in the plot. This represents the confidence interval. As we move further into the past, this interval widens. That makes sense, because the further back we go, the fewer observations we have, and the more variability we expect.

What this plot is telling us is very insightful. We can clearly see seasonality in the data. There is strong correlation at the most recent lags, which reflects the trend. But we also see notable spikes around lag 5 or 6, and again around lag 12. This suggests seasonal behavior at roughly six-month and twelve-month intervals.

This aligns well with intuition. There is information repeating itself on a seasonal basis. For example, something meaningful is happening every six months and every twelve months. This gives us strong evidence that seasonality plays a significant role in this dataset.

Again, the natural question arises: what is the value added by a specific lag? For instance, what does month three contribute that was not already present in month two or month one? This is exactly the kind of question that partial autocorrelation is designed to answer.

Overall, this exercise highlights how important it is to understand our data deeply. Autocorrelation helps us see whether past values contain useful information and how far back that information remains relevant. In this case, it looks like information up to around 12 months in the past can help us predict the future.

# **O) Partial Auto-Correlation**

Now that we understand the autocorrelation, let’s move on to the partial autocorrelation, which is a very cool concept in the world of time series. Don’t let this scare you—it's not super complicated at all. It’s actually really handy, and I’m going to break it down for you in very simple terms. So let’s kick it off.

What is this PACF, or partial autocorrelation function, all about? Imagine you’re trying to understand the relationship between your coffee consumption today and how much coffee you drank a few days ago. But here’s the twist: you want to know this relationship without the influence of all the days in between. That’s exactly what partial autocorrelation does. It tells you the direct relationship between your data points at different times, while removing the effects of the points in between.

Now remember the autocorrelation function. That’s like looking at the total correlation between a time series and its lagged values, including both direct and indirect effects. It’s similar to asking, “How does my coffee consumption today relate to all the past days?”

Partial autocorrelation, on the other hand, is more specific. It’s like asking, “How does my coffee consumption today relate specifically to three days ago?” while completely ignoring the days in between. So while autocorrelation gives you the overall picture, partial autocorrelation zooms in on specific, direct relationships.

When you plot the PACF, you’ll usually see bars at each lag, just like you do with the autocorrelation plot. If a bar stands out significantly, it means there is a noteworthy direct relationship at that particular lag. If these bars drop off quickly, it suggests that only recent values have a direct effect on current values. If they tail off slowly or oscillate, it indicates that older values still have a direct influence.

You might now wonder why PACF is needed when we already have ACF. The autocorrelation function starts the story by showing all correlations, but it can include noise from indirect correlations. The partial autocorrelation completes the story by isolating only the direct correlations, giving you a much clearer picture of how each point in time directly influences another.

# **P) Python - Partial Auto-Correlation**

Welcome back, and welcome to partial autocorrelation. Partial autocorrelation is very simple in terms of application. Just like before, we’re going to compute the PACF for Bitcoin adjusted close. There we go, and we’re going to do the exact same thing as we did earlier. Even Gemini catches on very quickly if you know what to do, and you can put it in comments and get this done very, very fast.

The only difference between this code and the ACF code is this “P” here. That’s literally the only difference. Now, look at this chart. It’s quite different, right? If you look at it closely, it’s just so different compared to the autocorrelation plot.

What this chart is telling us is that the only information that is actually relevant, the only information that is truly unique, comes from the day before. And this is kind of crazy when you think about it. You saw earlier that we had so much information, but basically all of that information was always connected to the information that happened just before it. There was no unique information beyond that. Of course, you do see some values here that go outside, but really, for Bitcoin—and please take this seriously—if you’re trying to predict Bitcoin, the only relevant information is what happened the day before. This is a very clear conclusion from this chart.

Let me actually write it down. The only relevant information for the price of Bitcoin is what happened the day before. There we go.

Now we can do the PACF for revenue in the data frame. Let me jump here using some shortcuts. Here we go, and here we are with the partial autocorrelation. In this case, we clearly see that there is information in the day before, but apparently there is also relevant information at two, three—so one, two—then four and five months in the past, and also around ten months in the past. Twelve months, apparently, is not so relevant.

So if we count the lags—one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen—this means we’re looking at the previous month and the previous year. Apparently, we did not get a lot of unique information overall, but you do get unique information if you don’t just look one month back. If you look four or five months back, ten months back, and apparently thirteen months back as well, those lags were important.

If you connect this partial autocorrelation chart to the autocorrelation chart, it becomes much more meaningful. From the partial autocorrelation, you can say, okay, these specific lags contain unique information. That’s how you can phrase it. For the autocorrelation, it’s more like saying, okay, I can use all this information from the past and still retain information. You kind of connect both ideas, and then you’re in good shape.

You can still see some level of information around twelve or thirteen months here as well, but it becomes very, very small. If you really need to nitpick, if you need to pick just a few lags, you would go with these ones. These specific lags contain very unique information that you don’t find anywhere else.

And that’s it for us. This is how you do partial autocorrelation. It’s very, very quick, and I’ll see you in the next video.

# **Q) Python - Building a Useful Function Script**

Welcome back for this last video. I want us to build a script that you can use whenever you’re working with some kind of time series analysis. Of course, it’s not perfect and it’s not a one-size-fits-all solution, but it works as a solid starting point. It gives you a building block so you can take whatever you’re doing and begin by analyzing the data. No matter which technique you later apply, exploratory data analysis, manipulation, and visualization are more or less the same and don’t really change that much.

I’m going to take this script here and move it into the main one. Let me open it, and then I’ll start with the first cleanup. The idea is to keep what’s always relevant and remove what’s not. For the setup section, everything up to the path stays the same. The Python time series forecasting libraries remain unchanged. The Bitcoin price file becomes a generic “xxx.csv” so it can be reused. I’ll clear the selected outputs to keep things clean.

For the time series index, I’m going to keep it because it’s important. Usually, the column is called “date,” so there’s no issue there. I’ll remove this part and instead use the version where we set the index directly and parse the dates at the same time. That’s the one I want to keep, because most of the time we set the index column and parse dates together. If I only want the actual date column, I can simply delete the index and parsed dates part.

I’ll remove the remaining unnecessary pieces and keep things as clean as possible. The idea is to load the data and immediately put the date on the index. For exploratory data analysis, I’m not going to use rolling prices, so that part is out. For data visualization, I will keep it. I’ll plot the series without a title, which is usually better. I typically call my time series “y” because it’s easier to visualize, easier to work with, and I don’t have to rename it all the time. It also makes sense conceptually, since this is what we’re trying to predict. The extra parts here are removed.

For data manipulation, we’re not going to focus on that here, so that section is gone as well. When it comes to seasonality, this part will stay. I’ll clean it up and leave out the extra data frame pieces and labels. If I want to add a label later, I can always do that. We’ll keep the monthly plot, and I should also add a quarterly plot. So I’ll include the quarter plot, show it, and keep the seasonal decomposition. The default multiplicative model is fine.

Next, we add the autocorrelation. I’ll set the number of lags to 100, and if I ever need to change it, I can. Then I add the partial autocorrelation as well. Now we have both ACF and PACF in the script.

It’s worth reinforcing that I’m trying to keep this course up to date. Even when future warnings appear, the goal is to maintain a clean and usable script. There may be discrepancies across different versions of libraries, and that’s expected. This script is meant to be a useful code template. If something behaves slightly differently in your environment, that’s normal.

If you have any questions, let me know. I try to make sure everything works in one go, but keep in mind this is a course that’s close to 40 hours long. There are bound to be moments where things don’t connect perfectly. I really appreciate your patience. If anything confuses you, just ask—I’m here to help.

As a final note, it’s a bit sad for me when people say they don’t understand something, not because I think I’m the best explainer in the world—I definitely don’t—but because often they don’t ask questions. If you have questions, let me know. I answer everyone seriously. Just post it in the Q&A, and I’ll be there to help you out. I’ll see you in the next video.

# **R) Can you predict stock prices?**

Have you ever been bombarded with stock price predictions? If so, then this video is for you.

As we’ve seen, distinguishing between trend and seasonality in time series can seem straightforward at first. So why is forecasting still considered such a complex task? Why do organizations dedicate entire teams to it? The main challenge lies in modeling and interpreting errors. Forecasting is not just about fitting a model, but about understanding where it fails, why it fails, and how those errors can be explained and reduced.

A big part of improving predictions comes from incorporating relevant factors or regressors. These can include specific events, temperature changes, snowfall, overall economic conditions, and even public sentiment. Each of these elements can significantly influence forecasting accuracy, depending on the problem you’re trying to solve.

Another crucial aspect is the time horizon of the data. Suppose you have data spanning six or seven years. Do you really need all of it? Older data can introduce a lot of noise into your models because past conditions may no longer reflect what is happening today or what will happen in the future. This is why it’s essential to evaluate whether your data is still representative of future trends, and if it’s not, to make the conscious decision to exclude it from your analysis.

This brings us directly to stock market prediction. We are constantly flooded with articles claiming extraordinary returns using different strategies and algorithms. But remember, everyone looks like a genius in a bull market. The real question is whether these approaches can consistently outperform professional investment firms. In most cases, that’s highly doubtful.

Price movements can give the impression of a clear trend, but predicting when that trend will reverse is extremely difficult if there is no reliable pattern. Traditional forecasting models struggle in these situations, especially when sudden changes occur.

This unpredictability became painfully clear during the Covid-19 pandemic in March 2020. The pandemic completely disrupted existing trends and seasonal patterns, making many forecasting models ineffective overnight. It was a powerful reminder that external, unprecedented events can drastically change market dynamics.

In summary, forecasting—particularly with stock data—is a complex and nuanced task. It requires careful consideration of data relevance, thoughtful modeling of errors, and an awareness of external influences that can break even the most well-designed models.

# **S) What did we learn in this section?**

We have just finished the crossing line, and for this introduction to time series forecasting, it’s a great moment to pause and reflect on everything we’ve accomplished. It’s been quite a journey when you think about it.

We started from the very basics, simply trying to understand what time series data actually is. At first, it can feel unfamiliar, almost like learning a new language. But now, it feels natural. We’ve gone from confusion to confidence, and we can clearly analyze how data behaves over time.

Diving into Python was a major turning point. What may have felt intimidating in the beginning quickly became empowering. We learned how to slice and dice time series data with confidence, using Python not just as a tool, but as a reliable partner in solving real problems. Those libraries are no longer abstract concepts; they’re part of your everyday toolkit now.

We also made big progress in data visualization. Instead of staring at raw numbers, we learned how to turn them into meaningful stories. The charts we create now aren’t just visuals, they communicate insights in a way that makes sense to anyone looking at them.

Seasonality was another important concept we tackled. What once seemed tricky is now something we can recognize and interpret with ease. Whether it’s sales patterns or recurring trends, we know how to identify and reason about seasonal behavior in data.

Autocorrelation marked another milestone. It’s one thing to look at individual data points, but understanding how values relate to each other across time is a whole new level. And you’ve reached that level by learning how past values influence the present.

The discussion around predicting stock prices was especially eye-opening. It showed us the realities of forecasting and the complexity of financial markets. There’s no magic crystal ball, but there is careful analysis, critical thinking, and a lot of smart, hard work behind every meaningful prediction.

So take a moment to appreciate how far you’ve come. You didn’t just learn concepts—you applied them, analyzed data, and visualized insights along the way. That’s real progress.

# **T) CASE STUDY: Forecasting Gone Wrong**

Hey everyone, and welcome. Today I want to talk about something pretty interesting: what happens when forecasts don’t quite hit the mark. We all try to predict the future in one way or another, whether it’s the stock market, fashion trends, or even just the weather. But sometimes things don’t go as planned, and that’s actually okay. When predictions go sideways, that’s often when we learn the most.

Let’s look at some memorable forecast failures and what they can teach us. Take the rise and fall of fidget spinners. Remember those little spinning gadgets that suddenly seemed to be everywhere? One day nobody knew what they were, and the next day every kid in school or university had one. Retailers and manufacturers assumed they had struck gold and ramped up production aggressively. But the hype faded almost as quickly as it appeared, and suddenly stores were left with huge piles of unsold inventory. It was a classic case of confusing a short-lived craze with a long-term trend. The key lesson here is the importance of questioning whether a trend is truly here to stay or just a passing fad.

Another famous example is Long-Term Capital Management, or LTCM. This hedge fund was once considered elite, staffed by brilliant minds using highly sophisticated mathematical models to predict market movements. For a while, it looked like they had cracked the code, generating massive returns. Then the Russian financial crisis hit in 1998, completely outside what their models anticipated. The markets behaved in ways their assumptions couldn’t handle, and the fund collapsed dramatically. It was a stark reminder that no matter how advanced a model is, there will always be events it cannot foresee. Markets are complex and unpredictable, and sometimes they move in ways no algorithm can capture.

Then there’s Google Flu Trends, an ambitious attempt to predict flu outbreaks by analyzing what people searched for online. The idea was simple and clever: if more people are searching for flu symptoms, a flu outbreak might be underway. Initially, it looked like a breakthrough in using big data for public health. Over time, however, it became clear that the model was frequently overestimating flu cases, sometimes by a wide margin. Changes in user behavior and constant updates to Google’s own search algorithms distorted the signals. This case shows that big data and powerful algorithms still need careful calibration and constant reevaluation, especially when human behavior is involved.

Another intriguing example is the Hindenburg Omen, a complex technical indicator designed to predict stock market crashes. Based on certain market conditions, such as the number of stocks reaching new highs and lows at the same time, it aims to warn of an impending collapse. While it has occasionally preceded market downturns, it has also triggered false alarms that caused unnecessary panic, and at other times it failed to signal real crashes. This highlights the danger of relying too heavily on a single indicator without considering broader context and supporting evidence.

All of these stories point to the same underlying truth. The world is full of surprises, and no forecasting model, no matter how sophisticated, can account for everything. Whether we’re dealing with financial markets, public health, or consumer trends, uncertainty is always part of the equation. Forecasts are valuable tools, but they work best when paired with humility, critical thinking, and an understanding that the unknown will always play a role.

# **V) Section 5: Time Series Analysis Practice**

# **A) Data Loading and Index**

Welcome to this practical activity!

In this exercise, we’re going to work with Python, and I’ll demonstrate it using Colab. Of course, you can use any Python environment of your choice. Alternatively, if you have access, the Udemy workspace is also available.

Here’s what we’re working on: we have the weekly sales of a department, and our goal is to explore this data. You’ll find the dataset in the folder I’ve shared, and it’s also preloaded in the Udemy workspace. You’ll see a starter file and Lab 1, which contains the solutions. We’ll begin with the starter file.

First, connect to your drive. Make sure to adjust your directory path if it differs from mine. I’ve also prepared all the necessary libraries for you.

We’ll complete three tasks in this lab. The purpose of this initial exercise is to prepare the dataset for time series analysis.

Task 1: Load the department sales dataset into a DataFrame. Use:

import pandas as pd


df = pd.read_csv("department_iPhone_sales.csv", index_col="date", parse_dates=True)

Here, we make sure the date column is used as the index, which is important for time series analysis.

Task 2: Convert the index into a standard date format (YYYY-MM-DD), which makes it easier to work with. By default, Pandas may infer a different format, so we explicitly specify:

df.index = pd.to_datetime(df.index, format="%d-%m-%Y")

Now, the index shows the year, month, and day correctly.

Task 3: Set the frequency of the data to weekly. Since our dataset is recorded weekly, we tell Python:

df.index.freq = "W-FRI"

We use W-FRI because the sales dates fall on Fridays. This ensures Python correctly interprets the weekly spacing.

And that’s it! With these three steps, your dataset is now properly formatted for time series analysis: the dates are clean, indexed correctly, and the frequency is set.

# **B) Data Visualization for Time Series**

Let's do Lab Number Two. You can either continue on the starter Lab One file, which is the same, or I’ve also prepared the starter Lab Two just in case you are starting with this video.

For these exercises, we are going to focus on data visualization. The first task, Task 1, is to plot the weekly sales using the weekly_sales column. Additionally, we’ll do a bit of formatting—like adding a title to the chart—to make it more readable.

To do this, we can use:

plt.plot(df['weekly_sales'])
plt.title("Weekly Sales")
plt.show()

This is probably the easiest initial component. You’ll notice, however, that the x-axis may look a bit off or crowded. To fix this and improve the visualization, we can adjust the figure size:

plt.figure(figsize=(10, 4))
plt.plot(df['weekly_sales'])
plt.title("Weekly Sales")
plt.show()

Now, we can clearly see the overall evolution of weekly sales. This completes Task 1, where we customized the chart with a proper size and a title.

Looking at the chart, you might notice several spikes—spike, spike, spike—and it draws our curiosity. There’s a column called is_holiday, and the next task will leverage this.

Task 2 involves adding a vertical line whenever is_holiday is set to True. Specifically, we want a vertical dashed red line on the chart.

To implement this, we iterate through the DataFrame rows. For each row, if is_holiday is True, we use plt.axvline to draw the vertical line:

for date, row in df.iterrows():
    if row['is_holiday']:
        plt.axvline(x=date, color='red', linestyle='--', linewidth=0.5)

Here:

color='red' makes the line red.

linestyle='--' makes it dashed.

linewidth=0.5 keeps it thin so it doesn’t dominate the chart.

After plotting, you can see that many spikes in weekly sales appear to align with holidays. Some spikes, like those in May, don’t perfectly align with holidays, which indicates other factors may also contribute. Overall, the red dashed lines help highlight a seasonality component in the data related to holidays.

With this, we have completed Lab Two, which had two tasks:

A simple plot of weekly sales with a title and customized size.

Enhancing the plot by adding vertical red dashed lines whenever there is a holiday, showing how holidays may impact sales.

This visualization is a great way to connect time series spikes with external factors before diving deeper into analysis or forecasting.

# **C) Exploratory Data Analysis for Time Series**

Let's do Lab Number Three. You will also find a starter file for this lab. We have three tasks, and the focus here is more on exploratory data analysis, with an emphasis on autocorrelation and seasonal decomposition. Basically, we want to see what kind of information is already in our data from a trend and seasonality perspective.

Task 1 is to plot the autocorrelation of the data with 60 lags. We can do this using the ACF function:

from statsmodels.graphics.tsaplots import plot_acf
plot_acf(df['weekly_sales'], lags=60)
plt.show()

Looking at the chart, the first lag has the most information. Lags one through six contain some information, and then there’s a noticeable spike at lag 52. This makes sense because it represents roughly one year before. Overall, the strongest signals are from recent weeks, while older lags have less relevance.

Task 2 is to plot the partial autocorrelation. Unlike autocorrelation, which looks at total correlations, the partial autocorrelation isolates the unique information at each lag by removing the influence of earlier lags.

from statsmodels.graphics.tsaplots import plot_pacf
plot_pacf(df['weekly_sales'], lags=60)
plt.show()

Here, we notice a slightly different picture. One year ago doesn’t carry as much unique information, but there is strong unique information one week, two weeks, and seven weeks before. The remaining lags are less relevant from a statistical perspective. This helps identify the most meaningful time intervals in the data.

Task 3 is seasonal decomposition, where we focus on a multiplicative model with a period of 52 weeks (one year):

from statsmodels.tsa.seasonal import seasonal_decompose
decomposition = seasonal_decompose(df['weekly_sales'], model='multiplicative', period=52)

We can improve visualization by adjusting the figure size:

fig = decomposition.plot()
fig.set_size_inches(12, 8)
plt.show()

Looking at the decomposition:

The trend initially goes down, then up, and stabilizes, but keep in mind the first and last six months don’t show a reliable trend due to the way seasonal decomposition works.

The seasonality appears unusual, likely influenced by external factors like holidays.

The residuals hover around one, with some spikes, reflecting irregular components in the data.

While the seasonality is strong, it’s influenced by external elements, so it might appear a bit odd. Nevertheless, this gives us a clear breakdown of trend, seasonality, and residuals in the dataset.

With this, we have completed Task 3 and the lab. I hope you enjoyed this exploration, and I’ll see you in the next video.

# **VI) Section 6: Exponential Smoothing & Holt-Winters**

# **A) Game Plan For Exponential Smoothing and Holt-Winters**

I am very excited to walk you through the world of exponential smoothing and Holt-Winters. In this video, we'll kick off with the agenda that we'll cover throughout this section. I have a very fun case study lined up. Think of it as the goal we need to achieve. It's about customer complaints, and it’s basically as real as it gets. We'll use this case study to apply the skills we’ve learned in a way that mirrors what you would encounter in the business world.

Next up, let's talk about Python. Whether you're new to it or already familiar, we'll be using it extensively. This is how we get hands-on experience. So please prepare yourself for some coding action—we’ll go deep and program everything we need to do.

We won’t just stick to one type of exponential smoothing. Oh no! We’ll explore simple, double, and triple methods, and each smoothing method will get its own spotlight in our Python sessions. This is where the magic happens—where the theory meets practice.

We also need to learn how to measure errors in time series forecasting. And because data comes in all shapes and sizes, we’ll learn how to handle weekly data, daily data, and more granular data. The more granular the data, the more complex it becomes, but this is a skill we need to master.

Last but not least, we’ll wrap up this section with a discussion on the pros and cons of exponential smoothing and Holt-Winters. It’s important to understand what a technique can and cannot do.

So, get ready to learn a lot and hopefully have some fun. By the end of this section, you’ll be able to handle Holt-Winters and exponential smoothing like a pro.

# **B) CASE STUDY BRIEFING: Customer Complaints**

So picture this. We have a challenge that we need to solve. Imagine there is a company called Telco Wave, a very big player in the telecom world. And they’re facing a real puzzle, a challenge. Their customer complaints are all over the place. Some weeks it’s smooth sailing, other weeks it’s total chaos. And guess what? They have asked us to figure it out.

So it’s our job to predict these unpredictable swings. Why, you ask? To help Telco Wave become better at customer service and to showcase how we can use data to actually solve real problems.

Here’s the problem statement. Telecom is really facing this issue because they are getting more and more complaints. They are literally scratching their heads over how many customer service reps they need for each week. If the prediction is wrong, resources are wasted, and customers end up unhappy. This isn’t just a numbers game—it’s about bringing order into chaos.

Therefore, we need to craft a strategy. But before we dive into any complex solutions, we need to understand the basics. We need to get to know the data. What’s behind these fluctuations? What hidden patterns might we be missing? We need to dissect the whys behind these numbers. This is where data analysis comes in.

Exploring the data is a big deal. If we understand it well, Telco Wave can shift from playing catch-up—wasting resources and frustrating customers—to being in control.

By the end, the goal is to empower them to match their workforce perfectly with what the customers need. Fewer complaints will fall through the cracks, more customers will be satisfied, and the business will be healthier, which ultimately means more profits.

# **C) Python - Exponential Smoothing Set Up**

Let's kick off Exponential Smoothing, which is really one of those foundational forecasting models. In this video, we’re going to focus on setup. You are going to take the useful code templates that you built before, and in case you haven’t, you’ll find one here that’s final. You can start from there, of course, but do it with me.

You’ll find one template there, or you can use the one that you built yourself. Then I’m going to do Control C and copy it into time series analysis → exponential smoothing and holt-winters, and let’s open it and see how far we can get in this video. The goal is to either completely do the setup or at least get very close to it. I also don’t want to have a super long video.

I’m going to call the file holt-winters. We’ll start by mounting the drive. This will prompt you to give permission to connect to Google Drive. Once that’s done, we can proceed.

While this is connecting, I want to introduce the dataset. It’s about weekly customer complaints, and the business scenario is simple. Imagine you work in an e-commerce company—or really, any company with customer support—and you need to figure out how many customer support people you need. Most customer supports work with short contracts. Big companies often hire agencies to provide a certain number of people for a week or month. To plan properly, you need to know the expected number of complaints so you can allocate the right workforce. This is where time series analysis and exponential smoothing comes into play.

Next, we copy the path for the dataset and add it to our script. For now, we won’t import any extra libraries; we’ll do that later as needed. The dataset is called weekly customer complaints. It has a few columns, but we will focus on complaints for now. We cannot use independent variables with Holt-Winters or any other exponential smoothing model, so the complaints column is our main focus. The time variable is called week.

After running the initial setup, we have week as the index and complaints as the main column. I like to rename the time series variable to y. This is done by renaming the column: data_frame.rename(columns={'complaints': 'y'}). Now the series is ready for analysis.

Next, we attempt to plot the time series. Initially, we might encounter a “no numeric data to plot” error. This happens because the complaints column contains commas and is recognized as an object rather than a numeric type. To fix this, we remove the commas and convert the column to an integer: data_frame['y'] = data_frame['y'].str.replace(',', '').astype(int). Now the column is properly numeric, and we can plot it.

After plotting, we see that complaints increase over time, with higher spikes and an upward trend. This suggests multiplicative seasonality—the seasonality grows with the level of the series. While the trend is apparent, the spikes indicate specific events or fluctuations. To fully understand the seasonality, we will need to perform seasonal decomposition.

This completes the setup for our exponential smoothing analysis. In the next video, we will continue exploring the data and applying the smoothing methods.

# **D) Python - Exploratory Data Analysis**

Welcome back. Let's continue here. First, we need to understand our seasonality. If we just run the model as-is, we're going to get an error. This is because our data is weekly, but we are trying to generate a monthly plot. Therefore, we need to resample the data to a monthly frequency. The suggestion is to use M, though this may show a future warning. To properly handle this, we should use month end. Once we do that, we have our seasonal data.

Looking at the monthly data, we can observe that there are noticeable bottoms in February and March, as well as in August and September. Peaks are visible around November and a bit in December. This gives us a better understanding of our data. Overall, Q4 (October to December) appears to be the strongest, while Q2 is moderate, and Q1 and Q3 are the weakest quarters.

We can also analyze seasonality by quarter. Similar to before, we need to resample the data, this time using quarter end and calculating the mean. The results confirm that Q4 is the strongest quarter, followed by Q2, then Q3, and finally Q1 as the weakest quarter.

Next, we move on to seasonal decomposition. Since we have weekly data, the period is set to 52, corresponding to the 52 weeks in a year. First, we try an additive decomposition. The seasonal cycles are visible but not extremely smooth; there are noticeable ups and downs. The seasonal component fluctuates by around 100–200 units. The residuals also indicate some variation, especially in March. The trend component appears somewhat S-shaped.

From observing the data, it looks like a multiplicative seasonality might be more appropriate. This is because the seasonal spikes seem to increase as the trend grows, making the amplitude of the cycles larger over time. So we run a multiplicative decomposition. The trend remains largely the same, and the seasonal cycles do not change dramatically, but the multiplicative effect aligns with the increasing amplitude pattern.

As we continue exploring and measuring the models, we will determine which approach—additive or multiplicative—performs better and yields the best results.

Moving on, we examine autocorrelation and partial autocorrelation. By plotting the autocorrelation, we notice that there is significant information in the first 25 periods. There is also a noticeable spike around 52 weeks, representing one year in the past. For the partial autocorrelation, we see strong signals from the previous month, the last week, two weeks prior, and so on. This pattern continues with relevance for about 16 weeks, and then we again observe a signal around 52 weeks. Overall, the recent past and one complete seasonal cycle from the past year seem most informative.

From this analysis, we can conclude that a model incorporating seasonality is needed. Specifically, a Holt-Winters model would be suitable, and this is something we will explore in the upcoming sections.

Finally, let's briefly focus on the time series frequency. To check the frequency of the time series, we can inspect DataFrame.index. This will extract key information about the index, including length, name, and frequency (initially set to None). We can then change the frequency as needed. For example, our weekly data starts on a Monday in January 2018. By setting the weekly frequency to W-MON, we can confirm the change by checking DataFrame.index again, which should now reflect W-MON.

# **E) Training and Test Set in Time Series**

Welcome back. Let me introduce a very important topic in time series: splitting the data into a training set and a test set. This concept is simple and straightforward, but extremely powerful.

Imagine that you have a dataset represented by a blue rectangle. In a typical scenario, you might randomly split it, leaving 80% for training and 20% for testing. The main idea is to build a model using the training data and then evaluate it on the test data. This provides an unbiased way to assess the model’s performance.

However, time series is a very different beast. Unlike regular datasets, information on a given day is meaningless without the context of surrounding days. Moreover, in time series, we usually aim to predict the future. So, when splitting time series data, the practice is to remove the last periods. For example, you might remove the last observation, which then becomes part of the test set. The remaining data—represented here as yellow balls—becomes the training set. Importantly, the training data is not shuffled; it retains its chronological order. The light blue portion represents the test set.

Another important point is that the test set should reflect the number of periods you expect the model to predict in practice. For instance, if you are building a model to forecast the next four weeks, you should evaluate it using four-week periods. Similarly, if your forecasting horizon is three months, your test set should span three months.

Lastly, for the training data, it is recommended to include at least two full periods of data. For weekly data, this means having at least two full years, and ideally three, to capture clear seasonal and trend patterns. The goal is to provide enough data to identify robust patterns, which ultimately leads to reliable forecasts.

# **F) Python - Training and Test Set**

Welcome back. Let's talk about training and test sets. The first thing we need to understand is our goal. Our goal here is to predict the next 13 weeks. It’s important to note that we always measure and assess our model based on the business goal.

To introduce this concept, we will split the data into a training set and a test set. In this case, our period is 13 weeks. The training set will consist of all data up until the last 13 weeks—everything except the last 13 periods. The test set will then consist of everything starting from the last 13 weeks.

If we examine the training set, we still have all the data, which is fine. However, for practical purposes—such as using double or triple exponential smoothing—we often only work with the target variable, Y. So we subset the data to include only the columns we need and remove everything else that isn’t required.

There are multiple ways to subset and split your data. One approach is simply:

train = df[:-periods]   # all data except the last 'periods'
test = df[-periods:]    # the last 'periods' of data

Another way (which I had prepared for a different setup) could be using index positions:

train, test = df.iloc[: -periods], df.iloc[-periods:]

The key takeaway is not necessarily the exact method, but that you understand the concept and purpose of splitting the data. Once you manage this, you’re doing it correctly.

# **G) Simple Exponential Smoothing**

What is this all about?

Imagine you are checking weekly sales figures or weekly complaint counts. Some weeks are very busy, while others are quiet. What you are really trying to find is a steady rhythm in these numbers—a sense of the underlying level without getting distracted by short-term noise.

That is exactly where simple exponential smoothing comes in.

This method is not just a simple average. Instead, it gives more importance to what happened most recently. In other words, it “listens” more closely to recent data points while still respecting the past. This idea is built directly into the formula.

Let’s break that formula down conceptually.

The next forecast is calculated as the current level, plus alpha multiplied by the difference between the most recent actual value and the current level.

First, the current level.
This represents our baseline or starting point. It reflects where we believe the series stands based on everything we have seen so far.

Next, the recent actual value.
This is the latest observation—yesterday’s sales or last week’s numbers. It tells us what actually just happened.

Finally, alpha.
Alpha is the tuning parameter. When alpha is close to 1, we place a lot of weight on the most recent observation. When alpha is close to 0, we trust the historical level more and adjust very slowly.

Now let’s put this into a concrete example.

Suppose our current level is 100 cups sold. That is our baseline. Yesterday, however, we actually sold 120 cups. Let’s say alpha is 0.2.

With this setup, our forecast does not suddenly jump to 120. Instead, it moves up slightly, acknowledging that sales were higher than expected, but without overreacting to a single day’s spike.

Applying the formula gives us:
100 plus 0.2 times (120 minus 100), which results in a forecast of 104.

The values such as the current level and recent actual are computed by the model as it processes the data. Alpha, on the other hand, is typically chosen by the practitioner. For now, we won’t focus deeply on how to select alpha. The goal here is to understand the structure of time series forecasting. Later in the course, we will explore parameter tuning and how different alpha values affect results.

Now let’s think about how this applies to real data.

When we apply simple exponential smoothing to weekly sales data, the goal is to look past spikes—those weeks where sales suddenly surge or drop sharply. This method helps smooth out those fluctuations and gives us a broad sense of direction.

It is especially useful when you want a high-level view of where things are heading, rather than reacting to every short-term change.

To wrap things up, simple exponential smoothing is a very solid tool in a forecasting toolkit—but it is not a fortune teller. It does not capture strong trends or seasonal patterns. The equation itself is simple, and as a result, the outcome is also simple.

That simplicity is exactly why it is called simple exponential smoothing.

It helps smooth chaotic data, reduce noise, and highlight the underlying level of a time series.

Next, we’ll see how this works in Python. From there, we’ll build up step by step—from simple exponential smoothing to double and then triple smoothing in the following videos.

# **H) Python - Simple Exponential Smoothing**

Here is the same lecture rewritten in clean, normal output, lecture-style prose, with the meaning preserved and the flow clarified—no restructuring of intent, just clarity.

Alright, this is the video you’ve been waiting for—the point where we actually start doing some modeling.

To begin, we need to import a couple of functions. I’m going to import them upfront because as we move from simple to double and then to triple exponential smoothing, we’ll be using different functions. All of these live inside the statsmodels library, specifically under the time series analysis module and the Holt-Winters section.

From there, we import both ExponentialSmoothing and SimpleExpSmoothing.

Once that’s done, we’re ready to work with simple exponential smoothing. This function is very straightforward. If you look at the documentation, you’ll notice that it mainly requires the endogenous variable—that is, your time series data.

There is also an option to specify an initial level. By default, this is set to None, which means the model will infer it automatically. Conceptually, the initial level is just the first value of the time series. In our case, that happens to be 1750. It’s important to remember that while the initial level starts there, the level itself is not constant. It adapts and evolves throughout the time series.

All of the other parameters are optional, and for now, we won’t use them. We’ll focus only on passing in our time series.

So we apply simple exponential smoothing to our training data and then fit the model. That’s it—the model is now trained.

Once the model is fitted, we can inspect a summary of the results. Printing the summary gives us details such as the initial level and the smoothing parameter. You’ll see that the initial level is 1750, as expected. Again, keep in mind that this is only the starting point. The level itself is continuously updated as the model processes the data.

At this stage, it’s important not to overthink these coefficients. Exponential smoothing models are not designed to be highly interpretable. They are internal mechanisms that serve a specific purpose. The intuition is what matters: we use past values to predict the future, and we give more weight to recent observations.

For example, if the smoothing level is around 0.51, that simply means the model is weighting recent information slightly more than older information. It doesn’t carry deeper business meaning, and that’s perfectly fine.

Now let’s move on to forecasting.

We generate predictions by asking the fitted model to forecast a number of periods equal to the size of our test set. When we do this, you’ll notice something interesting: all the forecasted values are the same.

This is not a bug. This is exactly how simple exponential smoothing works.

Because the model only estimates a level—and no trend or seasonality—the forecast is flat. Each future value is based on the last estimated level. Since that level does not change once forecasting begins, all predicted values are identical.

You might be tempted to manually recreate the calculation using the last training value, the smoothing parameter, and the initial level. However, that won’t work correctly because the initial level is not the same as the final level used internally by the model. The true level is updated at every time step, and that internal value is what drives the forecast.

Next, let’s visualize the results.

We create a plot and set a reasonable figure size. Then we plot the training data, the test data, and the forecasted values. Finally, we add a title and a legend and display the plot.

When we zoom in on the results, the outcome becomes very clear. This is a terrible forecasting model.

And that’s okay.

Simple exponential smoothing is doing exactly what it is designed to do. It produces a flat forecast because it assumes no trend and no seasonality. Historically, this was one of the earliest forecasting models ever created.

Things will improve significantly when we move on to double exponential smoothing, which introduces a trend component, and then to triple exponential smoothing, which adds seasonality.

So trust the process. We are building this step by step.

If you have questions—especially about the level parameter—don’t worry too much about it. From a practical standpoint, it’s not something you need to interpret deeply. What matters is knowing how to fit the model and how to generate forecasts.

Remember: the flat forecast is expected, and it comes directly from the simplicity of the equation—a weighted previous value plus a level term.

That’s it for this video.

Until the next one—have fun.

# **I) Double Exponential Smoothing**

Let’s dive into double exponential smoothing.

You might notice the two stars next to the name and wonder what exactly makes it “double.” The answer lies in what the model is smoothing.

With simple exponential smoothing, we focused only on smoothing the data itself—essentially filtering out short-term noise. Double exponential smoothing adds a second layer. In addition to smoothing the level, it also models the trend in the data.

This means we’re no longer just averaging out highs and lows. We’re also capturing whether our sales or metrics are generally increasing or decreasing over time.

If we break down the mathematics conceptually, double exponential smoothing is built on two equations.

The first equation smooths the level. This part is very similar to what we saw with simple exponential smoothing. It represents our updated baseline and incorporates the most recent actual observation.

The second equation smooths the trend. Here, we measure how much the level changes from one period to the next. This change represents the direction and strength of the trend.

Because of this second equation, we introduce an additional parameter: beta.

Now let’s look at the components involved.

The smoothed level is our evolving baseline. It updates over time by incorporating recent actual values.

The smoothed trend captures how fast and in which direction the level is changing. It tells us whether the series is generally moving upward or downward.

Then we have alpha and beta, which act as tuning levers.

Alpha controls how much weight we give to recent observations compared to the existing level and trend. A higher alpha makes the model react more quickly to recent changes.

Beta controls how much weight we give to changes in the trend itself. A higher beta means the model adapts more quickly when the trend accelerates or slows down.

Let’s put this into context with an example.

Imagine your sales have been increasing overall, but with noticeable weekly ups and downs. Double exponential smoothing helps separate short-term fluctuations from the underlying upward movement. It allows us to see whether that growth is driven by a genuine trend rather than random variation.

As always, a word of caution.

While double exponential smoothing is excellent at capturing trends, it still does not account for seasonality. Regular patterns such as monthly or yearly cycles are not handled here. That’s something we will address next with triple exponential smoothing.

To wrap things up, double exponential smoothing smooths both the current values and the trend. It helps us understand whether we are “catching a wave” and, more importantly, which direction that wave is moving.

That’s enough theory for now.

Next, we’ll see how to implement double exponential smoothing in Python.

# **J) Python - Double Exponential Smoothing**

Now we’re going to work with double exponential smoothing in practice. We’ll build the model, generate predictions, and then visualize the results to see how well it performs.

Let’s get started.

We’ll use double exponential smoothing from statsmodels. To do that, we rely on the ExponentialSmoothing class. If you look at the documentation, you’ll see that the key input is the endogenous variable, which is simply our time series data.

We begin by building the model. We’ll call it something like model_double. The core input is our training data, which represents the observed time series.

What makes this double exponential smoothing is the inclusion of a trend component. This is where we explicitly tell the model to account for upward or downward movement over time.

For the trend type, we need to specify how the trend behaves. The most common options are additive and multiplicative.

An additive trend is typically used when the data follows a roughly linear pattern, where changes are fairly constant over time. A multiplicative trend is more appropriate when changes grow or shrink proportionally, such as exponential growth or decay.

In this case, the data appears fairly linear, so we use an additive trend.

We also explicitly set seasonality to none. That’s because we are not yet modeling seasonality—this will come later when we move to triple exponential smoothing (also known as Holt-Winters).

Once the model is defined, we fit it to the training data using .fit().

At this point, you can inspect the model summary if you want. However, in practice, the summary is usually not very insightful. The smoothing level and smoothing trend parameters don’t give us much actionable information. Exponential smoothing models are designed more for forecasting accuracy than interpretability, so it’s often fine to skip the summary altogether.

After fitting the model, we generate forecasts. We forecast the same number of periods as the length of the test set. This gives us a prediction series that we can directly compare against the actual values.

To really understand how the model behaves, visualization is essential.

We plot three things:

The training data

The test data

The forecast produced by the double exponential smoothing model

Once plotted, the behavior becomes much clearer.

What we see is that the model captures the overall trend quite well. As the trend rises and then stabilizes, the forecast reflects that movement. The predictions are smoother and more realistic than what we saw with simple exponential smoothing.

However, there is still an important limitation.

The model struggles with large spikes and dips in the data. These fluctuations are caused by seasonality, which this model does not yet handle. Double exponential smoothing accounts for level and trend, but not repeating seasonal patterns.

This explains why the forecast looks stable but fails to react to sharp periodic changes.

That missing piece—seasonality—is exactly what we’ll address next when we move to triple exponential smoothing. Once we add that third component, the model’s performance will improve significantly.

# **K) Triple Exponential Smoothing aka Holt-Winters**

Here is the same content rewritten into clear, structured, normal output, with smooth flow and no loss of meaning.

Let’s talk about triple exponential smoothing, more commonly known as the Holt–Winters method.

You can think of this as the big sibling of simple and double exponential smoothing. While simple smoothing handles level and double smoothing handles level plus trend, triple exponential smoothing is designed for data that includes trend and seasonality.

This is especially useful for patterns like customer complaints, sales, or demand data where we observe strong and repeating seasonal cycles. Ignoring seasonality in those cases leads to poor forecasts, which is exactly what we saw when using double exponential smoothing.

So how does Holt–Winters work?

In triple exponential smoothing, the time series is decomposed into three components:

Level
This is the baseline value of the series. It represents where the data is centered at any point in time and is the same concept we introduced with simple exponential smoothing.

Trend
This captures whether the data is generally increasing or decreasing over time. This component was added in double exponential smoothing.

Seasonality
This represents repeating patterns that occur at regular intervals, such as daily, weekly, monthly, or yearly cycles. This is what makes the method “triple.”

In other words:

Simple exponential smoothing → level

Double exponential smoothing → level + trend

Triple exponential smoothing → level + trend + seasonality

The Holt–Winters method uses three equations, one for each component. While we won’t go through the full mathematical equations here—since they become more complex—the intuition is straightforward.

First, the method updates the level, similar to simple exponential smoothing.
Next, it updates the trend, measuring how the level changes over time.
Finally, it adjusts for seasonality, accounting for recurring cycles in the data.

Just like the previous methods, Holt–Winters relies on smoothing parameters:

Alpha (α) controls the level

Beta (β) controls the trend

Gamma (γ) controls the seasonality

Each parameter determines how much weight is given to recent observations versus past information. These parameters can be tuned to improve model performance, but we’ll postpone parameter tuning until later in the course. For now, the focus is on understanding the structure and intuition behind the method.

From a practical standpoint, Holt–Winters becomes essential whenever your exploratory data analysis reveals strong seasonal patterns. If you see deep and regular cycles in your time series, double exponential smoothing will not be sufficient.

Common real-world examples include:

Forecasting electricity demand with daily or seasonal patterns

Predicting retail sales that spike during holidays

Modeling customer complaints that follow weekly or yearly cycles

In all these cases, incorporating seasonality is critical for accurate forecasting.

To conclude, Holt–Winters is a powerful and robust tool for modeling complex time series. By accounting for level, trend, and seasonality, it allows us to generate far more realistic and reliable forecasts.

Next, we’ll apply this method and see just how easy it is to implement in practice.

# **L) Python - Triple Exponential Smoothing aka Holt-Winters**

Now we’re going to work with triple exponential smoothing, also known as the Holt–Winters method. This technique is named after the two researchers who developed it: Holt and Winters.

At this point, we’re extending everything we’ve learned so far. We’ve already seen simple exponential smoothing (level), double exponential smoothing (level and trend), and now we’re adding the final piece: seasonality.

Let’s start by building the Holt–Winters model.

We define our model and specify the trend as additive. This makes sense because the overall movement in our data follows a fairly linear direction over time.

For seasonality, we choose a multiplicative structure. The reason for this is clear when we look at the data: as the trend increases, the seasonal peaks also become larger. This tells us that the seasonal effect grows proportionally with the level of the series, which is exactly what multiplicative seasonality is designed to capture.

Next, we specify the seasonal period. Since our data is weekly and there are 52 weeks in a year, we set the seasonal period to 52.

Once we fit the model, we immediately notice that the complexity increases. We now have smoothing parameters for:

The level

The trend

The seasonality

In addition, the model estimates initial seasonal values for each season. This added complexity is expected, because we are now modeling much richer behavior in the data.

After fitting the Holt–Winters model, we generate forecasts for the same number of periods as our test set.

To evaluate how well the model performs, we visualize the results by plotting:

The training data

The test data

The Holt–Winters forecast

When we focus on a single year—such as 2022—the improvement becomes very clear.

The model now captures not only the overall trend but also the seasonal cycles. Those recurring peaks and dips that were completely missed by simple and double exponential smoothing are now clearly reflected in the forecast.

This confirms what we suspected earlier: the missing piece was seasonality. By adding it, the model’s performance improves dramatically, even though the change in implementation is relatively simple.

The takeaway here is powerful. Incorporating seasonality can significantly improve forecasting accuracy when seasonal patterns are present in the data.

In the next step, we’ll move on to error measurement—how to quantify forecast performance and evaluate these models properly using Python.

# **M) Measuring Errors for Time Series Forecasting**

Now let’s talk about how to measure accuracy and errors in time series forecasting.

I’ll be honest—this may not be the most exciting topic, but it is one of the most important. No matter how sophisticated your model is, if you can’t measure its error, you can’t judge whether it’s actually useful.

The core idea behind error measurement is always the same, whether you’re working with regression or time series forecasting.

You have:

A model, which produces predictions (often visualized as a line)

Actual values, which represent what truly happened

The error is simply the difference between the actual values and the predictions. This difference is often referred to as the delta.

What changes is how we measure and summarize these differences.

Mean Absolute Error (MAE)

The first key metric is the mean absolute error.

Here, we calculate the difference between the actual value and the prediction, take the absolute value, and then average those values across all observations.

The absolute value is critical. It ensures that positive and negative errors do not cancel each other out.

For example:

If one error is +100 and another is −100, the average error would be zero

But the mean absolute error would still be 100

This makes MAE very interpretable. If the MAE is 2, it means that, on average, your predictions are off by about 2 units.

Root Mean Squared Error (RMSE)

The second major metric is the root mean squared error.

This metric also starts by computing the difference between actual and predicted values, but instead of taking the absolute value, it squares the differences, averages them, and then takes the square root.

Squaring the errors has an important effect:

Large errors are penalized much more heavily than small ones

This makes RMSE particularly useful when outliers matter and you want to strongly penalize large mistakes.

The downside is that RMSE is less interpretable than MAE. Once you start squaring and taking square roots, the final number doesn’t translate as intuitively into “average error.”

Comparing MAE and RMSE

Both metrics are widely used, and each has advantages.

MAE is easier to interpret and treats all errors equally

RMSE punishes large errors more strongly and is better when extreme mistakes are costly

In practice, it’s common to calculate both. However, when selecting or fine-tuning a model, RMSE is often preferred as the primary optimization metric because of its sensitivity to large errors.

Mean Absolute Percentage Error (MAPE)

The third metric is mean absolute percentage error, often abbreviated as MAPE.

Instead of measuring error in absolute units, MAPE expresses the error as a percentage.

This makes it very easy to interpret. For example, a MAPE of 10% means your predictions are off by about 10% on average.

However, MAPE has an important limitation.

It gives equal weight to all percentage errors, regardless of scale. For example:

Predicting 100 with an error of 10 results in a 10% error

Predicting 10 with an error of 1 also results in a 10% error

In many real-world scenarios, being off by 10 units on a large value is more significant than being off by 1 unit on a small value. MAE and RMSE reflect this difference, but MAPE does not.

Because of this, MAPE can sometimes be misleading, especially when values vary widely in magnitude.

What Is the Ideal Error?

This is one of the most common questions—and the answer is simple: there is no universal ideal error.

The acceptable level of error depends entirely on:

The business context

The use case

The tolerance of stakeholders

What matters most is improvement over time. As you use better data, better features, and better models, your error should decrease.

Determining whether an error is “good enough” is ultimately a business decision and should be discussed with stakeholders and senior decision-makers.

# **N) Python - MAE, RMSE, MAPE**

In this video, we’re going to do a very important task: analyzing our outcomes. Specifically, we’re going to measure the errors produced by our time series forecasting model.

To do this, we need a few specific functions. From sklearn.metrics, we’ll import the metrics required to evaluate our predictions. These include the root mean squared error (RMSE), the mean absolute error (MAE), and the mean absolute percentage error (MAPE).

Initially, there may be some confusion because earlier versions of sklearn did not include a direct function for RMSE. In those cases, people used the mean squared error and then applied a square root manually. However, now RMSE exists directly, so we can use it without additional tricks. Throughout the course, you may still see some references to mean squared error because it was commonly used before RMSE was added.

The key idea behind all these metrics is the same: we compare y_true (what actually happened) with y_pred (what our model predicted). This structure is always the same across all sklearn metrics. No matter which error metric you use, you pass in the actual values first and the predicted values second.

Once the functions are imported, we calculate and print RMSE, MAE, and MAPE using our test data and predictions. When we do this for our current model, we get results such as an MAE of around 366, an RMSE of around 424, and a MAPE of about 8 to 8.5%.

At this point, it’s useful to control how many digits we display. In practice, whether RMSE is 424.5 or 430 doesn’t really matter much. What matters is the scale of the error, not tiny decimal differences. So we usually limit the number of decimal places, especially for readability.

The same idea applies to MAE. For MAPE, since it represents a percentage, it makes sense to multiply it by 100 and display it as a percentage. We can also limit it to one decimal place, which is more than enough. For example, reporting a MAPE of 8.5% is much clearer and more useful than showing many decimal places.

Once this setup is done, we can start experimenting with different model configurations. For instance, we might switch the seasonality from multiplicative to additive and observe how the metrics change. When we do this, the results turn out to be much worse—MAPE jumps to around 12.7%, MAE increases to about 867, and RMSE to around 622. This clearly indicates that additive seasonality performs poorly for this dataset.

We can also experiment with different trend configurations, such as using a multiplicative trend. In this case, we might see a runtime warning during internal calculations, indicating numerical issues. However, despite the warning, the resulting error metrics may actually improve. This highlights an important point: we should be results-driven. If a model produces better evaluation metrics, it may still be worth considering, even if there are warnings, as long as the results are stable and sensible.

That said, these results are based on a single test set. Later on, we’ll cover cross-validation, where we measure errors across multiple test sets to ensure the model generalizes well.

The final step in this video is to build a reusable function that assesses the model and visualizes the results. This function takes the training data, test data, predictions, and an optional chart title. The title is not mandatory, allowing the function to be reused easily in different contexts.

Inside this function, we plot the training data, test data, and forecasted values on the same chart. We then add a title (if provided), a legend, and display the plot. After that, we calculate RMSE, MAE, and MAPE using the same metrics as before and print the results.

Once the function is defined, we can apply it directly by passing in the train set, test set, and predictions. This gives us both a visual comparison and numerical evaluation in one step. If we want to zoom in on a specific time range—such as starting from 2022—we can adjust the inputs accordingly. When we do this, we see very strong results, confirming that the model is performing well.

And that’s it for this video. This is how you evaluate a time series forecasting model, both visually and quantitatively. You now have a reusable function that you can apply to future models and datasets.

# **O) Python - Predicting The Future**

Sure 🙂
Here is the full explanation rewritten end to end, organized into clear multiple paragraphs, keeping all details intact and improving readability—without removing or adding meaning.

Welcome back. Let’s now move on to predicting the future.

At this stage, the idea is simple. Once you are satisfied with your modeling—once you’ve evaluated the errors, explored different configurations, and decided that the model is “good enough”—you move forward and use it to forecast future values. Throughout this course, we’ll continue to explore how models behave under different scenarios, but for now, we’ll take a straightforward approach.

The question we ask ourselves is: Are we satisfied with this model?
If the answer is yes, then we proceed to forecasting.

This step is meant to be easy. Since we’ve already compared models and visually inspected performance, we’re going to build a Holt-Winters model. The reason we choose Holt-Winters is simple: visually and quantitatively, it performed the best.

We build the Holt-Winters model using the complete dataset. We start by inspecting our DataFrame—just taking a quick look at the first few rows using .head() rather than displaying everything. From this DataFrame, we extract the target variable y and feed it directly into the model.

At this point, we choose between additive and multiplicative components. Based on our earlier analysis, multiplicative seasonality is already top of mind. There’s no need to overthink this here, especially since parameter tuning will be covered extensively later in the course.

To keep things simple, we copy the parameters we already identified as good and plug them into the model. Once the model is fit, we generate predictions and preview the first few forecasted values to confirm that everything looks reasonable.

Initially, it might seem like the predictions haven’t changed, which can be confusing. But this turns out to be a simple oversight—the model parameters were not actually changed. After rerunning everything correctly, it becomes clear that using additive seasonality results in a significant deterioration of performance, with MAPE jumping to around 12.7%.

This reinforces our earlier conclusion: multiplicative seasonality is the correct choice. It also aligns with what we observed visually—the seasonality in the data is clearly multiplicative.

Once the model is finalized, we proceed to forecasting. We generate a forecast for 13 periods ahead. This number is not random; it matches the length of the test set we used earlier. This alignment makes business sense—if we tested on 13 weeks, then predicting 13 future weeks is a logical next step.

After generating the forecast, we preview the first few values and then plot the training data alongside the forecast. Using plt.plot, we plot the historical y values and then overlay the forecast. This gives us a clear visual representation of how the model projects future behavior.

To make this reusable, we wrap the plotting logic into a function. We define a function called plot_future, which takes three inputs: the historical y values, the forecast, and an optional chart title (defaulting to None). Inside the function, we plot the data, apply the title if provided, and display the chart.

We then call this function using our DataFrame’s y, the forecasted values, and a title such as “Holt-Winters”. Just like before, if we want to zoom into a specific time range—say, from 2022 onward—we can apply .loc[2022:].

At first, this zooming doesn’t work as expected, which reveals a small bug in the function. The issue is that the function parameters were not fully dynamic. After correcting this and passing y properly, everything works exactly as intended.

In the end, it turns out that the model was right all along—the issue was simply in the function definition.

With that, we’re done. We’ve successfully predicted the future using our Holt-Winters model, and we’ve seen that doing so is actually very straightforward once the model is finalized.

Next, we’re going to shift focus to daily data, which is one of the most common and most challenging types of time series data. Daily data comes with its own quirks and considerations. To explore this properly, we’ll work with a Bitcoin dataset and see how forecasting works in that context.

# **P) Python - Daily Data**

We’ll start by loading the Bitcoin price dataset. To do this, we use pandas.read_csv and load the file named Bitcoin_price.csv. While loading the data, we set the index column to date and enable parse_dates=True. This ensures that the date column is properly recognized as a datetime index, which is exactly what we want for time series analysis.

From the dataset, we only need one KPI. Although there are many available, we’ll focus on the adjusted close price. This allows us to zoom in on exactly what we need without unnecessary columns. We store this series in a variable called daily_data. For convenience, we can also rename the series so it’s easier to work with going forward. If desired, all of this could even be done in a single line of code.

Once the data is loaded, it’s always good practice to do a quick preview to confirm everything looks correct. After that, we check the index. One important thing to verify with daily data is whether the frequency is properly set. Often, the frequency is missing, even if the dates are continuous. To ensure completeness and avoid issues later, we explicitly set the index frequency to daily. After doing this, the data is exactly in the format we want.

Next, we move into model assessment. We decide to set aside the last 30 days as our test set. We define periods = 30, then split the data accordingly. The training data includes everything except the last 30 observations, and the test data consists of those final 30 days.

At first, an error appears indicating “too many indexers.” This happens because we’re working with a Series rather than a DataFrame. Once we adjust the slicing accordingly, everything works as expected. We quickly verify the training and test data and confirm that the test set starts on December 1st, which is exactly what we want.

With the data split correctly, we proceed to build a Holt-Winters model for daily data. This is where an important discussion comes in. Daily data often contains multiple seasonal cycles. However, exponential smoothing—specifically Holt-Winters—only allows for one seasonal cycle, so we must choose carefully.

At this point, we need to decide what seasonal period makes the most sense. Should it be 365 days for yearly seasonality, or 7 days for weekly seasonality? In many cases, weekly (intra-week) seasonality is stronger and more important, so it’s common to start with a seasonal period of 7. Yearly seasonality can also be meaningful, but only if it’s very clearly defined. Ideally, we’d capture both—but Holt-Winters doesn’t allow that. More advanced models will, and we’ll explore those later.

We start by fitting the model to the daily data. During fitting, we see a convergence warning. This warning essentially tells us that the model is struggling to fit the data well. We can experiment by changing the trend type, for example trying a multiplicative trend, but we still see warnings. This isn’t entirely surprising—cryptocurrency data is extremely volatile and inherently difficult to predict.

Despite this, we proceed to generate predictions using the Holt-Winters model. Even here, the convergence warning persists. This reinforces an important reality: crypto prices are noisy, volatile, and very challenging for traditional forecasting models.

Next, we assess the model using our reusable model assessment function. We pass in the training data, test data, and the daily predictions. We also zoom in on the test period to better visualize the results.

Interestingly, the MAPE comes out at around 4.7%, which looks quite good at first glance. However, when it comes to stock or crypto prices, even small errors can be problematic. Trading involves taxes, transaction fees, slippage, and opportunity cost. In practice, errors often need to be closer to 1–2% to be truly useful.

This is a good moment for an important caution. If someone claims they can reliably predict the stock or crypto market, you should be extremely skeptical. Markets are influenced by countless unpredictable factors, and no model can consistently forecast them with high accuracy.

Visually, the results look okay. We see the test data alongside the forecast, and the model does a reasonable job of following the general movement. When we zoom further into the period starting from late November 2023, the performance still appears acceptable.

However, from a practical standpoint, this is where things become tricky. You might see the forecast indicating an increase, decide to buy, and then wait several days only to find that the price never reaches the predicted level. By the time the forecast turns downward, you’ve already missed opportunities or incurred losses. This highlights just how difficult real-world trading decisions are.

To further experiment, we change the seasonal period to 7 days and refit the model. Immediately, the results look very different. The MAPE increases to around 5.2%, and the forecast line changes significantly. This suggests that there isn’t a strong weekly seasonal cycle in the data, which makes sense—traditional stock data does not have a weekly cycle, and Bitcoin’s weekly behavior is debatable at best.

This experiment highlights an important takeaway: changing the seasonal period can drastically alter the model’s behavior. When using Holt-Winters, it’s crucial to experiment and let the data guide your decisions rather than relying on assumptions.

As we move forward, we’ll explore more advanced models that allow for multiple seasonalities and greater flexibility. That’s where things get really exciting. If this already feels interesting, the upcoming models will open up even more possibilities.

# **Q) Python - Working on the Useful Code Script**

Welcome back! To wrap up this video, let’s focus on building on our template. We’ll go to our main template file, specifically the useful code template, and start enhancing it. One important addition we can make is including extra libraries. I’ll add everything initially and then remove what’s unnecessary. For example, I’ll remove the Holt-Winters-specific libraries, since they are very model-specific. Everything else stays, and that gives us a clean, reusable template.

Next, we ensure proper renaming and organization. This includes adding placeholders like XXX for parts that may need future adjustments. Frequency handling is another key part—at some point we worked with time series frequency, so it’s useful to include that as a function in the template. This allows you to change the frequency dynamically, rename your time series to Y, set the index when importing data, and easily plot the series. These improvements make the template more flexible and user-friendly.

We also aim to improve the plotting and documentation. By passing the plotting and template sections through an AI or a documentation improvement step, the template becomes cleaner, easier to read, and more professional. The goal is to have a ready-to-use template that you can adapt for various projects while keeping the code organized and comprehensible.

Next, we include useful functions like model_assessment. Adding these directly into the template ensures that you can evaluate models, visualize train/test/forecast splits, and calculate error metrics quickly, without rewriting the code each time. One limitation I noticed in Colab is that building a simple library of reusable functions isn’t as smooth as in Jupyter or other environments. In Jupyter, you can build a lightweight library over time and import it easily. Colab doesn’t make this as seamless yet, but including functions in a template is a practical workaround.

We also keep the plot future function in the template. This allows for easy visualization of forecasts whenever we want to predict future data. By having both model_assessment and plot_future in the template, you now have a foundation for end-to-end time series analysis: importing data, preprocessing, building models, assessing them, and forecasting.

Finally, we save this as useful_code_template_final. This final version of the template is clean, reusable, and ready for use whenever you start a new time series project. It’s flexible enough to adapt as your workflow evolves, and over time you can continue to improve it. I hope you had fun building this with me, and I’m looking forward to seeing you in the next video!

# **R) Holt-Winter Pros and Cons**

Hey everyone! In this video, I’m going to walk you through the pros and cons of the Holt-Winters method, breaking it down into key advantages as well as its limitations.

As a general introduction, Holt-Winters is a favorite in the forecasting world. If you have a problem that isn’t overly complex but exhibits trend and seasonality, Holt-Winters does a very good job. Its first advantage is that it is simple to implement. The method is straightforward—you don’t need a PhD to get it up and running—which makes it accessible for many people. I also find it quite intuitive: the model’s logic, which revolves around the current level, the trend, and the seasonality, resonates easily.

The parameters we discussed—alpha, beta, and gamma—exist in the model, but for a simple implementation, we often don’t even need to tune them. This simplicity makes the model highly adaptable to changes. Because the method emphasizes recent past observations, it naturally adjusts to shifts in trends and patterns, making it responsive to new data.

However, there are some limitations to be aware of. First, Holt-Winters has only one seasonal component. For daily data, for example, you might need to choose between weekly or yearly seasonality, but it cannot account for both at the same time. This limitation is shared with older methods, including models from the ARIMA family, which can struggle with complex time series.

Another limitation is that Holt-Winters cannot incorporate external regressors. Factors such as weather, market conditions, or major events like Covid cannot be used to refine forecasts. The method relies entirely on historical time series data, which may not always be sufficient for accurate prediction in real-world scenarios. When multiple seasonalities or external drivers are important, Holt-Winters can fall short.

In conclusion, Holt-Winters stands out as a reliable and quick-to-use forecasting method. You can put it in place easily and quickly gauge how predictable your data is. That said, it’s important to recognize that it may not always be a perfect fit. For complex scenarios with multiple seasonalities or external influences, you may need more advanced models.

That wraps up this section. Until the next video, have fun exploring time series forecasting!

# **VII) Section 7: HOLT-WINTERS CAPSTONE PROJECT: Air miles**

# **A) Capstone Project Presentation**

Welcome to this capstone project.

In this video, I’m going to walk you through the six tasks that you need to complete. To take on this challenge, you essentially have two options for your working environment. You can either use your own setup—something like Google Colab, which is what I’ll be using—or you can use the Udemy workspace that’s provided. The Udemy workspace is mainly there so you can review and work through the tasks themselves, but all the necessary information is also explained here.
Alright, let’s get started.

The challenge you’ll be working on is centered around air miles data. It’s a very straightforward dataset that contains the number of air miles accumulated per month over a period of roughly ten years. There are six main tasks that you need to complete as part of this project. Along with that, you also have access to starter code, which is provided in the workspace. This starter code is also available to you outside the workspace, and it’s essentially the same in both places.
If you decide to use Google Colab, the main extra step you’ll need to take is setting up the environment. If you’re using another setup, that may not be necessary. In any case, you’ll find the required libraries, the dataset itself, and a few initial code snippets that are meant to help you get started. All of this is also preconfigured in the Udemy workspace, along with the dataset.
Now, let’s go through the tasks one by one.
The first task is to set the data frequency. This is clearly indicated in the instructions. You need to specify the frequency as monthly start. Completing this correctly is your first task.

After that, the second task is to visualize the data. The main purpose here is to really become familiar with the dataset—think of this as exploratory data analysis. In my own solution, I’ll be doing this step by step. For your part, you can keep it simple if you want, such as basic plots. You can also go further and include seasonal decomposition, autocorrelation and partial autocorrelation plots, monthly plots, quarterly plots, or a combination of these. I would recommend doing as many as possible, because each of these visualizations provides complementary insights into the data.

The third task is very straightforward. You need to split the data so that the last 12 months are used as the test set, and all the remaining data is used for training. The idea behind this is to have an unbiased way to evaluate the model. Typically, the test set should consist of the most recent observations, because that’s where, in real-world scenarios, we most want our forecasts to be accurate.

Task number four is where you build the model. Here, you’ll create a Holt-Winters model. You’ll need to specify the type of seasonality—whether it’s additive or multiplicative—set the seasonal period, and then fit the model to the training data.

Task number five is about forecasting. Using the fitted model, you’ll predict the next 12 months of air miles. You’re also encouraged to visualize the forecasted values along with the historical data, if you’d like, to better understand how the model is behaving.

Finally, the sixth and last task is model evaluation. Here, you assess how well your model performed. You can use mean absolute error, or you can use the three commonly used metrics: root mean squared error, mean absolute error, and mean absolute percentage error. You may also choose to visualize the errors, but the key objective is to measure the model’s error and clearly present it to the user.

So, to summarize, there are six tasks in total. You’ll find the solutions for these tasks in the task folder, with separate code files for task one through task six. There is also an overall script that brings everything together.

And that’s it. I hope everything is clear. If you have any questions, feel free to ask them in the next video. In the next session, I’ll start from the starter code and solve the capstone project step by step with you. I’ll see you in the next video.

# **B) Python Solutions: Task 1 and Task 2**

All right, let’s solve this together.

I went ahead and ran the initial setup. I set the directory in my Google Colab environment, and inside it we can find our data file, air.csv. I also set the index to be the date column, enabled date parsing, and specified dayfirst=True. What we now have is monthly data ranging roughly from 1996 to 2005, with about 113 entries in total. I then renamed the main series to Y.

Now let’s take a look at the tasks.

The first task is to set the data frequency and use MS (monthly start). Let’s give this a try. For task one, we set the index to monthly frequency using MS. That’s the very first step. I apply this directly on the DataFrame index, and once that’s done, everything looks good and ready to go.

With task one completed, we can move on to task two, which is data visualization and exploratory data analysis. The goal here is really to understand the data deeply.

The first visualization is a simple time series plot. We plot dataframe.Y and display it. After adjusting the figure size to make it clearer, we can immediately see a few important patterns. There’s a clear upward trend over time, and the data also appears to be strongly seasonal.

If we look closely, there’s a noticeable dip around 2001. This aligns with the September 2001 terrorist attacks, which understandably reduced people’s willingness to fly. This real-world event is clearly reflected in the data.

Next, we build a monthly plot. We plot the monthly seasonality of dataframe.Y and set the y-axis label to “Monthly Air Miles.” From this visualization, the seasonality becomes very clear. Air miles are highest roughly between March and August, while the lowest values appear between September and February.

To further confirm this pattern, we move on to a quarterly plot. We resample the data by quarter and then plot it. Initially, we run into a small issue because resampling requires an aggregation function, so we apply a sum. Once that’s fixed, the plot works as expected. From this view, Q1 shows the lowest values, Q4 is slightly higher, and Q3 has the highest seasonal peak.

Next, I like to perform seasonal decomposition. Since the data shows that as the trend increases, the magnitude of the seasonal fluctuations also increases, it suggests that a multiplicative model is more appropriate than an additive one. We perform seasonal decomposition with a period of 12 and set the model to multiplicative.

When we plot the decomposition, we can clearly see the upward trend, a dip around 2001, and then a recovery afterward. The seasonal component fluctuates roughly between 0.9 and 1.1, which means about plus or minus 10 percent seasonality. The residuals are centered around 1, which indicates a very reasonable and clean decomposition. Overall, this looks like a good fit for the data.

There are two more plots I want to include here. The first is the autocorrelation plot (ACF). When we plot the ACF for dataframe.Y, we see strong peaks around lag 12, as well as at multiples of that lag. This clearly indicates strong seasonality. We also see that most correlations die off quickly, which suggests that there isn’t an extremely strong long-term trend component.

Finally, we look at the partial autocorrelation plot (PACF). Here, we see significant information around lags of 6–7 months and again around 12–13 months. This also aligns very well with the seasonal nature of the data and further confirms our earlier observations.

With that, we’re done with task one and task two. In the next step, we’ll move on to task three, which is creating the training and test sets. I’ll see you in the next video.

# **C) Python Solutions: Task 3 and 4**

The next step is to move on to task three, which is creating the training and test sets.

For this task, we’re going to define the test set as the last 12 months of data, and the training set as everything that comes before that.

Let’s get started by setting up the test set. We take the DataFrame and slice it so that everything up to index -12 becomes the training data, and from -12 to the end becomes the test data.

This approach works quite well and is very clean. Once this split is done, task three is effectively completed, because that is all the task requires.

Next, we move on to task number four, which is to build the Holt-Winters model.

In this task, we build the Holt-Winters model using the time series data from the training period only. It’s important to note here that Holt-Winters does not allow external regressors, so we are working purely with the time series itself.

Now let’s build the Holt-Winters model. We create the model using exponential smoothing and pass in dataframe.Y as the input series.

For the seasonality, we initially consider a multiplicative form because earlier analysis suggested multiplicative seasonality. However, when we look at the trend more closely, it appears to be fairly linear.

Since linear trends typically work well with additive seasonality, we decide to go with additive seasonality instead.

After setting up these parameters, we fit the model by calling .fit(). Once this runs successfully, the model is built.

At this point, task four is complete. We’ll stop here for now and come back in the next session to wrap things up with tasks five and six. I’ll see you then.

# **D) Python Solutions: Task 5 and 6**

Task five is focused on forecasting.

In this step, we are going to generate predictions using the model we built earlier. Since task six is about accuracy assessment, we first need forecasts to compare against the test data. Specifically, we are going to predict the next 12 periods using the trained model.

We start by taking the fitted Holt-Winters model and calling the forecast method. To keep things clean and flexible, the number of steps is set automatically based on the length of the test dataset. This ensures the forecast aligns perfectly with the test period. We then store these predictions and label them clearly as Holt-Winters predictions.

At this point, we have our forecast values, which means we are ready to move on to task six: accuracy assessment.

For evaluating the model, we compute error metrics. You can absolutely use pre-built metric functions, and they work quite well. Over time, though, I tend to rely less on them. The reason is that in environments like Colab, especially when working alongside large language models, natural-language-driven exploration often feels more intuitive and flexible.

We assess the model using MAPE and RMSE as our primary metrics. After computing them, we display the results to the user. The MAPE turns out to be around 0.03, which corresponds to roughly a 3% error. That’s an extremely small error and indicates that the model is performing very well.

Next, we visualize the results. We plot the training data, the test data, and the model’s predictions on the same graph. When we look at the plot, we can clearly see that the predicted values are very close to the actual test values. This visual confirmation reinforces what the error metrics already told us.

You’re welcome to customize this plot further—styling, colors, labels, and so on—but even in its basic form, it communicates the model’s performance quite effectively.

Now, let’s do one final bonus step: forecasting the future. After all, this is really the main reason we build forecasting models in the first place.

To do this, we rebuild the Holt-Winters model using the complete dataset, not just the training portion. We fit the model on all available data and then forecast the next 12 months into the future. These become our future predictions.

Finally, we visualize the full historical data together with the future forecasts. This gives us a clear picture of how the model expects the series to evolve going forward.

This wraps up how we assess the model and how we generate future forecasts. Feel free to experiment with the model settings we defined earlier in task four—such as trend and seasonality types—and see how they impact accuracy. Exploring these variations is something we’ll dive into more deeply later in the course.

For now, the key takeaway from this project isn’t just the Holt-Winters model itself. It’s the full workflow: setting the correct frequency, performing solid exploratory data analysis, splitting the data properly, and evaluating results thoughtfully. These building blocks are the real foundation.

And with that, we’re done with the project. If you have any questions, let me know, and I’ll see you in the next video.

# **VIII) Section 8: ARIMA, SARIMA and SARIMAX**

# **A) Game Plan for ARIMA, SARIMA and SARIMAX**

In this section, we’re going to learn some very important foundational models in time series analysis: ARIMA, SARIMA, and related variants. These are models you absolutely need to understand. Alongside them, we’ll also master several core concepts that form the foundation of time series forecasting. Everything we’ve learned so far leads into this part of the journey.

This phase of time series analysis is really about strengthening your forecasting skills before we move on to more complex models. If you want to truly understand ARIMA and its extensions, you’re definitely in the right place. ARIMA stands for Autoregressive Integrated Moving Average, which is quite a mouthful. On top of that, we also have SARIMA and ARMAX, and we’re going to learn all of them, model by model.

We’ll initially put most of our focus on ARIMA, because it contains the majority of the core concepts you need to understand time series modeling. Once you truly understand ARIMA, moving on to SARIMA and ARMAX becomes much easier. For more advanced time series forecasting, especially when seasonality and external factors are involved, we’ll rely more heavily on SARIMAX. But again, if ARIMA makes sense to you, the rest will feel very natural.

It’s really important that by the end of the ARIMA section, you feel confident with it. If you don’t, come to me and I’ll help you. Getting ARIMA right makes everything that follows much simpler and more intuitive.

Of course, we won’t rely on ARIMA alone. ARIMA is the simplest of these models, and as we’ll see, it has limitations. In real life, most data is seasonal. A classic example is ice cream consumption—you don’t eat ice cream all year long. That’s the perfect illustration of why we need seasonal models like SARIMA. On top of that, SARIMAX allows us to incorporate external factors, which are often crucial.

To make this concrete, imagine I live in Berlin. It’s summer while I’m recording this, but it’s raining. Do I want ice cream? Absolutely not. Even though it’s summer, the external factor—rain—changes my behavior. That’s exactly why models with exogenous variables are so powerful.

Throughout this section, we’ll apply these models to real-world data. We’ll explore each model carefully, understand when to use which one, and learn how to choose the best model—typically the one with the highest accuracy. We’ll also spend time tuning our models using techniques like cross-validation and parameter optimization. We’ll really go into the details here.

There’s one more important thing I want to point out. This is probably the third or fourth iteration of this SARIMAX section. The course was launched a few years ago, and I’ve continuously updated it. This is actually one of the biggest updates so far. Previously, I was using the PMDARIMA library quite heavily. Unfortunately, its last update was in 2023, and parts of it no longer work properly, especially in Colab environments.

Because of that, we’re moving away from PMDARIMA. If at any point in this section something looks odd or broken, please let me know. I do my best to keep everything up to date, but I’m not perfect. I’ve remade almost all of the Python-related videos from scratch, and I’ve updated the others where it made sense. Some parts remain unchanged simply because ARIMA itself dates back to the 1960s, and the core ideas haven’t really changed.

Going forward, we’ll be using statsmodels, which we’ve already used earlier for things like seasonal decomposition and plotting. It’s a very strong and reliable library for time series analysis. While the older library had some convenient extra features, moving on is part of the natural evolution of the course—and that’s perfectly fine.

I’m genuinely excited about this section. We’re going to explore these models in depth, really push our understanding, and make sure you’re set up for success. Most importantly, I want you to succeed, and I’ll be here to help you every step of the way.

If you have any questions, let me know, and I’ll see you in the next video.

# **B) CASE STUDY BRIEFING: Predicting Daily Revenues**

In this video, I’ll introduce you to the case study for this section.

Imagine this scenario: you are the owner of a chocolate retail shop, and your goal is to predict your daily revenue. You might ask why this is so important. The answer is simple—understanding and managing cash flow is critical for any business. If you know how much you are likely to earn, you can plan in advance how much stock you need and how many people should be working in the store.

When you picture yourself running this shop, it becomes clear why forecasting matters. Every single day requires planning. Forecasting allows you to prepare for what’s coming, so your shop can stay one step ahead instead of reacting at the last minute. By predicting daily revenue—whether it’s for the next day or the next 30 days—you can make much smarter decisions.

This directly leads to fewer unsold chocolates and better staffing decisions. You don’t want excess inventory sitting on the shelves, and you also don’t want employees standing around without work. Idle staff means wasted money, which is a loss for the company. Accurate forecasts help ensure that people are working when they are actually needed.

As a result, inventory management also improves. You order the right amount of chocolate at the right time, reducing waste and improving efficiency. Overall, this means the business runs more smoothly—and in this case, a bit more sweetly as well.

So what is your role in this case study? You are acting as an analyst or data scientist. Your goal is to analyze past sales data, identify patterns, and forecast future sales. You can also take special events into account, such as weekends or holidays. For example, Valentine’s Day is a huge event for chocolate sales, and ignoring it would lead to poor forecasts.

This case study is especially exciting because it clearly shows how data can influence real business decisions. We’re not just applying algorithms for the sake of it. We’re learning about consumer behavior, understanding seasonality, and finding the right balance between demand and supply.

By the end of this case study, you’ll see firsthand how to forecast the future using ARIMA, SARIMA, SARIMAX, and external regressors. You’ll understand how these models can be applied in a real-world scenario, in a way that closely mirrors what you would do in a day-to-day job as a data professional.

Most importantly, you’ll see just how relevant and impactful this skill is. Forecasting isn’t just theoretical—it can directly affect business performance and decision-making.

That’s it for the introduction. I hope this motivates you. Let’s get started, and I’ll see you in the next video. Have fun!

# **C) Python - Setting Up ARIMA**

Welcome to this first tutorial on ARIMA.

Inside the Time Series Analysis section, you’ll find a folder dedicated to ARIMA, SARIMA, and SARIMAX. In that folder, you’ll also find a starter file. This is a very simple setup that I prepared for you, containing some of the functions we’ve already built and used earlier.

One of the first things you’ll notice is that we connect to the drive. This is something you may need to adjust depending on your setup. You’ll need to mount your drive and possibly change the folder path, but by now you should already be comfortable doing that.

Let’s briefly review the functions and libraries used here. We import pandas and matplotlib, and we also bring in several plotting and modeling utilities from statsmodels. Some of these, like seasonal decomposition, we’ve already used. Others are commented out for now and will be activated later when we need them, such as SARIMAX. We also see imports like ParameterGrid, which we haven’t used yet, and NumPy, which is a standard dependency you’ll see often.

After running the setup, we load the chocolate sales dataset. The index column is the date column, which is why it’s set as the index. We also enable date parsing. However, there’s an important additional detail here: the date format. Because the dates in the CSV file are day-first, we explicitly set dayfirst=True. This ensures the dates are interpreted correctly.

It’s very important to always double-check the date format. If the date is parsed incorrectly, everything downstream—seasonality, trends, forecasts—will be wrong. In this dataset, the original date format could easily be misinterpreted as month-first, so correcting this is a crucial step.

The dataset contains revenue, discount rate, and coupon rate. These are fairly straightforward. The discount rate refers to the visible discount applied to a product, while the coupon or voucher rate is applied later, typically at checkout. These are different mechanisms and often influence customer behavior in different ways.

To make this more concrete, think of an online store like Amazon. A discount might be displayed directly on the product page, such as “15% off,” while a coupon might apply an additional percentage reduction during checkout. These two concepts are handled differently and can have different impacts on sales.

Some preprocessing has already been done in the dataset. For example, the revenue column originally contains commas, which causes it to be read as an object rather than a numeric type. To fix this, we remove the commas from the strings and then convert the values to floats. Once that’s done, the revenue column becomes numeric, which is exactly what we want.

Next, we set the data frequency to daily and rename the main target variable to Y. Setting the correct frequency is always important in time series analysis. We also briefly review the possible frequencies that could be used, depending on the dataset.

Now we move into exploratory data analysis. When we visualize the data, we can see that revenue is generally growing over time. There are also clear spikes at certain times of the year, which strongly suggests seasonality. These spikes repeat year after year, reinforcing the idea that seasonal effects are present.

Looking at the monthly plot, we can clearly see the seasonal ups and downs. Revenue tends to be higher in the second quarter, lower in the first and third quarters, and then rises again in the fourth quarter—especially in November, which shows a very strong seasonal spike.

The quarterly plot confirms this observation. Q1 and Q3 are relatively similar, Q2 acts as a transition, and Q4 consistently shows higher values. This kind of confirmation across multiple plots is exactly what we want during EDA.

We then perform seasonal decomposition using a multiplicative model. The reason for choosing multiplicative seasonality is the presence of large spikes—the size of the seasonal effect grows with the level of the series. The trend component shows steady growth that eventually stabilizes, while the seasonal component exhibits strong recurring spikes. The residuals still contain some large values, indicating that there is room for improvement in the model.

Next, we look at the autocorrelation function. This clearly shows strong seasonal behavior, with spikes at regular intervals such as 7, 14, and 21 days. These repeating patterns confirm that the data is deeply seasonal and that a seasonal component will be essential in our modeling.

The partial autocorrelation plot provides even more insight. We see strong relationships with values from one day ago, two days ago, five days ago, seven days ago, and even up to two weeks back. This again highlights the strong seasonal and short-term dependencies in the data.

At this stage, we may start guessing initial parameters for our models, such as autoregressive and seasonal terms. However, it’s important to stress that visualization is just the starting point. These guesses are not final decisions.

The key takeaway is that while visualization helps us build intuition, our final parameter choices should be driven by data and metrics, not just visual inspection. As we progress through the tutorial, we’ll move away from guessing and toward data-driven decisions. This brings clarity, confidence, and a much stronger foundation for our models.

We’ll stop here for now. There’s a lot more to learn, and the next steps will be both detailed and fun. I’ll see you in the next video.

# **D) ARIMA**

In this video, we are going to cover the ARIMA concepts, and don’t worry if it feels a bit complicated at first. I’ll break everything down in a very simple and intuitive way so you can follow along comfortably.

ARIMA stands for Autoregressive Integrated Moving Average, and it is one of the most popular models in the forecasting world. The reason for its popularity is that it is relatively easy to apply, and the intuition behind it is also straightforward once you understand the basics.

Apart from ARIMA, there is also an extended version that deals with seasonality. This additional layer makes the model extremely useful for data that follows seasonal patterns, such as monthly sales, yearly demand, or weekly traffic. On top of that, ARIMA can also be enhanced by including external factors, known as exogenous regressors, which allow us to incorporate outside influences into our forecasts.

Although all of this might sound complex at first, the good news is that ARIMA is actually very easy to work with in practice. For this particular video, however, we will focus only on the core ARIMA model.

ARIMA has three main components.

The first component is Autoregressive. This part is essentially about looking at the past to predict the future. In other words, we use previous values of the time series to forecast what comes next. If past values have a strong influence on future values, this component captures that relationship.

The second component is Integrated, and this is where stationarity comes into play. A stationary time series is one where the mean, variance, and covariance remain constant over time. Simply put, stationarity means the data follows a consistent pattern that can be predicted. The integrated part of ARIMA helps us transform non-stationary data into stationary data, usually through differencing. We will cover this topic in detail, and in the practice videos, we will place special emphasis on stationarity and the integrated component.

The third and final component is the Moving Average, and this part is quite clever. Instead of only using past values, the model also looks at past errors. Yes, the mistakes the model made in earlier time steps are treated as valuable information and are used to improve future predictions.

From a more mathematical perspective, an ARIMA model is typically written as an equation. In this equation, y(t) represents the value of the time series at time t, which is what we are trying to forecast. The alpha term is the constant or intercept, acting as a baseline starting point. Then we have coefficients for the autoregressive part, which indicate how much influence previous values such as y(t−1) have on the current forecast. We also include coefficients for the moving average part, which determine how much influence past errors should have.

Finally, there is the error term, which represents everything that is not explained by the constant, the autoregressive terms, or the moving average terms.

Even though this may seem mathematically heavy, the core idea is actually simple. A forecast at time t is made up of a baseline value, plus information from the most recent past values, plus information from the most recent errors.

In the next videos, we will zoom in on each of these components individually and build a strong intuition around them.

# **E) Auto-Regressive**

After we have a good grasp of ARIMA, let’s zoom in on each of its components, starting with the Autoregressive part, or AR for short.

In many ways, the autoregressive component is the heartbeat of ARIMA. Understanding it properly is absolutely crucial if you want to understand ARIMA as a whole. So let’s break it down step by step and see what it’s really about.

The core idea behind autoregression is very simple: the past influences the present. Imagine you are trying to predict how much coffee you’ll drink tomorrow. A very reasonable place to start would be to look at how much coffee you drank over the past few days. If you drank a lot yesterday and the day before, chances are you’ll drink a similar amount tomorrow. This intuition is exactly what the AR component captures.

In technical terms, the AR component looks at past values of the time series and uses them to predict the next value. In our coffee example, past daily coffee consumption helps us forecast tomorrow’s consumption.

From a more mechanical perspective, the autoregressive model works using lags. A lag is simply a previous data point in the time series. You can think of a lag as a step backward in time. A lag of 1 means we are looking at yesterday’s value to predict today. A lag of 2 means we are looking at the value from the day before yesterday, and so on.

Within the ARIMA framework, the autoregressive part is represented by the P in the model notation. The value of P tells us how many lagged values of the series we are going to use. For example, if we choose an ARIMA model with P = 2, it means we are using the last two observations—lag 1 and lag 2—to predict the future.

In this case, the model would look something like this: today’s coffee consumption is equal to a constant term (often called alpha) plus a coefficient multiplied by yesterday’s consumption, plus another coefficient multiplied by the consumption from the day before yesterday, and so on. The number of coefficients depends on how many lags we include. With two lags, we have two different coefficients—one for yesterday and one for the day before yesterday.

The constant term, alpha, acts as a baseline level, while the coefficients determine how strongly each past value influences the current prediction. These coefficients are usually denoted by symbols such as φ (phi), δ (delta), or ω (omega), depending on notation, but the idea is always the same: they measure the influence of past values on the present.

Autoregression is fundamental to time series modeling because time series data is all about patterns over time. In many real-world problems—such as coffee consumption habits or daily chocolate revenue predictions—the past contains valuable information about the future. If a problem had no autoregressive component at all, it would mean that the past provides no useful signal for prediction, which rarely makes sense in time series analysis.

If you’re wondering whether autoregression has something to do with autocorrelation, the answer is yes. They are closely related. Autocorrelation tells us how strongly current values are related to past values, while autoregressive models explicitly use that relationship inside the ARIMA framework. In a sense, they describe the same underlying information from two different perspectives.

So, in a nutshell, the autoregressive component uses lagged values of the time series to help predict the future. That’s the essence of AR.

Next, we’ll move on to the Integrated part and see what that’s all about

# **F) Integrated**

Let’s now move on to the second component of the ARIMA model: Integrated.
This part plays a key role in making our predictions as accurate and reliable as possible.

So what does Integrated actually mean, and how does it relate to stationarity?

A good way to think about a stationary time series is to imagine a reliable train moving at a steady pace. Its speed doesn’t suddenly change, and neither do its characteristics. In statistical terms, a stationary time series has a constant mean, constant variance, and constant covariance over time. In simple words, it doesn’t swing wildly. Its patterns are stable, and that stability makes it much easier to predict what will happen next.

The challenge is that, in practice, most time series data is not stationary. This is where the concept of differencing comes in.

If our data is not steady—for example, if it has a clear upward or downward trend, like we often see with financial data such as Bitcoin prices—we need to apply differencing. Differencing helps smooth out the data. What we do is subtract one day’s value from the previous day’s value. The result is a new series that is much more stable and, therefore, more predictable.

We’ll explore this visually in the practical tutorial, where I’ll show you what a raw time series looks like and how it changes after differencing is applied. For now, it’s enough to understand the idea: differencing removes trends and stabilizes the data.

Now let’s connect this back to ARIMA.
The Integrated component tells us how many times we need to difference the data to make it stationary. If we difference the data once and that’s enough to stabilize it, then the model has an I value of 1. In other words, we would be working with an ARIMA model where I = 1.

To build intuition, imagine four different time series:

The first has a fairly stable pattern over time. This one would be considered stationary.

The second steadily grows over time. This means its mean changes, so it is not stationary.

The third shows fluctuations with changing amplitudes—think of seasonal sales that peak and dip at different levels each year. Since the variance changes, this series is also not stationary.

The fourth has cycles of different lengths and irregular ups and downs. Because there is no consistent structure, this series is not stationary either.

In real-world scenarios, this becomes tricky because most real-world data is not stationary. Financial data, revenue data, sales data, and almost anything that represents a flow over time usually changes its behavior. These series don’t follow a clean, stable pattern that’s easy to predict.

That’s exactly why differencing is so important in ARIMA. It transforms messy, time-varying data into something more manageable—something we can actually model and forecast.

A natural question at this point is: How do we know if our data is stationary?
There is a formal statistical test for this called the Augmented Dickey–Fuller (ADF) test. It allows us to check whether the mean, variance, and covariance are constant over time. In the next video, we’ll walk through this test and I’ll show you how to automate the process so you don’t have to rely on guesswork.

From experience, a practical mindset is to assume that most time series are not stationary and to rely on differencing as a tool to reveal usable patterns. This idea of differencing is quite unique to ARIMA—most other models don’t explicitly include it as a built-in concept.

The good news is that this process is largely automated in practice, so you don’t need to overthink it. We’ll see how straightforward it actually is when we apply it step by step.

Let’s put all of this into practice in the next video.

# **G) Python - Stationarity**

We are now going to look at stationarity, which is a core concept behind the Integrated (I) part of ARIMA. This is something we need to understand and inspect, just like all the other parameters. However, it’s important to be very clear about one thing from the start: we are not going to make final modeling decisions based on a single chart or a single test. Instead, we look at the problem globally.

I want to show you how stationarity works and why it matters. It’s an important topic in time series analysis overall, but in practice, it’s usually not the final deciding factor. In the beginning, we may use it as guidance because it gives us useful information, but later on, what really matters is that we test many combinations of parameters and let the data and metrics decide. That’s ultimately how ARIMA modeling works.

So let’s focus on stationarity. As mentioned earlier, we imported the ADF test, which is our statistical test for stationarity. This test comes from statsmodels, and we apply it directly to our target time series.

We run the ADF test on DataFrame.y, and the output gives us several values. At first glance, it may look overwhelming, but we are going to focus on just one thing: the p-value. The p-value is what allows us to test a hypothesis and make an inference about our data.

To understand the p-value, we need to talk briefly about hypotheses. Every statistical test has:

a null hypothesis (the status quo), and

an alternative hypothesis (what we want to check against the status quo).

For the Augmented Dickey–Fuller test, the null hypothesis is that the time series has a unit root, which means it is not stationary. The alternative hypothesis is that there is no unit root, meaning the series is stationary.

This is where things can feel a bit counterintuitive. We are testing for stationarity, but the null hypothesis actually assumes non-stationarity. So interpretation is crucial:

If the p-value is greater than 0.05, we fail to reject the null hypothesis → the time series is not stationary.

If the p-value is less than or equal to 0.05, we reject the null hypothesis → the time series is stationary.

In our case, the p-value is around 0.1, which is greater than 0.05. That means our original time series is not stationary.

Now let’s look at what happens under the hood when we treat a time series as non-stationary in ARIMA. The model applies differencing. Practically, this means subtracting the previous value from the current value.

So we create a new series:
y_diff = DataFrame.y.diff()

This operation takes today’s value and subtracts yesterday’s value. Since the first observation has no previous value, we drop the resulting NaN. If you manually check the math between two consecutive values, you’ll see that it matches exactly—this is just simple subtraction.

Once we have the differenced series, we run the ADF test again on y_diff. This time, the p-value becomes very small, close to zero. That tells us that the differenced series is stationary.

So what does this mean for us?

It means that our original time series is not stationary—which is completely normal. In fact, most real-world time series are not stationary. They usually have trends, seasonality, or both. If you just look at our data visually, you can already see a trend and seasonal patterns. Both of these violate the assumptions of stationarity.

Stationarity depends on three things:

constant mean

constant variance

constant covariance

If a time series has a trend or seasonality, these conditions are almost always violated. So it is very fair—and very realistic—to assume non-stationarity in most cases.

That’s exactly why ARIMA includes the Integrated component. Differencing allows us to transform complex, real-world data into a form that is more stable and easier to model.

We’ll stop here for this video. There is still a lot more to learn, and things will become even clearer as we move forward and start combining all these pieces together.

# **H) Moving Average**

Now it’s time to cover the final piece of the ARIMA puzzle: the moving average (MA) component. I’ll explain this in a simple, intuitive way, using the same coffee example we used earlier when discussing the autoregressive part.

Imagine you’re trying to predict how much coffee you’ll need tomorrow. One obvious approach is to look at how much coffee you drank yesterday or the day before. That’s exactly what the autoregressive (AR) component does—it learns from past values. But there’s another smart thing we can do, and this is where the moving average comes in.

Instead of only looking at past coffee consumption, what if you also considered the mistakes you made in your earlier predictions? For example, maybe yesterday you predicted you’d need three cups of coffee, but in reality, you only drank one. That difference between what you predicted and what actually happened is an error. The moving average component learns from these “whoopsies.”

In simple terms, the MA part looks at the errors from previous predictions and uses them to improve future forecasts. It’s like saying, “I thought I needed three cups yesterday, but I only had one—let me remember that when guessing today.” This helps the model adjust its predictions more intelligently.

One of the main reasons we need the moving average component is to smooth out random fluctuations in the data. Real-world data often has sudden bumps or noise that don’t represent long-term behavior. The MA helps prevent the model from overreacting to those random changes and making drastic decisions based on very recent events.

At the same time, the moving average is great for quick adjustments. If there’s a sudden drop or unexpected change, the MA component adapts quickly because it directly accounts for recent prediction errors. It doesn’t just rely on past values—it also learns from past mistakes.

So, in a nutshell, the moving average adds a layer of wisdom to the forecasting process. It teaches the model not only to learn from what has happened before, but also from where it went wrong. That balance makes predictions smoother, smarter, and more responsive.

# **I) Python - ARIMA**

Okay, now it’s time to move on to ARIMA. In our functions, you’re going to find SARIMAX, and we’ll be using this to build our ARIMA model. Sorry, SARIMAX, for everything—but the reason we use it is because SARIMAX can also be used to build a plain ARIMA model. And just to be complete, there’s also ARIMAX, which is ARIMA with external regressors but without seasonality. That setup is quite unique and not something you see very often in real-world time series, but it’s good to be aware of it.

As always, the first thing we do is assess our data properly. That means splitting it into training and testing sets. Here, we’ll keep the last 30 days as our test data. Everything before that becomes the training set. Once we do this split, we can quickly preview the test data to make sure everything looks correct. So far, everything looks fine.

At some point in the future, when we include variables like discount rate and coupon rate, we’ll need to deal with the fact that they’re stored as objects—probably percentages. We already saw that when checking the dataset info earlier. But that’s a future problem, and for now, we’ll ignore it.

Next, we create the ARIMA model itself. Since this is a non-seasonal ARIMA, the seasonal order will be set to (0, 0, 0, 0). That means no seasonal autoregressive terms, no seasonal differencing, and no seasonal moving average terms. For now, we keep it simple.

For the main ARIMA order, we use (1, 1, 1) for p, d, and q. The differencing term d is set to 1 because we already saw that the data is not stationary. It’s important to highlight that choosing these values is often a bit of a guessing game, especially in the beginning. Personally, I like to look at PACF values, and here I’m choosing to look back three days for the autoregressive component. That’s why we include three lags when thinking about the AR structure.

Once the model is defined, we immediately fit it using .fit() and then print the model summary. While this is running, a quick reminder that documentation is always important—the SARIMAX documentation link is already included at the top of the script.

The model fits very quickly, in about two seconds. In the summary, you’ll see the endogenous variable, which is our main time series, and the exogenous variables, which would be things like discount rate or coupon rate if we had added them. You’ll also see the order and seasonal order listed. There are many other parameters shown, but from an application perspective, most of them aren’t critical, and honestly, I’ve never really needed to use them.

There’s also the disp parameter in the fit method. If you ever see a lot of convergence warnings and your model isn’t behaving well, you can set disp=False to suppress those messages. It’s just a boolean flag. For us, though, everything looks fine, so we don’t need to worry about it.

Looking at the coefficients, we can see three autoregressive terms, corresponding to the three past days we’re using, and one moving average term. The moving average coefficient is around -0.9. I’ll explain what this means visually in a moment with a simple drawing, even though I’m terrible at drawing. The idea is just to make the intuition clearer.

Before we move to the drawing, let’s generate predictions. We use the model’s forecast method and set the number of steps equal to the length of the test set, which is 30 days. Since the test data length is already a number, we don’t need to do anything fancy here—just generate 30 predictions.

When we look at the predicted values, something interesting happens. The predictions start increasing—141, 149, 151, 52, 54, 55, 56, 57—and then they flatten out. Eventually, they stabilize at a fixed value. This happens because ARIMA is a simple model that only looks at the last three days and the average error from the previous day. Over time, it converges to a steady level and stops changing much.

Now let’s visualize the forecast and assess the model. When we compare predictions with actual values, the results are not great. To make it easier to see, we focus only on the year 2022. Even then, the error is quite large—around 24%, which translates to roughly 7 million units. That’s a significant error, and visually, the fit looks pretty bad.

If we zoom in on the forecast, we clearly see the plateau effect. Because the model only looks at the last three days, repeated forecasting causes it to settle at a constant level. This behavior is exactly why ARIMA is considered an introductory model. It’s great for learning the fundamentals, but it has clear limitations when applied to complex real-world data.

This is why we’re using ARIMA mainly as a stepping stone. It helps us understand how autoregression, differencing, and moving averages work together. In the next step, we’ll draw this out visually to really see how it behaves, and then we’ll build on it with more advanced models.

# **J) ARIMA in Action**

Okay, welcome back. Let’s now look at ARIMA in practice. I went ahead and made a few drawings to help visualize what’s actually going on under the hood. I’ll be the first to admit that I’m really bad at drawing, but I genuinely hope this still helps make the idea clearer.

In the drawing, we start with our time series, and I’ve shown four periods here. The model itself is using three previous periods, which matches the way we configured it earlier. You can see coefficients like 0.35, 0.02, and 0.12—these represent the weights applied to the previous values in the time series. Let me zoom in a bit so it’s easier to see what’s happening.

So here’s the idea. We have values from the time series at t-1, t-2, and t-3. These past values are multiplied by their respective coefficients and combined together. At the same time, we also consider the forecasting errors. I’ve drawn the actual time series in one color and the forecasting errors in another to distinguish them.

To predict the value at time t, we combine all of this information. That includes the values from the previous three periods and the forecasting error from t-1. The ARIMA prediction is essentially a mixture of past observations and past mistakes. On top of that, there is also a constant term that is always applied as part of the ARIMA equation. Overall, it’s a very straightforward structure once you see it visually.

I also added another part to the drawing to show that this process is iterative. If the current point is time t, then naturally the previous points become t-1, t-2, and t-3. This keeps rolling forward as time progresses. That’s why this is often called a rolling approach—we’re always moving forward one step at a time, using the most recent information available.

At the same time, we again bring in the forecasting error from the most recent step. So when we predict a future value, we’re always using the last three observed periods and the most recent error. This repeats continuously as the model generates forecasts further into the future.

Even though the drawing isn’t perfect, the key takeaway is that t is always influenced by three previous periods, because that’s how the model was defined: (3, 1, 1). The 1 for differencing means the model internally differences the data to make it stationary, and then it reverts that differencing when producing the final forecast.

Now, when we move fully into forecasting mode—where the model starts predicting future values based on its own previous forecasts—the amount of information available becomes limited. There are no external variables influencing the predictions, and eventually the model starts predicting based on predictions. Because of that, the forecasts converge toward a certain level and stay there.

That behavior explains why we saw the plateau earlier. The model simply doesn’t have enough new information to keep adjusting, so it stabilizes. This is completely expected for a basic ARIMA setup and is one of the reasons why it’s mainly used as an introductory model.

We’ll keep building on this idea when we move to SARIMA, where seasonality adds more structure and usefulness. For now, this visualization should help solidify how ARIMA actually works behind the scenes.

# **K) SARIMA**

Alright, let’s do this. Now we’re going to talk about SARIMA, which naturally builds on ARIMA. If you already understand how ARIMA works, then SARIMA is actually going to feel quite easy, because all we’re really doing is adding a seasonal component on top of what we already know.

The reason this matters is that most real-world data is seasonal. What we do at 10 a.m. is very different from what we do at 10 p.m. What happens in February is not the same as what happens in August. Behavior changes over time in predictable patterns, and because of that, incorporating seasonality becomes extremely important if we want better forecasting results. SARIMA takes us one level deeper and helps us capture those repeating patterns.

The overall purpose here is simple: ARIMA alone is not enough to represent the real world. Even SARIMA is not perfect, but it’s definitely a step in the right direction. When time series methods were originally developed, computational power was very limited. A lot of calculations were done by hand, or with very basic computers and calculators. Given those constraints, these models were designed to be practical and interpretable, and they still make sense today.

Seasonality is everywhere. You don’t eat ice cream all year round, and that example is often used as the poster child for why SARIMA exists. Certain behaviors repeat at regular intervals, and SARIMA is designed specifically to capture that structure.

In terms of components, we already know the lowercase p, d, q from ARIMA. SARIMA adds uppercase P, D, Q, along with m, which represents the seasonal period. One drawback of SARIMA is that it only allows for one seasonal component. In the real world, we often have multiple seasonalities—within a day, across days of the week, across months, and across years. SARIMA can’t handle all of those at once, but it still captures a major part of the structure.

These components work in a fairly straightforward way. For example, earlier we used (3, 1, 1) for the non-seasonal part—three previous days, one level of differencing, and one moving average term. For the seasonal part, we might look at what happened one week ago. In that case, we could set the seasonal autoregressive term P = 1. If we want to look at the forecasting error from one week ago, we would set the seasonal moving average term Q = 1. We can also apply differencing specifically to the seasonal component using D.

The value m represents the number of periods in a season. For example, if the seasonality is weekly with daily data, then m = 7. As we move on to more advanced models later, we’ll see ways to handle multiple seasonalities more effectively, but SARIMA is a solid foundation.

Under the hood, what SARIMA is really doing is splitting the data into seasonal and non-seasonal components. This should sound familiar from when we talked about seasonal decomposition. Once the data is separated, the AR and MA components are applied to each part appropriately.

One major reason SARIMA works better than ARIMA is that it stabilizes the data. ARIMA often struggles with regular ups and downs caused by seasonality. From ARIMA’s perspective, those fluctuations look confusing, so it tries to adapt without fully understanding what’s going on. By removing or explicitly modeling seasonality, SARIMA gives us much cleaner data, making it easier to understand how much information truly comes from the previous days or weeks.

At the same time, we also care about the errors from previous seasons. Are the errors consistently positive or negative week over week? That’s where the moving average component plays a role again. In the end, SARIMA is about balancing information from past values and past errors, both in seasonal and non-seasonal terms.

If you remember Holt-Winters, we also had trend and seasonality components there. Even just adding seasonality alone can explain a large portion of what’s happening in many time series. That’s why SARIMA often performs much better than plain ARIMA.

On the slide, you’ll see references to data with predictable fluctuations. That idea applies to almost all time series forecasting methods—many principles are shared across models.

And that’s it for the theory. Next, we’ll look at SARIMA in practice, just like we did with ARIMA, and see whether we can actually improve our results.

# **L) Python - SARIMA**

Alright, let’s do SARIMA. Here we go. At this stage, we’re simply going to build the model and start with some initial parameter choices. We’ll stick with the non-seasonal order (3, 1, 1) and then add a seasonal order. Since we’re working with daily data, a natural starting point is 7 days, and we’ll also experiment with 365 days to see how the model behaves and whether there are any noticeable differences.

Before finalizing the seasonal order, let’s take another look at the PACF. We can still see meaningful information one week and even two weeks back. Based on that, we’ll use a seasonal autoregressive order of 2. Once again, it’s important to emphasize that a lot of this is based on intuition and exploratory analysis. Another thing to keep in mind is that the lower the parameter values, the less computationally expensive the model becomes. That’s why we’ll keep the seasonal differencing at zero for now. We already have differencing happening in the non-seasonal part, so this should be sufficient as a starting point.

With that in mind, we go ahead and define the SARIMA model. Everything is organized, and we run the model. Initially, something looks odd in the output, but that turns out to be a simple mistake—we were still referencing the ARIMA model instead of the SARIMA one. Once that’s corrected and the proper SARIMA model is used, everything behaves as expected.

Now we can clearly see the components in the model output. We have the non-seasonal autoregressive terms L1, L2, and L3, along with the moving average term. On top of that, we also see the seasonal autoregressive terms, specifically L7 and L14, which correspond to one week and two weeks back. We also include the seasonal moving average error at L7. When we check statistical significance, everything appears to be significant, which is a good sign—it tells us that the model is capturing meaningful information.

The next step, as always, is to look at predictions. We generate forecasts using the SARIMA model for the same number of days as our test set. Instead of focusing on the raw prediction values, we move straight to model assessment and visualization. To make the plot easier to interpret, we again focus only on the year 2022.

Visually, we can now see some ups and downs, which indicates that the model is capturing at least a small seasonal pattern. However, the improvement isn’t as strong as we might have hoped. In fact, the error metric comes out to around 25.9, which is actually worse than what we had before. Even though the curve looks slightly better toward the end, it’s still off overall.

This highlights an important point: not every model works well on the first try. The time period chosen here is especially difficult—think Black Friday and similar events. Those spikes are hard to model without additional information. Just because the model underperforms in one segment doesn’t mean it’s useless. In fact, this is a perfect example of why we need external variables to help explain what’s going on.

One thing we can always try is changing the seasonal period to 365. We can even experiment with two years if we want. However, we need to be very careful here. The further back we go, the heavier the computational load becomes. From a computational perspective, this quickly becomes tricky.

We try running the model with a seasonal period of 365. At first, there are no immediate errors, which seems promising. But as time goes on, the model becomes extremely slow. This is expected—long seasonal periods combined with higher-order models are computationally expensive. The hope is that once we add external variables, the model will improve, even if it still doesn’t perform perfectly. At the very least, we’d expect to see improvements in our evaluation metrics.

At this point, the model is still running. After letting it run for a long time—long enough to go for lunch, cook, eat, relax, stop it, and try again—it becomes clear that the 365-day seasonal setup simply isn’t working for this dataset and parameter combination. It could be the dataset itself, the model complexity, or simply not a good day for this experiment.

A natural question here is whether using a GPU would help. Unfortunately, it wouldn’t. SARIMA is a CPU-only process, and there’s no way to accelerate it using a GPU at the moment. So the only real option is to interrupt the process and restart.

In the end, we revert back to using a seasonal period of 7, which is the standard choice for daily data and works reliably. That’s what we’ll stick with moving forward.

And that’s it for this video. If you tried this yourself and managed to get the 365-day seasonality working, I’d genuinely love to hear about it—what you changed, what worked, and what made the difference. It’s very possible that for a dataset this large and with these parameters, a yearly seasonal component is simply too demanding from a programming and computational perspective.

The key takeaway is this: the more orders and seasonal components we add, the harder the problem becomes. That’s something you should always keep in mind when working with SARIMA.

# **M) SARIMA in Action**

Okay, welcome back. Let’s recap what we were looking at last time and then build on it step by step.

Previously, we focused on the ARIMA forecast. That forecast was a combination of several elements. First, we had the integrated part, which comes from differencing the data to make it stationary. Then we had the forecasting error from time t–1, multiplied by its coefficient. On top of that, we included the autoregressive component, which uses the values from the last three periods—t–1, t–2, and t–3—each weighted by their own coefficients. All of this together makes up the ARIMA side of the model.

If we try to write this conceptually, that’s essentially what ARIMA is doing. It combines recent past values, recent errors, and the differenced structure of the data to produce a forecast.

Now, when we move from ARIMA to SARIMA, we are simply adding more structure to this same framework. The final forecast is still one single value, but it now becomes a combination of more elements. The ARIMA components remain exactly the same, but we add seasonal information on top of them.

So, in the final SARIMA forecast, we are combining four main pieces. First, we still have the ARIMA part, which includes the forecasting error at t–1. Second, we still use the time series values at t–1, t–2, and t–3. Third, we now add the seasonal autoregressive terms, which in our case come from t–7 and t–14. You can see those clearly reflected in the table, where the seasonal lags are 7 and 14. Finally, we also include the seasonal forecasting error, specifically the error from t–7.

All of these elements are combined together to produce the final forecast. It’s important to highlight that although we talk about the integrated part, differencing itself doesn’t have a coefficient. It’s simply a transformation applied during modeling and then reversed when we generate forecasts.

In addition to all of this, we also have the sigma term, which acts like an intercept or baseline. You can think of it as the starting level of the forecast, and then all the coefficients from the AR, MA, and seasonal components adjust that baseline up or down.

At this stage, we don’t yet know whether this is the best possible combination of parameters. That’s something we’ll explore later in the section through experimentation and tuning. For now, we’re using this setup to clearly understand how the model is structured.

The main takeaway from this drawing is that SARIMA is actually a very simple and logical framework. We look a little bit into the recent past, we look further back into the seasonal past, and then we combine everything together into a single forecast. That’s really all there is to it conceptually.

There’s still a lot more to learn, but this foundation is crucial. I’ll see you in the next video.

# **N) SARIMAX**

In this video, we’re going to go one level up and talk about SARIMAX. So what exactly is it? Quite simply, it’s SARIMA plus exogenous variables. That’s where the “X” comes from. Conceptually, SARIMAX is just SARIMA with the ability to include external factors that influence the time series.

Even though the idea sounds simple, it’s extremely powerful. Imagine you’re trying to predict ice cream sales. With SARIMA, you’re already considering past sales and seasonal patterns—things like weekly or yearly cycles. But what about temperature? A very hot day could suddenly cause a spike in ice cream sales that SARIMA alone would struggle to explain.

This is where SARIMAX really shines. It allows you to include exogenous variables, such as temperature, directly into the model. That’s the real “X factor”—pun intended. These variables help explain changes in the time series that don’t come purely from its own past behavior.

In the ice cream example, exogenous variables could include weather conditions, holidays, promotions, or even nearby events. SARIMAX takes these into account, giving you a more complete and realistic view of what actually drives the data.

From a modeling perspective, we’re not adding brand-new internal components like we did when moving from ARIMA to SARIMA. Instead, we’re augmenting SARIMA with these external inputs. The model analyzes how these exogenous variables have historically influenced the main time series and uses that information to improve future forecasts.

One of the biggest advantages of SARIMAX is higher accuracy, especially when external events clearly impact the data. If those outside factors matter—and often they do—SARIMAX can significantly outperform models that only rely on past values and seasonality.

Another major benefit is greater insight. Not only can we make better predictions, but we can also study how different external factors influence our data. This helps with understanding, decision-making, and even business strategy.

Of course, there are downsides as well. The biggest one is complexity. Adding exogenous variables means we need more data, and that data needs to be reliable. We also need to think carefully about data availability, because SARIMAX requires both historical exogenous data and future values of those same variables in order to make forecasts.

In short, using SARIMAX effectively comes down to careful variable selection. We don’t want to make the model unnecessarily complex. As always, if a simpler approach works well, it’s usually the better choice. The key is to experiment—add external factors thoughtfully and see whether they actually improve accuracy.

# **O) Python - SARIMAX**

Alright, welcome back. We’re done with SARIMA, and now it’s time to move on to SARIMAX. This is the next step, and it’s also the one we’re really going to explore in more depth.

If we take a look at our dataset—starting with a quick dataframe.head()—we can see that we now have two additional variables: discount rate and coupon rate. Both of these are currently stored as objects, which is not ideal for modeling. The reason is simple: these values include percentage symbols, and the model expects numerical input.

So the first thing we do is clean this up. We remove the percentage symbol from both the discount rate and the coupon rate columns and then convert them to floats. I’m being very explicit here so we can avoid confusion and get some shortcuts later. Once this transformation is done, the columns are properly converted, and everything looks good.

Next, we split the regressor data into training and testing sets. Just like before, the training regressors will go into the model, and the test regressors will be used during forecasting. This split has to align perfectly with how we split the target variable, otherwise the model won’t work correctly.

To do this cleanly, we select only the discount rate and coupon rate columns and then split them using the same logic as before. The training regressors go up to minus the test days, and the test regressors run from the last test days to the end. We double-check the dates to make sure everything lines up correctly—training ends on October 31st, and predictions start on November 1st. That confirms the split is correct.

With the regressors ready, we can now build the SARIMAX model. We pass in the training target variable along with the training regressors as exog. We keep the same configuration as before: non-seasonal order (3, 1, 1) and seasonal order (2, 0, 1, 7). The documentation reminds us that the exogenous input must be shaped as the number of observations by the number of regressors, which is exactly what we have.

We fit the model and print the summary. This takes a few seconds to run. Once it’s done, we can immediately see what’s new compared to SARIMA. The key difference is that we now have coefficients for discount rate and coupon rate. That’s exactly what we expect—these are the new external variables influencing the forecast.

With that, the modeling part is essentially complete. The next step is prediction. We generate forecasts using the SARIMAX model, specifying the number of steps equal to the test days and passing in the test regressors as exogenous inputs. Once that’s done, we move straight to model assessment and visualization.

The results show a clear improvement. The error drops to 21.7%, and the MAE is around 630, which is definitely better than what we had before. So adding regressors helped—this is a win.

That said, we’re still missing something important. We do capture weekly seasonality, but we’re missing yearly effects—things like Christmas, Thanksgiving, and Black Friday. These events have a big impact on the data, and they’re not explicitly represented in the model right now. If we were building this from scratch in a real project, we would likely add additional variables to represent those events.

The key takeaway from this section is not feature engineering. We’ll do plenty of that later. The real focus here is on understanding the frameworks. From this point onward, we’re going to spend a lot of time on cross-validation, parameter tuning, and reusing the same modeling patterns again and again.

If we truly understand these frameworks now, everything that comes later—especially more advanced models—will be much easier to grasp.

So yes, we improved the model by adding regressors, which is great. But there’s still room for improvement, and that’s exactly what we’ll continue working on next.

# **P) SARIMAX in Action**

In this final video of the section, we take everything we have built so far and bring it together into one complete picture. At this point, we already have our ARIMA and SARIMA components clearly defined, and now we focus on how they combine with external variables to form the final forecasting model.

First, we start with the ARIMA (non-seasonal) part of the model. This captures short-term patterns in the data. In this case, it includes:

A moving average (MA) component of one period

An autoregressive (AR) component of three periods

This means the model looks at recent errors and values from the last few time steps to explain current behavior.

Next, we include the seasonal ARIMA (SARIMA) part, which handles repeating seasonal patterns. Here, the model:

Uses the error from the previous seasonal period, such as one week ago

Uses seasonal autoregressive terms, for example at time steps t − 7 and t − 14

These components help the model capture weekly seasonality and recurring trends that repeat over time.

Now comes the key extension: exogenous variables. In this example, we add two external factors:

Discount

Coupon

These variables are applied at the same time step t, meaning they directly influence the forecast for that specific point in time. While only two are shown here for simplicity, in practice you can include more external variables if they are relevant and available.

When everything is combined, the final forecast is built from multiple elements:

Non-seasonal AR and MA components

Seasonal AR and MA components

Exogenous variables (discount and coupon)

A baseline intercept term (α)

Although we might initially count six major components, the true number is higher because seasonal and non-seasonal parts each contain multiple lag terms. All these elements are weighted and combined according to the model’s equation to produce the final prediction.

It’s important to note that this example represents a basic SARIMAX setup. We are deliberately not introducing feature engineering yet. Feature engineering would involve transforming variables, adding lagged versions of discounts or coupons, encoding promotions differently, or creating interaction effects. That level of complexity is intentionally avoided in this section.

The main focus here is on understanding the fundamentals. Before adding more features, it’s crucial to master:

Cross-validation for time series

Parameter tuning

Evaluating whether additional complexity actually improves accuracy

As always, simpler models are often better if they perform well. External variables should be added carefully and tested incrementally to confirm that they genuinely improve the forecast.

That wraps up this section. If you have any questions or face any issues, feel free to ask. In the next video, we’ll move on to cross-validation and parameter tuning, which are the most important skills to master at this stage.

# **Q) Cross-Validation for Time Series**

Cross-validation is a fundamental concept in time series forecasting because it adds credibility and robustness to our models. The core idea is to repeatedly test the model under different conditions, such as different periods of the year, to ensure that it performs consistently and reliably.

Instead of relying on a single train–test split, we create multiple training and testing scenarios. In time series forecasting, this is especially important because data is ordered in time, and patterns such as trends and seasonality can change throughout the year. A model might perform well in one seasonal period and poorly in another, which does not necessarily mean the model is bad overall.

To address this, we evaluate the model across different seasonal segments. By doing so, we can test whether the model generalizes well across the full range of seasonal behaviors rather than just a specific time window.

When we zoom in on cross-validation for time series, there are two main approaches.

The first approach is called rolling forecast cross-validation. In this method, after each evaluation, we add the test set to the training set before making the next prediction. Over time, the training data grows larger and larger. This approach reflects real-world forecasting scenarios, where all past data is available and should be used to improve future predictions.

The second approach is known as sliding forecast cross-validation. Here, the size of the training set remains constant. Each time we move forward, we add new test data to the training set but remove the same amount of the oldest data. As a result, the training window slides forward in time while maintaining a fixed length.

Between the two, the general preference is often for the rolling forecast approach. The reasoning is simple: if historical data is valuable enough to be used for evaluation, then it should also be valuable for training the final forecasting model. If certain data is not useful, then it probably should not be included at all, even during experimentation.

Rolling forecasts ensure that all available information contributes to both model assessment and future predictions. Sliding forecasts can be useful in cases where very old data is no longer relevant, but this should be a deliberate and well-justified choice.

To summarize, cross-validation is a simple yet powerful concept for building reliable forecasting models.

Rolling cross-validation continuously expands the training set by incorporating past test data.

Sliding cross-validation keeps the training window fixed by adding new data and discarding the oldest observations.

In the next video, we’ll see how this cross-validation strategy is applied directly to our forecasting model. Have fun, and feel free to ask if you have any questions!

# **R) Python - Cross-Validation**

Alright, let’s move on to cross-validation and see how it actually works in practice.

For this, we are going to use time series cross-validation, specifically the TimeSeriesSplit approach. I already have the module ready, and we’ll include it directly in our workflow. This method is designed specifically for time-dependent data, where maintaining the order of observations is critical.

We’ll start by clearly defining the cross-validation configuration. This keeps the code clean and avoids unnecessary noise later on. The non-seasonal order remains (3, 0, 1) and the seasonal order stays (2, 0, 1, 7). We also define the number of splits as five. This means the model will be trained and tested five separate times across different temporal segments of the data.

Next, we import TimeSeriesSplit from sklearn.model_selection. We initialize it with:

n_splits = 5, meaning five cross-validation runs

test_size = number_of_test_days

The choice of test size is very intentional. Our forecasting use case always focuses on 30-day forecasts, so our test window is consistently set to 30 days. This is a crucial difference from traditional machine learning, where people often use fixed ratios like 80/20. In forecasting, we must evaluate the model in the same horizon we plan to use it in production.

The max_train_size parameter is left as None, which means we are using a rolling forecast strategy. The training data grows with each split instead of being restricted to a fixed window.

Before running the model, we inspect how the time series splits actually look. When we print the train and test indices for each split, we see that only indices are shown. To interpret this correctly, we check the total length of the dataset, which is 1,795 observations. Since indexing starts at zero, the final index is 1,794.

Each split consistently takes the last 30 observations as the test set, then shifts backward in time for earlier splits. For example:

One split tests days 60–30

Another tests days 90–60

Another tests days 120–90
and so on. This confirms that our cross-validation setup is behaving exactly as expected.

Now we move on to actually performing the cross-validation loop.

We begin by initializing an empty list to store cross-validation scores. Inside the loop, for each train-test split:

We retrieve the corresponding indices

We isolate the correct training and testing data

We extract the exogenous regressors (discount rate and coupon rate)

We build the SARIMAX model using the defined orders

We generate forecasts for the test period

We evaluate predictions using:

RMSE (Root Mean Squared Error)

MAE (Mean Absolute Error)

MAPE (Mean Absolute Percentage Error)

Each metric is stored for later aggregation. This loop is essentially the same modeling logic we used before, but now it’s wrapped in a systematic evaluation framework.

At this point, it’s important to acknowledge the warnings that appear during model fitting. These warnings are not errors—they simply indicate that from a statistical standpoint, the model may not be perfectly specified for some splits. This does not automatically mean the model is unusable. From a business perspective, performance may still be acceptable, and the final decision should always be based on the actual error metrics.

Once the loop finishes running, we aggregate all results into a DataFrame. This allows us to inspect how the errors vary across different splits. In our case, the MAPE values range roughly between 6% and 24%, showing noticeable variability depending on the time period being forecasted.

This variability is expected in real-world time series. Ideally, we would like more stability, but performance always depends on the underlying volatility of the data.

Finally, we compute and print the average errors across all splits:

Average RMSE

Average MAE

Average MAPE (around 18%)

There is clearly room for improvement. However, that is not the purpose of this stage.

The key takeaway here is that we now understand:

How the model behaves across multiple time periods

What level of error we can expect for this specific configuration

That cross-validation provides a much more reliable assessment than a single train-test split

At this point, we know exactly where we stand with this set of variables and parameter choices. The natural next step is parameter tuning, where we systematically search for better configurations.

# **S) Parameter Tuning**

Parameter tuning is what takes a forecasting model from good to great.

It’s true that the programming involved can feel a bit challenging at first, but the good news is that once you understand the structure, you’ll be working with a reusable template. This makes the process much easier to replicate across models and projects.

Let’s start with the most important question: why do we need parameter tuning?

Modern analytics gives us an incredible amount of flexibility. We can customize models, adjust assumptions, and fine-tune how they behave. However, this flexibility also means that there is no single “default” configuration that works best for every problem. To get the highest possible accuracy, we must actively search for the optimal set of parameters for each model and each dataset.

From a process perspective, parameter tuning follows a very logical sequence.
First, we define a range of possible parameter values.
Next, we run the model using each of these combinations.
Then, we measure the model’s accuracy and store the resulting error.

At its core, this is nothing fundamentally new. It’s exactly what we’ve already been doing—but now we do it systematically, repeatedly, and in an automated way.

To make this concrete, imagine we are tuning the autoregressive component of a model. We might test lags of 1, 2, and 3. For each lag value:

We fit the model

We compute the error

We store the result

Once all combinations are evaluated, we simply select the parameter value that produced the lowest error. For example, if lag 1 performs best, then lag 1 becomes our optimal autoregressive parameter.

From an intuition standpoint, this process is very straightforward.
We explore different combinations, compare their performance, and keep the one that works best for our specific forecasting problem.

To summarize, parameter tuning is about finding the optimal configuration of a model that maximizes forecasting accuracy. It transforms model development from guesswork into a disciplined, data-driven process.

In the next step, we’ll implement this entire concept in Python, using the structure we’ve already built.

# **T) Python - Setting the Parameters**

Okay, parameter tuning. This part is going to be split into three videos. In the first video, we are going to focus purely on the configurations. The goal here is not to run anything yet, but to get everything properly set up so that parameter tuning can run smoothly later.

In video two, we will actually run the parameter tuning. This step takes a few minutes to execute, which is why we are not going to try a large number of configurations. You will already have the full structure in place, so scaling it up later will be easy. For now, we are going to work with eight configurations. You could easily run 512 configurations if you want, but eight is more than enough for us to clearly understand the flow.

Then, in video three, we will look at the results and interpret them. That is the overall purpose of this section and how it is structured.

Now let’s get started.

First, let me quickly check the imports. Everything looks good. One important thing we need here is the parameter grid. This parameter grid is what is going to automatically create all the combinations for us. I’m going to run this import, and once that’s done, we can kick things off.

Next, we define the parameter options. The parameter grid is honestly one of those pieces of code that you’ll end up using forever. We use it so often that it becomes second nature.

We start by defining the non-seasonal parameters. For p, I’m going to use values 1 and 3. Three is the value we’ve been using so far. For d, we’ll stick with 1. For q, we’ll test 0 and 1.

Then we move on to the seasonal parameters. For uppercase P, we’ll use 1 and 2. For uppercase D, we’ll use 0. And finally, for uppercase Q, we’ll use 1, just one value.

Once all of these are defined, we bring everything together by applying the parameter grid. This creates all the possible combinations based on the values we specified. At this point, you can convert the grid to a list and inspect all the combinations if you want. Personally, what I usually do is just check the length of the grid.

When we do that here, we see that the total number of combinations is eight. So we’re going to run eight different configurations. I always like to stay on the lower side at the beginning because it’s much faster. Cross-validation itself can take a couple of minutes per run. So eight configurations means we can expect something like 10 to 12 minutes overall, give or take.

Now, regarding the setup, we actually don’t need to redefine everything because we’re going to reuse the same setup we already have. The number of splits is already defined, and the test size is also defined. That said, one thing I like to do is to explicitly include the time series split here again. Yes, this is technically a repetition, but I personally prefer having everything visible in one place.

In fact, I like to include as much as possible here. That way, if you want to change something later—like increasing the number of splits—you can do it easily without hunting through the notebook.

It’s also important to understand that cross-validation is realistically an in-between step. In practice, you could skip explicit modeling and cross-validation and go straight to parameter tuning, because parameter tuning is really the last step before forecasting the future. However, since we’re doing this step by step for learning purposes, this is how I prefer to structure it.

Finally, just to have something ready for the next step, we initialize an RMSE list. We’re going to track only one KPI, and that’s more than enough to make a decision. There’s no need to overcomplicate things at this stage.

And that’s it for now. Everything is ready.

In the next video, we’re going to actually perform the parameter tuning.

# **U) Python - Parameter Tuning**

The way we kick this off is by iterating over the parameters. That’s always step one. So we start with a loop over the parameter grid. For each set of parameters in the grid, we are going to evaluate how well the model performs.

So essentially, we loop through the grid, and for each parameter combination, we initialize a few things. We create an empty list for the fold errors, and we also prepare an empty list that will later store the average error for that parameter configuration.

Once that’s set up, we move into the next loop, which is the cross-validation loop. For each train index and test index coming from the time series split, we isolate the training data and the test data. This gives us the specific slice of the time series that we are working with for that fold.

After that, we isolate the exogenous regressors. We create the training regressors and the testing regressors, making sure they align perfectly with the train and test indices. This step is crucial because the model needs the correct regressors for both fitting and forecasting.

Now comes the modeling part. We build the SARIMAX model using the current parameter combination. From the parameter grid, we extract p, d, and q, along with the seasonal parameters, where the seasonal period is set to seven. We then fit the model.

At this stage, we are not interested in the summary output. We don’t inspect coefficients or diagnostics here. That exploratory phase is already done. In parameter tuning, the focus is purely on output, output, output—we care only about the error metrics.

Once the model is fitted, we generate predictions for the test period. With those predictions, we calculate the RMSE by comparing the predicted values with the actual test values. We triple-check that the test target and predictions are correctly aligned.

That RMSE value is then appended to the list of fold errors. This process repeats for every fold in the time series split. Each fold contributes one RMSE value to the list.

After all splits are completed for that specific parameter combination, we calculate the mean RMSE across all folds. This gives us a single performance score for that parameter set. We then append this average RMSE to our results list.

To summarize the flow clearly:
We start with an empty list → loop through each parameter combination → create a fresh list for fold-level errors → loop through each train-test split → isolate data and regressors → build the model → make predictions → compute RMSE → store it → repeat until all folds are done → compute the mean RMSE → store the result.

This structure is consistent regardless of the model you use. Whether it’s ARIMA, SARIMA, or SARIMAX, the flow remains the same.

At this point, everything looks good. The logic is sound, the structure is clean, and the code is running. If this finishes within about ten seconds, then everything is good to go. Otherwise, just give it a bit more time.

Either way, once this is done, we’ll move on to the next video, where we connect this logic to the final results and analyze them.

# **V) Python - Parameter Tuning Results**

Alrighty, so it actually took just about five minutes to run, which is quite good. It was way faster than I expected, so that’s definitely positive news. Let’s now check the output and see what we got.

The way this works is that we aggregate the RMSE values together with the parameter grid and then transform everything into a DataFrame. So first, we transform the grid into a DataFrame, which makes things much easier to inspect and work with. After that, we add the RMSE results as a column.

Instead of just looking at the head of the DataFrame, it’s much nicer to view the full table. One of the great things about working in Colab is that you can turn this into an interactive table. You can click around and immediately see which configuration performs best.

Looking at the results, the best configuration turns out to be with parameters corresponding to something like 1-1-0-2-1-0. That’s actually not very far off from what we were using before. Previously, we were using 3-1-1-2-1-0, so this is a reassuring result. It shows that parameter tuning is refining the model rather than completely changing it.

What’s even better is that we clearly see an improvement. The RMSE went from around 4.5 million down to about 4.3 million. And this improvement came from just eight iterations, which is quite encouraging. It shows how effective parameter tuning can be, even with a small search space.

Next, we extract the best parameters. There are a couple of ways to do this. One option is to convert values and items into a dictionary, which works fine. However, a cleaner approach is to keep everything as a DataFrame and use .loc to filter the row with the minimum RMSE. This keeps the structure consistent and makes the result easier to reuse.

Once we have the best parameters, we can also store them externally if we want. For example, we can save them to a CSV file like best_sarimax_params.csv. This is useful if you want to reuse the results later without rerunning the tuning process.

At this point, we are in a very good position to move forward. The next step is to start thinking about predicting the future using this optimized model. Now that parameter tuning is done, the process becomes fairly straightforward.

From here on, the main focus will be on understanding and preparing the future regressors. You can already see them in the dataset. Although there are some suggested ideas—like using lagged versions of regressors—we are not going to apply those just yet, because they were not part of the modeling step so far.

That said, lagging a feature by one time step is a very powerful form of feature engineering. For example, instead of only using today’s discount rate, you could also include yesterday’s discount rate as an additional input. This is almost like applying an autoregressive concept to the exogenous variables themselves.

We’ll focus on these ideas in the next step. The next video is really about setting everything up properly so that we can use our tuned model to predict the future.

# **W) Q&A Highlight: Handling Future Data in Forecasting**

I created this video because this is probably the number one question I get: how do I deal with future data when my model needs values that I may not know yet? The reality is that many forecasting models require information about the future in order to work properly. This could include upcoming events, planned promotions, or even weather estimates. The goal of this video is to walk through each of these aspects, explain the overall process, and share my experience as a real-world forecaster from when I worked at an e-commerce company—what we actually did, what worked, and what questions we had to ask.

The first and most important question is: why do we need future data at all? The answer is simple. Models like SARIMAX, Prophet, and LSTM require exogenous variables for future dates if those same variables were used to explain the past. In other words, if you use a variable to explain historical behavior, you must also provide that variable for the future in order to make predictions. These inputs could be marketing campaigns, holidays, major events like Easter, Black Friday, or Thanksgiving, and similar calendar-driven effects.

You also have planned promotions, which are slightly different. These are not necessarily external factors, but internally driven events such as birthday sales, flash sales, or major discount campaigns. On top of that, there are clearly external variables like weather forecasts or economic indicators. The key point here is that without these future values, the model cannot produce a meaningful forecast. It would be like asking the model to predict demand without telling it the conditions that actually shape customer behavior.

So the next natural question is: how do we actually get this future data? In practice, the answer is collaboration. In real-world forecasting, you work closely with other teams and rely on their plans and forecasts. Finance teams are a great starting point because they almost always have budgets, expense plans, and revenue targets. In larger companies—especially publicly traded ones—these forecasts exist for the rest of the quarter, the semester, and often the entire year. As a forecaster, you depend heavily on this information.

Marketing teams are equally important. They know when campaigns will run, when promotions will happen, how long they will last, and what discount levels will be applied. These things do not happen randomly; they are carefully planned both operationally and financially. Marketing and finance teams are usually tightly connected, so this information is often already aligned.

Sales teams also play a key role. They understand targets, growth plans, and what actions are expected to drive sales. Other teams may also be relevant depending on the business, but the bottom line is clear: collaborate. These teams already have the data you need. For example, if marketing plans a major holiday sale, they will know the exact dates, duration, expected discount rates, advertising intensity, and more. Go to them—this information already exists.

From an operational standpoint, creating the future dataset can feel challenging, but it follows a clear process. First, identify the regressors you need, such as temperature, discount rate, or holiday flags. Next, define the future time range you want to forecast. Are you predicting seven days, 30 days, 60 days, or even 365 days? Based on this, you create a future date range.

Then you fill in values for each regressor. For fixed regressors—like holidays or campaign dates—this is straightforward. These are often binary values that indicate whether an event happens or not. Dynamic regressors are trickier. Weather forecasts, for example, require external APIs and come with uncertainty. Economic indicators behave similarly. After filling everything in, you must carefully check for consistency, because it is very easy to make mistakes when manually preparing data.

Your target time series itself, of course, does not have future values. That’s exactly what you are trying to predict.

Another important aspect is forecasting horizons, which vary widely depending on the use case. Daily or weekly forecasts are common when fast reactions are needed, such as for demand planning or short-term promotions. Monthly forecasts are often used for financial planning and budgeting. Quarterly forecasts are critical for scheduling, reporting, and even stock market expectations—companies constantly report performance quarter by quarter. Yearly forecasts support long-term strategic planning, such as capacity planning, hiring, warehouse expansion, and investment decisions.

Regardless of the horizon, the approach to preparing future data stays the same. You extend the dataset to cover the full forecast period. If you want to forecast one full year at a daily level, your future regressors dataset must contain one row per day for that entire year. The same logic applies for weekly or monthly granularity.

At this point, a very reasonable question arises: what if these regressors are important for explaining the past, but I simply don’t have them for the future? This does happen, and it can feel tricky—but there are solutions.

One solution is N-BEATS. This model can work using only the historical target series, and optionally past regressors if they are available. In fact, N-BEATS was originally designed to work without any regressors at all. It is a very powerful model, and we cover it in the course. If this setup fits your situation, I strongly encourage you to explore it.

Another option is the Temporal Fusion Transformer (TFT). You can think of it as N-BEATS on steroids. We also cover this model in the course. TFT can handle static variables, past inputs, and future inputs—but crucially, it can also work when future regressors are missing. The main difference between the two is that with N-BEATS, you cannot include future regressors at all, whereas with TFT you can choose whether to include them or not. Both scenarios are supported.

If you don’t have future regressors, I strongly recommend trying these models and going through their respective sections. The deep learning foundations are mainly covered in the LSTM section, and the theoretical explanations for N-BEATS and TFT are included where they are introduced.

To conclude, forecasting often depends on future values of exogenous variables. When you can plan those future regressors with help from other teams, models like SARIMAX or Prophet work very well. When you cannot, models like N-BEATS and TFT really shine. The guiding principle is simple: if a variable is useful to explain the past, it is probably relevant for the future—but you need a plan for how to obtain or handle it.

Know your forecasting horizon, identify the key regressors, collaborate with the right stakeholders, and build a future dataset that mirrors your training setup. Follow these steps, and you’ll be well on your way to mastering time series forecasting.

# **X) Python - Predicting The Future Set Up**

Alright, we’ve done quite a few transformations and written a fair amount of code so far. Let’s get started right away with the next logical step.

The first thing we’re going to do is isolate X and Y. This is always the foundation. Our Y is very straightforward—it’s simply the target variable from our dataframe. Our X consists of the discount rate and the coupon rate, which we’ve already prepared earlier in the process.

Next, we need to fetch the best parameters. This is the next stage in the workflow. In principle, we could recompute them here, but since we already have them available in the same session, we’ll keep things simple and reuse them. The parameter p will be set equal to the previously identified best parameters.

One important thing to keep in mind here is that if we inspect best_params_p, we’ll notice that it’s a float stored as a NumPy type. What we want to do is transform this value into an integer—so, for example, turning 1.0 into 1. As a best practice, we explicitly apply this conversion. In the past, I’ve run into issues when this step wasn’t made explicit, so it’s always better to be very clear about what’s happening. This way, we know exactly what value we’re passing forward.

Once this is done, we move on to loading the future regressors. We load them into a dataframe—let’s call it future_reg—and take a quick look at the first few rows. You’ll notice that revenue is not present, which makes sense because this is the variable we’re trying to predict. Instead, we only have future values for the discount rate and coupon rate, and they’re currently in a slightly different format, which we’ll need to handle.

You may also notice that lag variables are present. For this particular session, we’re not going to use them, even though they’re already prepared. They were created for a different section of the workflow, but we’ll simply ignore them here.

Now, if we look closely at the date column, we can see that something isn’t quite right. The format isn’t ideal. To fix this, we explicitly specify dayfirst=True and enable date parsing. Once we do that, the dates are properly interpreted and everything looks correct. At this point, we’re good to proceed.

The next step is to isolate the future discount rate and future coupon rate. These values are currently divided by 100, which comes from how the data was sourced. This kind of issue is actually very common in real-world projects, since different data sources often use different units or conventions. To standardize things, we multiply both variables by 100 so that they match the scale used in the historical data.

We assign this transformed data to a new variable—X_future. After applying the multiplication, we verify the result and confirm that the units now match the format used earlier in the modeling process. This ensures consistency between training data and future inputs, which is absolutely critical for reliable forecasting.

At this point, we’ve completed everything we need for this video. In the next one, we’ll build the model, generate predictions, and then visualize the results.

# **Y) Python - Predicting The Future**

If you take a moment to reflect on everything we’ve done so far, it’s actually quite impressive. We started from the basics and went all the way through SARIMA, ARIMA, SARIMAX, cross-validation, and step-by-step parameter tuning. It was a lot of work, but now we’ve reached the point where everything finally comes together.

Now it’s time to build our fully tuned SARIMAX model.

We start by defining the model using our prepared inputs: the target variable Y, the exogenous variables X, and the tuned parameters p and q. Once the model is specified, we fit it and then print the model summary.

At this point—fingers crossed—it should work, and realistically, there’s no reason why it shouldn’t.

Once the model is fitted, we look at the summary output. The results look really solid. You’ll see the coefficients listed, but it’s important to understand that the raw coefficient values themselves don’t mean much in isolation. Interpretation is always relative.

For example, both the discount rate and the coupon rate are expressed as percentages. When we compare their coefficients, we can see that the coupon rate has roughly twice the magnitude of the discount rate. This suggests that coupons are more effective at driving the target outcome than discounts. That’s exactly how you would interpret these coefficients in practice.

Another good example is the autoregressive terms. If you look at the coefficients associated with different lags—say lag 7 versus lag 14—you would typically expect the more recent lag to have a higher coefficient. Higher coefficients generally imply higher relevance, and that intuition aligns well with how time series data behaves.

Next, we move on to making future predictions.

We generate forecasts for the next 30 steps and pass in the future exogenous variables using X_future. At this point, we double-check the dimensions and notice something interesting: the future dataset actually contains 31 rows, while we’re predicting 30 steps.

This is a good observation. In principle, we could have aligned this perfectly by setting the test window to 31 instead of 30. In our case, we’ve been working month over month, so this small mismatch isn’t a problem, but it’s a useful reminder.

A more robust and dynamic approach is to always tie the number of prediction steps directly to the length of the test set or the future exogenous data. That way, these mismatches never happen.

Finally, we visualize the results.

We use the built-in plotting functionality to display the historical data alongside the future predictions. We include the year 2022 in the plot so we can clearly see where the forecast begins and how the model projects forward.

Once the plot appears, everything looks exactly as expected. The future predictions are smoothly aligned with the historical data, and the forecast behaves in a realistic and interpretable way.

And with that, we’re done.

This is the full end-to-end process: from raw data, to transformations, to parameter tuning, to building a tuned model, to generating and visualizing future forecasts. This is how you build a truly strong forecasting solution.

I hope you enjoyed this journey as much as I did. As always, if you have any questions, let me know—I’m here to help. Looking forward to seeing you in the next video.

# **Z) SARIMAX Pros and Cons**

First and foremost, congratulations on completing this section.

I know this was a long one, and we went through quite a lot of steps when it comes to the actual modeling implementation. That said, from my perspective, this is actually a big advantage. Thanks to the pmdarima library, we were able to move relatively quickly despite the complexity. Having ready-made functions for tasks like cross-validation and parameter tuning makes these models far more accessible, especially for beginners.

Now that we’ve built a general structure for Holt-Winters and SARIMAX, we also have a reusable mental framework. You won’t always be able to apply it exactly as we did here, but the underlying logic is solid. And once you understand that logic, adapting it to new datasets becomes much easier.

Even though SARIMAX is considered an older methodology, it can still deliver very strong results—as you’ve clearly seen. That said, it does come with some limitations. One important drawback is that SARIMAX does not always perform well on very long time series. To be fair, this is true for many forecasting models, so it’s not a deal-breaker.

Another key point is that when a forecasting model relies heavily on autoregressive terms, it usually performs best in the short term. Think of forecasts for the next few days or weeks. As you extend the horizon further into the future, the predictions tend to become less stable and less reliable.

When it comes to regressors, SARIMAX uses a simple linear regression framework. This means that if your data suffers from multicollinearity or contains strong non-linear relationships, the model may struggle to capture those effects properly.

Finally, SARIMAX does not support multiple seasonalities. You saw that while we could model weekly seasonality, it would have been ideal to also include yearly seasonality. This limitation is shared with Holt-Winters as well. However, as we move forward into more modern time series techniques, this will no longer be an issue.

All things considered, SARIMAX is still a great model. It’s easy to apply, relatively intuitive, and remains one of those must-know forecasting tools that every data scientist and analyst should have in their toolkit.

# **IX) Section 9: PART 2: MODERN TIME SERIES FORECASTING**

# **A) Modern Time Series Forecasting Overview**

Welcome to the modern era of time series forecasting.

Ever feel like you're stuck in the slow lane of the data? Not anymore. This is where you shift into overdrive and start predicting the future like a pro, ready to make your data work harder than Jeff Bezos in his garage days. Let's get started.

Ever wonder how big players like Meta and LinkedIn stay ahead of the game? They use cutting-edge tools like Prophet and Silver Kite. And guess what? So will you. Let's turn that plain vanilla approach into something as irresistible as double choc fudge with extra sprinkles.

We'll kick things off with Facebook Prophet. Now, this bad boy is a beast at handling real-world data. You'll go from zero to hero. Setting up the Python environment and building the Prophet models makes managing seasonality and holidays look easy.

And to keep things spicy, we'll dive into a case study on bike sharing. Now let's get technical. Holidays can mess your data, but don't worry. You've got dynamic holidays covered. You learn how to incorporate this into your models and keep your forecasts sharp.

Plus, you'll get the lowdown on Prophet model parameters, learning how to tweak them for peak performance. But this is just the beginning. Next, we unleash LinkedIn's Silver Kite. This tool is like Prophet on steroids. You'll see how it stacks up and when to use it.

We'll set up Silver Kite, load and prep data, and dive into its unique features. Ever wonder how to handle different types of seasonality or manage change points and lag regressors? Silver Kite's got you covered.

Seasonality is a big deal in time series forecasting. You'll master handling different types of seasonality with Silver Kite and explore how it manages change points and lagged regressors. These features will make your models rock solid.

We'll also cover fitting data using ridge regression and gradient boosting. Learn to boost your model's performance with feature sampling and custom elements. Cross-validation is key to accuracy, and you'll practice this with both Prophet and Silver Kite.

Kite parameter tuning can make or break your model. I'll guide you through setting it up and executing parameter tuning in Python, ensuring your models are spot on. You'll see the tuning results and visualize your forecasts, making it easier to impress your boss and crush your goals.

By the end of this part, you'll be crushing time series analysis like a young, balding Jeff Bezos. See you in the next video.

# **X) Section 10: (Facebook) Prophet**

# **A) Game Plan for Facebook Prophet**

Alrighty, let's get started into the heart of forecasting with Prophet and Python. And we are going to do all of this with a very cool case study which is on bike sharing. Our adventure will begin by taking a close look at this data set, and we'll build and build and build. Imagine being able to predict the bike rental demand in any weather, season, or city event. And this is where we're headed.

And you know what is the best part? Yeah, I have not told you you'll learn all of this by doing so. You'll get hands-on experience that really sticks. So then, what I'll also do is guide you through setting up the workspace. We'll sort out all the directories and libraries, and then we'll put our data scientist hat on. We'll prep the data, clean it, and start asking questions that only EDA, or exploratory data analysis, can actually answer.

With our bike rentals data, we will explore what factors affect bike rentals more. So what is it? Weather? Is it holidays? We will find that out. And speaking of holidays, you'll learn how to factor those in when the city is either bustling with activities or really quiet as a mouse. This step is really crucial because it shows you how real-world events twist and turn data trends.

Next, we'll dive into the heart of the section and also forecasting, which is the Prophet model. We'll customize models, understand what makes them tick, and really tweak them to perfection. You'll learn not just how to use the model, but to bend it to your own will, making forecasts that are really as accurate as they are insightful.

But one thing is that data is not always straightforward. So we need to tackle anomalies, rework our approach for special days, and refine our forecasts. This is where your analytical skills are sharpened. You'll learn how to spot and smooth out bumps in the data.

Because we are not just dabbling but mastering, you'll dive into feature engineering and parameter tuning. These are your tools for turning a good model into a great one. We'll push our forecast from kind of right to spot-on.

Finally, with our model tuned and ready, you'll step into the future. We'll predict trends, demands, and really the pulse of our bike-sharing rentals with confidence. And on top of predicting, we'll also visualize it. We'll share our insights in a way that people can understand, in ways that turn heads and win nods.

Throughout this journey, you'll not just learn—you'll do. And that's the most important thing: practice, practice, practice. Each step is a step you can take to go from a normal, kind of banal time series forecasting model to a really great one. And that's the goal, really.

So we have learned quite a bit in this course, and it's time to go pro. Until the next video, have fun.

# **B) Structural Time Series and Prophet**

Do you remember the specifics of time series? In very simple terms, it is data that occurs in consecutive periods—for instance, days, weeks, or months—to understand, study, and predict time series. There are loads of concepts and some algorithms that were developed. The most common framework that is currently applied, and also my favorite, is the structural time series.

Now, what does it mean? Let's visualize. Imagine that this is our time series. A structural time series is a decomposition into, at least—and “at least” is a keyword—a trend, which is the growth of the data; seasonality, which are the cycles; the exogenous regressors, which are extra things that affect our time series; and then everything else that cannot be explained, which is the error term.

If we look at the trend, it's usually a line and represents the general direction of the data. It doesn't always have to go up, but it’s not something that fluctuates every day. We also have the seasonality, which are the cyclical patterns in our data. For instance, the consumption of ice cream is cyclical since it peaks in specific periods—specifically, the warmer months of the year. When it's colder, people don’t eat ice cream as much.

Then, we have exogenous events. These are external to our time series but can affect it. Think about the weather, economic sentiment, or even sales events, which can really impact the time series. The secret to a good forecast is really determining this exogenous impact—what they are and figuring out what makes them go up and down with the time series. That being said, even with the best regressors, one thing we need to accept is that the error is part of the prediction. It’s impossible to have 100% accuracy. If you have an error of 5%, you’re doing amazing already. Trying to achieve 100% is not only impossible but can result in overfitting, meaning your model won’t perform well in the real world.

To recap, a time series is the decomposition into trend, seasonality, exogenous impacts, and the error term, which is basically what cannot be explained by the first three. This decomposition can be further split, allowing different types of seasonalities. Mathematically, we can represent it as an equation: the time series 
𝑦
𝑡
y
t
	​

 at time 
𝑡
t equals the trend at time 
𝑡
t plus the seasonality at time 
𝑡
t, plus the exogenous regressors, and finally, the error term.

After understanding structural time series, Prophet becomes very easy. But first, let’s start with some curiosities. The first one, which you may have guessed, is that Prophet was developed by Facebook, now called Meta, in 2016. The mathematics behind it are based on Stan, which is used in causal inference. I won’t go into the details of Stan—it’s complex and beyond the scope of this course—but if you’re curious, there are plenty of YouTube resources available. And, as a fun aside, there’s a famous Eminem song called Stan.

Another revolutionary feature at the time was dynamic holidays. They’re very cool, and I have a dedicated video explaining how to model events in a way that’s not complex but easy to apply. Prophet also has intuitive parameters that are easy to grasp and a built-in cross-validation system, making parameter tuning simple even for beginners in Python.

To sum it up, Prophet brings a lot to the table. Whenever I start a new time series forecasting project, I use a pre-built Prophet script. I make a few tweaks, understand my time series data, get a baseline accuracy, and then refine it further. I’m really excited to show this to you.

Mechanically, Prophet shares many similarities with structural time series. The equation is a bit longer: 
𝑦
𝑡
y
t
	​

 is the time series at time 
𝑡
t, with trend, seasonality, holidays or event impact, exogenous regressors, and the error term. Holidays and events are especially crucial because they occur at irregular intervals—Thanksgiving doesn’t happen on the same day every year, and Easter even varies in month. Prophet allows us to model these efficiently, making forecasting much easier.

We still have seasonality, trend, and regressors, as we know from previous sections. But Prophet lets us go further with practical modeling, programming, and hands-on tutorials. There’s a lot to unpack, but step by step, we’ll practice and get started. Until the next video, have fun.

# **C) CASE STUDY BRIEFING: Bike Sharing**

Before we get started, I want to have more of a formal lecture on the case study because it’s going to be a long one, and hopefully, you find it very cool. But just to make sure that you have a complete understanding, let me give this brief introduction with the help of some slides.

The dataset is all about bike sharing and bike demand. This is where we are going to focus, try to understand it, analyze it, and ultimately predict demand. The dataset was built by Professor Hadi Forney from the University of Porto, Portugal. It takes us to the streets of Washington DC in the USA and is a detailed version of the bike-sharing data collected during 2011 and 2012.

I find this dataset extremely juicy—not just because it gives us all this rental information, but because it really makes us understand what drives rental demand. We are going to focus on factors like temperature, whether it was raining or sunny, and even the day of the week. All of these factors have a strong effect on demand.

Why am I really geeking out over this dataset? It’s packed with information—everything you could imagine. This is a proper dataset. If I were working to predict demand, this would be the kind of dataset I would try to build. Its level of detail makes it perfect for forecasting with Prophet, where we’ll see how all of these variables actually affect bike rentals.

So, what is our plan for this data? We will explore forecasting demand and understanding how city events impact rentals. We’ll even do some research online if we have questions. This helps us get a “big picture” view—not just Python programming, but really understanding, exploring, and putting on our detective hat to mimic a real-life project that any company would face.

In a nutshell, we’re going one step further with this dataset. We have the data and the documentation—please have a look at it. It’s very important to know what is happening because anyone can just create a Prophet model. But moving from step A—understanding the problem—to each step of analysis, while constantly assessing, evaluating, and questioning, is what takes you from being a mediocre data analyst or data scientist to a great one.

And that’s exactly what I want to do with you. Until the next video, have fun.

# **D) Python - Directory and Libraries**

Welcome to this practice tutorial. Please navigate to the folder: first go to Modern Time Series Forecasting, then to Prophet, and click on New → More → Google Colaboratory. For this video, we are going to focus on loading the data and preparing the script.

Let me also get our main script where we have all the core components, just to speed things up. This useful code template will be here for us to use and modify to ensure efficiency. While it opens, let me give a brief overview of this section. It’s going to be long, and the goal is to go from start to finish, trying things, sometimes failing, in a way that closely mirrors a real-world project.

First, let’s get our libraries and the initial part of the code. We will organize what we need and what we don’t. Let’s give our script a name—Prophet—and this will be your template. At this stage, we don’t need everything yet. First, we’ll mount the drive and change the working directory. Follow the usual Google Drive connection steps and continue.

As a quick overview, Prophet differs from ARIMA and exponential smoothing because its library has a lot of built-in functionality. Not everything from our template will be needed or work directly. Let’s get the path: drive → My Drive → Python Time Series Forecasting → Modern Prophet. Copy the path and we’re ready.

The libraries we need are mostly numpy, pandas, matplotlib. We don’t need sklearn because Prophet already has built-in metrics like RMSE and MAPE. We’ll also use ParameterGrid for parameter tuning because I find this approach more intuitive from a programming perspective, even though there are different ways to tune parameters in Prophet.

For the data, navigate to the Prophet folder. Here we have daily bike sharing training. Once opened, you’ll see columns like date, season, year, month, holiday, weekday, and working day (a 0-1 flag indicating whether people were working). Other important columns include weather situation, temperature, A temperature (the normalized “feels like” temperature), humidity, windspeed, and demand metrics: casual, registered, and count. We will focus on count, which is the total demand.

The dataset also comes with detailed documentation. Zoom in on the dataset characteristics. For example, weather situation is categorical even though it’s coded as 1–4 (1 = clear, 2 = mist, 3 = light snow, 4 = heavy rain). Temp is normalized temperature, while A temp is normalized “feels like” temperature. Reading and understanding these descriptions is crucial to truly “become one with the data.” Otherwise, we are just coding, not solving a problem.

Now, let’s load the CSV: daily bike sharing training CSV. For Prophet, we don’t use the index column—it will function as a normal column. Once loaded, Colab might show recommended plots, but we won’t use them because our date column is a regular column and these plots aren’t suitable for time series.

Next, check the data info. Are there null values? No, we have 701 entries, all consistent. All columns are integers except for date, which is fine. At this stage, we need to figure out which columns are essential, which ones to transform, and which can be ignored—this is a core part of data preparation. We don’t need to set frequency because we haven’t set date as the index yet; frequency will be handled later.

I’ll stop here for this video. The focus was on loading our new script and getting the data. In the next video, we will dive into preparing the variables for modeling. Until then, have fun.

# **E) Python - Preparing Data**

Welcome back! In this video, we’ll make several data changes to prepare our dataset for modeling. We’ll start by renaming variables, then adjust the date format, and finally process the weather situation variable. Even though the date is not part of the index, it still needs to follow the standard format: year-month-day.

Next, let’s focus on the weather situation variable. Although it’s numeric (1, 2, 3, 4), it is actually categorical and ordinal. Ordinal variables are categorical variables with a specific order. In this case: clear → mist → light snow → heavy rain. Using 1, 2, 3, 4 directly as numbers doesn’t make sense, so we need to rework this variable into a categorical style.

First, let’s rename our target variable for Prophet. Instead of revenue, we’ll work with count, which we’ll assign as y. For the date column (ds), we’ll convert it using pandas.to_datetime() so that it’s in the correct format (yyyy-mm-dd) even though it’s not the index. The original format is month/day/year with single digits for month and day, so we specify the format accordingly in our conversion.

Next, we prepare the weather situation variable using pandas.get_dummies(). We apply it to the weather situation column and set drop_first=True to avoid redundancy. After this, the dummy variables generated are two and three (note that situation 4 did not occur in the dataset period). We then concatenate these new dummy columns back to our main dataframe on the right side.

Once the dummy variables are added, we rename them to make them more meaningful. Number 2 becomes weather_situation_2 and number 3 becomes weather_situation_3. This makes the dataset easier to work with in Python, avoiding numeric variable names.

Finally, we drop unnecessary columns to simplify the dataset. Columns like instant (observation number), season, year, month, and weekday are removed because Prophet handles seasonality automatically. We also remove casual and registered since our focus is on count. We keep holiday and working day as they may impact bike rentals, along with temp, A temp, humidity, and windspeed. Using inplace=True ensures these changes are applied directly to the dataframe.

At this point, we’ve accomplished quite a bit: we renamed variables 2 and 3, prepared the weather situation, adjusted the date format, and removed unnecessary columns. This completes the first layer of data pre-processing. Now, we’re ready to explore the dataset more deeply in the next video. Until then, have fun!

# **F) Python - Exploratory Data Analysis**

Welcome back! In this video, we’ll focus on exploratory data analysis (EDA), using our script as much as possible. Let’s start by opening our useful code template and copying the EDA section into our Prophet template.

Our first task is to handle the date column. Since ds is a regular column, a simple way to work with it is to create a temporary dataframe and set the date as the index. We’ll create a copy of our dataframe called dataframe_temp and then set ds as its index using inplace=True. Once this is done, we inspect the head of the dataframe to ensure ds is correctly set as the index.

Next, we’ll set the daily frequency for the index using dataframe_temp.index.freq = 'D'. Now, all plotting and resampling operations will be aligned correctly with daily intervals. Every reference to dataframe in our template should now be replaced with dataframe_temp.

With this prepared dataframe, we can start looking at daily demand trends. Plotting daily demand reveals a growing trend, with seasonal cycles appearing larger in the second half of the dataset. We also notice some spikes: one at the very end near zero, several in 2011, and others in March and April 2012. These outliers are important because they highlight the portion of demand that can be explained by our regressors versus the unexplained error.

Next, we inspect monthly trends by resampling the data to monthly frequency. From January to May, demand gradually increases, peaking between May and September, with October roughly at the same level. November and December show a decline. Quarterly resampling confirms this: Q1 has the lowest demand, Q2 increases, Q3 peaks, and Q4 drops again. This confirms that our data is highly seasonal, which is where Prophet’s capabilities will be valuable.

We also attempted seasonal decomposition to see the underlying trend and seasonality. However, decomposition requires at least two complete cycles (730 days), which our dataset does not fully satisfy. Weekly decomposition (period = 7) gives some insight but isn’t fully representative. Prophet handles this more elegantly with structural time series decomposition, incorporating trend, seasonality, and regressors automatically, including complex seasonality like yearly or weekly cycles.

Next, we examined autocorrelation and partial autocorrelation. The autocorrelation plot shows significant correlation for the first 50 days, indicating that recent values strongly influence future demand. The partial autocorrelation indicates that the most relevant lags are the first 6 days, suggesting that weekly seasonality is not strong in this dataset. This observation will influence how Prophet interprets the data, since it doesn’t explicitly use autoregressive components.

In summary, after EDA, we observe a huge trend, multiplicative seasonality (amplitude increases over time), spikes in Q2 and Q3, and very low demand in Q1. Recent observations carry a lot of information, while weekly seasonality is minimal. Overall, we’ve gained a much deeper understanding of our dataset and are ready to explore Prophet modeling in the next video.

Until then, have fun!

# **G) Dynamic Holidays**

Alrighty! In this section, we’re going to talk about dynamic holidays, and I really hope I haven’t overhyped this feature because it’s genuinely very useful. I find it simple to apply, and I’m excited to show you how it works.

We’ll illustrate this with an example: Valentine’s Day, a day celebrated on February 14th. Let’s imagine we’re analyzing chocolate demand around this holiday. The demand typically increases from February 11th, peaks on the 14th, and then drops slightly on the 15th. Even after the holiday, demand remains slightly higher than average because of last-minute purchases—people forgetting or making late decisions. This creates a demand curve that rises before the event, peaks on the day itself, and tapers off afterward.

The next question is: how do we model this demand curve? With traditional models like SARIMAX, you’d need to create a separate variable for each day of interest—so in this case, five variables for February 11th, 12th, 13th, 14th, and 15th. This quickly becomes cumbersome, especially when you have multiple events.

Prophet makes this process much simpler. You only need to specify the day of the event (February 14th in our example) and define a lower and upper window of impact. The lower window specifies the number of days before the event, and the upper window specifies the days after. For our example, a lower window of -3 captures February 11th–13th, while an upper window of 1 captures February 15th. Prophet automatically applies the event effect across these days.

The result is a much more beginner-friendly and intuitive approach. You can assess the impact of each day, visualize it easily, and avoid the complexity of managing multiple variables manually. Dynamic holidays in Prophet let you capture real-world events in your forecasts without excessive programming effort.

Until the next video, have fun experimenting with dynamic holidays!

# **H) Python - Holidays**

Welcome back! In this video, we’re going to focus on holidays in our Prophet model. We’ll explore how they work, what’s already included in our dataset, and what adjustments we might need to make.

First, let’s inspect our data. We can check the holiday column to see which dates are marked as holidays. Using a subset of the data where holiday == 1, we can extract only the relevant dates. However, we notice that the holidays don’t always repeat consistently, and some important holidays like Christmas are missing. Additionally, the dataset may mark some holidays as “observed,” which means that if a holiday falls on a weekend, it’s officially celebrated on the nearest weekday instead. While this is useful, we want to make sure key holidays like Easter and Christmas are explicitly included because their impact on demand is likely relevant.

To create a complete set of holidays for Prophet, we start by defining general holidays using a DataFrame. For each holiday, we specify:

holiday: a name for the holiday

ds: the date of the holiday (as a timestamp)

lower_window and upper_window: the number of days before and after the event where the holiday may have an impact

For example, we might start with lower_window = -2 and upper_window = 2 to capture two days before and after each holiday.

Next, we explicitly add Christmas, New Year’s Eve, and Easter. For Easter, we looked up the actual dates in 2011 and 2012:

2011: April 24

2012: April 8

Each of these holidays is assigned its own identifier so we can later visualize and assess their individual effects on demand.

Finally, we combine all holidays into a single DataFrame using pandas.concat. This combined holidays DataFrame now includes:

General holidays (e.g., Martin Luther King Jr. Day, Presidents Day, Memorial Day, etc.)

Christmas

New Year’s Eve

Easter

At this point, you can also customize the impact window for each holiday based on domain knowledge, stakeholder input, or observed demand patterns. For instance, if you know that Christmas impacts sales for several days beforehand, you can adjust the lower_window accordingly.

With this setup, our Prophet model can now incorporate holidays in a way that’s intuitive, flexible, and aligned with real-world events. This is a crucial part of feature engineering for time series forecasting, as holidays often drive significant spikes or drops in demand.

Now that our holidays are ready, we can move on and get more familiar with the Prophet model itself.

Until the next video—have fun exploring!

# **I) Prophet Model Parameters**

The Prophet model has a lot of parameters for you to tweak, and thus I actually really wanted to introduce you to them just before we actually apply them. This is so that you can familiarize yourself with them. What we're going to do is that we're going to start with seasonality, which can be yearly, weekly, or daily. For daily data, we include weekly and yearly seasonality, and for hourly data, we would also select the daily option.

The important thing here is that we go one step further versus SARIMA and also Holt-Winters. If you recall, with those models we were setting this “M,” the seasonality cycle, and we could have put one. But here what we're saying is that we have more than one seasonality. If you are working with daily data, you have the weekly seasonality and the yearly seasonality. This is important because, in the end, we have different seasonal cycles that need to be accounted for, and this is now possible with Prophet as well.

The seasonality can be multiplicative or additive. This is something that we have covered as a very quick recap. If the sales of ice cream increase by 50% in August, we’re talking about multiplicative seasonality. If it increases by €70 or $70 in absolute terms, then it is additive. The difference lies in the dimension: percentages for multiplicative, absolute figures for additive seasonality.

We need to include the holidays as a DataFrame. We have built it, we know what it is, and it’s just a matter of including it here. These parameters are part of the model, and they are intuitive. We have three key values that we include, which allows us to tune the model. We'll start with the default values, and then as we move on throughout this section, we'll tune these parameters.

The first parameter is the seasonality prior scale, which reflects the strength of the seasonality curve. The next is the holidays prior scale. This determines how much the holidays actually affect the seasonality curve. The seasonality and holidays are connected, but for holidays to have an impact, they need to interfere with the seasonality curve. This is exactly what this parameter accounts for.

Finally, we move on to the trend with the change point prior scale. This controls how easily the trend changes. If the trend changes too easily, we risk overfitting. If it doesn’t change at all, we risk underfitting. The goal is to find the optimal value that allows the trend line to adjust appropriately. We'll also see this visually as we build a chart showing when the trend has inflection points or changes.

Again, this can be a lot of information, but ultimately it’s all about reflecting the components of a structural time series: seasonality, trend, and holidays. We don’t have anything specific here for regressors, but we will include them in the model. This is the part where we aggregate everything, and we'll do that in the next video.

To recap, there are three key parameters we’ll need to tune because these are values that directly affect the model. Anything else is more straightforward, as the seasonality values are already there. We can also tune the seasonality mode (additive vs. multiplicative), but we will focus on that in the future. For now, let’s build our first Prophet model.

Until the next video, have fun!

# **J) Python - Prophet Model**

Welcome back. This is a very exciting video because this is really where the Prophet model starts. Let me include it here and do shift+enter. The way that I want to work with this is by first looking at the data frame. This is always important. Checking data_frame.head() with just one row is okay. This is version 1.2. Remove any NaN or missing values that you may have—they won't work with Prophet. I usually include a drop step just in case to remove any such values. We don’t really have any right now, but this is a template and it must be complete.

Next, let's import Prophet. From prophet import Prophet. Shift+enter. Here we will be building the Prophet model. The model object is usually called m. When it comes to Prophet, they often refer to it as m. We start with Prophet and can include the seasonality. For instance, we can specify yearly or weekly seasonality, which you can see on the screen. But the auto part that Prophet sets usually works very well. Of course, you should never blindly trust technology, but in this case, it works fine.

We initialize Prophet with holidays=holidays and set the seasonality_mode, which I will start as multiplicative based on our previous analysis. Then we have the seasonality_prior_scale (default is 10), the holidays_prior_scale (default 10), and the change_point_prior_scale (default 0.05). This sets up the first part of the model. We then fit the model with m.fit(data_frame).

But we are not done yet. We need to include regressors, which must be added one by one using m.add_regressor(). Holidays are already covered, so let’s add the other variables. First, working_day, then temperature variables like temp_a and temp, humidity, wind_speed, and finally the weather situation variables weather_situation_2 and weather_situation_3. You can copy these names carefully to avoid spelling mistakes.

Once all regressors are added, do control+enter. You will see output indicating the Prophet run. For example, it may mention that yearly seasonality is disabled or daily seasonality is disabled—this is something we can review later. For now, let’s keep it simple. Set yearly_seasonality=True and weekly_seasonality=True explicitly. Then run the model again, and it will be ready.

This is how you build a Prophet model with regressors. These are the default parameters that come with Prophet, and these are the ones we will need to tune over time. For now, we keep building on this foundation.

# **K) Python - Regressor Coefficients with ChatGPT**

We'll come back. In this video, we are going to focus on the coefficients, and this is a very cool feature of Prophet: regressor coefficients. Prophet is very good at dealing with non-linearity. For instance, consider our example where we have temperature, felt temperature, humidity, wind speed—there are bound to be very high correlations here. Fortunately, in the background, Prophet compensates for this multicollinearity. This is why I love using Prophet for insights, because it handles this naturally.

To get started, we import the utility: from prophet.utilities import regressor_coefficients. Then, it’s as easy as calling regressor_coefficients(m). Here we go. For instance, working_day has a positive coefficient of 0.33, which implies that the target increases by 33%. Temperature variables are normalized, so it’s not super easy to interpret directly, but both have positive coefficients, which makes sense. For bike sharing or bike demand, if the weather is warmer, people ride bikes. If it’s humid or windy, then they don’t.

The same applies to weather situation variables. Weather situation two and three correspond to “mist plus cloudy” and “light snow/light rain,” which are not nice weather, so they have negative coefficients as expected. One thing you can do is ask ChatGPT to build a function to interpret the coefficients of a Prophet model. We tried this, but initially, the output was not ideal—especially with GPT-3.5. The interpretation function produced a dictionary, but it wasn’t very helpful, and the delta trend output was confusing.

We then tried a different approach: building an agnostic function to read tables like the coefficient table so that it can work on different datasets. This function should be able to interpret coefficients correctly, taking into account whether the model is multiplicative or additive. After iterating and refining, we now have interpret_prophet_coefficients, which provides a much clearer understanding of each regressor’s impact.

For example, when we apply this function to our model m, it outputs a table with clear interpretations: for each unit increase in working_day (0 or 1), the target variable is expected to increase by 33%. For temperature, the target increases by 81%. For weather situation two, the demand decreases by 11.56%. This gives a straightforward interpretation of the model coefficients and helps us understand the impact of each regressor.

Of course, some details like multiplicative vs additive scaling can be tricky to recall, but having this function makes it much easier to interpret the results correctly. It also handles dummy variables and normalized features, providing a starting point for understanding the influence of regressors.

It took quite a bit of time to get this function working and to interpret the coefficients properly, but persistence pays off. With this, we can clearly see how each variable affects our target, and we are ready to move forward. Let’s continue in the next video. Have fun!

# **L) Python - Cross-Validation**

We'll come back. In this video, we are going to focus on cross-validation, and we are going to make it happen. In the next video, we will focus on the performance of it, but for now, this is really about the setup and making it run. So let's kick it off. Let me also start here with a new section on cross-validation.

We do need a specific function for this. So we import it using: from prophet.diagnostics import cross_validation. Let’s do that and move forward. After that, this is where we apply the cross-validation to the model. In the end, we build a DataFrame, which I’m going to call dataframe_cross_validation, using the cross_validation() function.

To use this function, we need to include several parameters. First, we specify the model. The model is a Prophet class object, so we set model = m. Next, we need the period. The period determines how often we simulate a forecast. For example, if we do a forecast today, then in 15 days we will do another, and so on. In general, using seven or fifteen days is more than enough to give a first flavor of the model.

After the period, we set the initial parameter, which determines how much data to use for training. This is important because it sets the cutoff for the initial training set. Currently, our dataset has 701 days (dataframe.shape[0]). Usually, we subtract 180 days, which gives 521 days for initial training. This is roughly six months, which is a reasonable period to train the model. If more data is available, you could go up to twelve months, but not more than one year. The idea is to ensure that the model works now, as current performance is what matters most.

Next, we specify the horizon. The horizon defines how long we are going to predict. In our example, we take the future DataFrame from December 2nd to December 31st, which is 30 days. Therefore, we set horizon = 30. Finally, we set parallel to processes. This ensures that the cross-validation runs efficiently without needing a distributed client like Dask. Processes work best for this setup.

Now that the parameters are set, we run the cross-validation. The result is stored in dataframe_CV, which is a pandas DataFrame. We can check it using .head() to ensure it ran correctly. This process does not take very long. Essentially, what we have done is taken our data and validated it at different periods. We went back 180 days in the past, and at periods of 15 days, we made a new 30-day forecast repeatedly across the dataset.

With this, the cross-validation setup is complete. In the next video, we will focus on actually evaluating and interpreting the performance of our cross-validation results. Till the next video—have fun!

# **M) Python - Performance Metrics**

Welcome back. I realized that we didn't actually look at the output, so let's do it in this video. We will look at the cross-validation (CV) output and understand what actually comes out of it. The most important thing is to know what information we get. We can start by checking dataframe_CV.head(). Here, we see five columns.

The first column is ds, which is the date related to that forecast. This is different from the cutoff, which indicates when the forecast was made. For instance, on June 19th, we might forecast for June 20th, 21st, 22nd, 23rd, and so on. Next, we have yhat, yhat_lower, and yhat_upper. These represent the prediction and the lower and upper levels of the confidence interval. Finally, we have y, which is what actually happened. This is essentially what comes out of the cross-validation process.

So what can we do with this output? Prophet provides a function for evaluating the performance metrics. We import it using: from prophet.diagnostics import performance_metrics. Then, we call performance_metrics on our cross-validation DataFrame: performance_metrics(dataframe_CV). This gives us a variety of metrics across the horizon, such as MSE, RMSE, MAP, MAPE, SMAPE, and coverage.

As usual, I like to focus on the MAPE because it provides a clear sense of relative error, and I also pay attention to RMSE because it reflects the impact of outliers. ME (mean error) can also be useful. For simplicity, we will focus on RMSE and MAPE. To do this, we first take the mean across the metrics using .mean(). Then we round the results for readability. For example, RMSE can be rounded to zero decimal places.

For MAPE, we multiply the decimal output by 100 to express it as a percentage. After calculating, I noticed that our MAPE was extremely high—about 100%, which is massive and indicates that something is off. This could be due to predictions or possibly issues in the dataset itself.

Fortunately, there is a simple way to investigate this: by plotting the metrics over time. Prophet provides the plot_cross_validation_metric function for this. We import it using: from prophet.plot import plot_cross_validation_metric. To use it, we simply pass our CV DataFrame and specify the metric, e.g., metric='mape'. Adding a semicolon at the end avoids duplicate output in Jupyter notebooks.

Looking at the plot, we see that while most points seem reasonable, there are some extreme outliers that are causing the MAPE to inflate significantly. These small dots represent periods where the forecast error is very high. This indicates that we need to investigate both our predictions and our dataset more closely to understand the cause of these anomalies.

In the next video, we will focus on exploring these issues in detail, analyzing why the MAPE is so high, and determining how to address it. Until then, have fun!

# **N) Python - Fixing 2012-10-29 with ChatGPT**

Welcome back. Let's explore this error and see what is happening. First and foremost, we need to look at the performance metrics. Diving deep into the performance metrics, we see that our MAPE is mostly okay, but on days 12, 13, and 14, it explodes to 400%, which is massive. Something is clearly wrong. Similarly, on days 27, 28, and 29, it spikes again. This indicates a significant issue that we need to investigate.

To address this, we’ll start an exploration process, and I’m calling this section “exploring the error.” In a sense, this is where we put on our detective hats. I tried to find a detective hat, but I didn’t, so I’ll use a magician’s hat instead for this video. It’s inconvenient, but it works for now.

The first step is to identify when our predictions differ drastically from the actual values. A simple approach is to compute the residuals or deviations. We can do this with dataframe_CV['deviation'] = dataframe_CV['yhat'] - dataframe_CV['y']. Once we have this, we can look at the days with the highest deviation by sorting the DataFrame based on the deviation column. Using sort_values with ascending=False allows us to see the top deviations.

Looking at the results, we see that some deviations are extremely high. Interestingly, when we check the performance metrics, the RMSE spikes, but not as dramatically as the MAPE. The RMSE shows small increases around 1.2 to 1.5, but the MAPE explodes on certain days. This indicates that the issue is not the magnitude of the error itself but how MAPE reacts to very small actual values.

To better understand this, we compute the deviation in percentage using deviation_percentage = (yhat / y - 1) * 100. After correcting a small error in referencing the DataFrame column, we see that October 29, 2012, shows a deviation of over 11,000%. This is reminiscent of the “It’s over 9000!” meme from Dragon Ball, highlighting how extreme this spike is. October 30, 2012, also shows a significant deviation.

Investigating these dates, we find that Hurricane Sandy made landfall on October 29, 2012, affecting Washington, DC, and the surrounding East Coast areas. Heavy rainfall, strong winds, flooding, and power outages caused extreme anomalies in the data. On October 30, 2012, cleanup and recovery efforts continued, which also affected the data. These unique events caused these extreme outliers in our dataset.

To handle this, the simplest approach is to replace these outlier values with the value of the previous day. This allows us to avoid skewing the seasonal patterns and regressor effects in the model. We retrieve the value of October 28, 2012, and replace the y values for October 29 and 30 with it. This correction ensures that our cross-validation and error analysis are fair and not distorted by these rare events.

After replacing the values, we return to the modeling component. Our Prophet model and regressor coefficients might now show slight differences, but the main improvement is in our performance metrics. The MAPE now appears more realistic, around 16% when multiplied by 100, which is a better representation than the previous extreme spikes. While it’s not perfect, it’s a more accurate reflection of the model’s performance.

Examining the errors again, the days with the highest deviation now correspond to more typical events, such as July or Thanksgiving, rather than extreme anomalies. This allows us to focus on feature engineering to further improve the model’s predictive power. In the next video, we will work on feature engineering and continue refining the model.

# **O) Python - Feature Engineering**

Welcome back. In this video, we are going to focus on feature engineering. Specifically, I will be looking at temperature and its lagged values. My hypothesis is that if someone is planning their next day—whether to rent a bike or do something else—the weather they see outside will influence what they plan to do tomorrow. To test this hypothesis, we first need to see if there is a correlation between the lagged weather values and what actually happened in terms of bike demand, which is our target variable, y.

To begin, we need to create lagged variables for temperature. We will create these lagged versions for 1, 3, 5, and 7 days. I think this range is sufficient because otherwise the dataset gets too crowded. For each lag, we create a new column in the dataframe using an f-string, such as temp_lag_{lag}, and assign it the shifted temperature values. For now, we focus only on temperature (temp and a_temp), though in the future, we could include wind speed or other weather variables. The key here is to understand the method, and replacing or adding other variables is straightforward.

Once we have created the lagged columns, we can inspect the dataframe with .head() to see the new lagged values. Naturally, there will be some NaN values at the beginning of each lag series. The next step is to compute the correlation between these lagged values and y to determine how strong the relationship is. To do this elegantly, we loop over variables (temp and a_temp) and lags (1, 3, 5, 7), isolate the relevant columns, and compute a correlation matrix.

From the correlation analysis, we notice that the correlation between y and each lagged value is roughly similar across lags. This indicates that including all lags may not add much new information. In fact, the information in lag 1 is nearly identical to lag 7. Therefore, for simplicity, we can choose to include only one lagged variable, such as lag 1.

To handle the NaN values that were introduced by shifting, we remove all other lagged columns except for temp_lag_1. This is done using dataframe slicing with .iloc, keeping only the relevant columns. Although this approach is not the most elegant or dynamic, it is simple and effective. Once the dataset is adjusted, we drop any remaining NaN values and prepare it for modeling.

With the new lagged variable in place, we can run our Prophet model and perform cross-validation. The resulting performance metrics show that the inclusion of temp_lag_1 slightly changes the model's RMSE from 16.021274 to 16.01266. While the improvement is minor, it is a start and demonstrates the effect of feature engineering.

Looking at the coefficients, we see an interesting result: temp_lag_1 has a negative effect of about 95%. This suggests that if the weather was favorable yesterday, bike demand tends to decrease the following day. This counterintuitive effect may indicate that high demand on one day can “borrow” demand from the next day, which is a very interesting insight.

Although the numerical improvement in RMSE is small, adding lagged features is an important step in exploring potential enhancements to the model. The next step will be parameter tuning, which we will start in the following video. Parameter tuning is a more involved process, so make sure to spare a few minutes to code along and see it run.

Overall, this video demonstrates how to create lagged variables, check correlations, and interpret their effect on bike demand. It is a foundational step in feature engineering that can inform future model improvements. Until the next video, have fun experimenting with your own lagged features!

# **P) Python - Parameter Tuning Set Up**

Welcome back. In this video, we are going to focus on parameter tuning—or better yet, we will set up the parameter grid in this video and do the actual tuning in the next one. The results will be discussed in the following video. We are breaking this down into multiple videos so that it’s easier to follow and digest.

We start by defining the parameter grid to search. We create a dictionary called param_grid. The first parameter is the change_point_prior_scale. Here, we give it a couple of values: 0.05 and 0.5. Next, we define the seasonality_prior_scale with values 10 and 20. Then, we include the holidays_prior_scale and again give it 10 and 20. You can add more values if you want, but keep in mind that the more options you include, the longer the tuning will take. In a real-world scenario, you would want to try several values to get the best results. For the sake of this demonstration, we are using only two values for simplicity.

Finally, we set the seasonality_mode, which can be either additive or multiplicative. In this example, we start with multiplicative. While entering the dictionary, we initially get an error because we mistakenly used an equal sign instead of a colon. After correcting it to use a colon, the dictionary works as expected.

Next, we generate all combinations of the parameters. We create a list called all_params and use the parameter grid we defined earlier to generate every possible combination. If you want to see the generated combinations, you can print all_params and it will display all the possible sets of parameter values.

For now, we are only preparing the parameter options. We also create a placeholder list to store the tuning results called tuning_results, which is currently empty. In the next video, we will build the loop to run the tuning using these parameter combinations.

This sets up everything needed for parameter tuning. In the next video, we will run the tuning pipeline and evaluate the results. Until then, have fun.

# **Q) Python - Parameter Tuning**

Welcome back. In this video, we are going to build the pipeline for parameter tuning. The pipeline works in the following way: we first build a model with a set of parameters, then perform cross-validation, compute the error, and store the error. This process needs to be prepared carefully so that we can systematically test different parameter combinations.

To start, we set up a loop over all parameter combinations. For each set of parameters, we first build the model. This is step one. Step two is performing cross-validation. After the cross-validation is complete, we compute and store the error. Essentially, we are taking all the building blocks we’ve already created—model creation, cross-validation, and error computation—and integrating them into a single loop. Even though it may seem like a lot of code, because the pieces are already built, it is straightforward to assemble.

We begin by copying our Prophet model code into the parameter tuning section. All we need to do is replace the specific parameters with the current combination from our loop. This is done easily by using **params in the model initialization, which unpacks the dictionary of parameters into the constructor. The regressors remain the same, and we fit the model to the target variable as before.

Next, we add the cross-validation step. The cross-validation settings, such as a 15-day period, 521 initial days, 30-day horizon, and processes for parallelization, remain unchanged. Once the cross-validation is complete, we compute the error. In this case, we focus on the RMSE (Root Mean Squared Error) as the most relevant metric. Using the performance_metrics function, we calculate the RMSE, take its mean, and store the result.

Finally, each computed error is appended to our tuning_results list. This list was initially empty and now gets populated with the RMSE for each parameter combination tested. At the end of the loop, we will have a complete record of the error for every set of parameters, which allows us to identify the best performing combination.

In summary, in this video we took all the building blocks we had prepared—model creation, cross-validation, and error measurement—and assembled them into a loop that systematically tries all parameter combinations. The logic is straightforward, even if the programming looks a bit complex. In the next video, we will analyze the results of this parameter tuning, see how long it took to run, and determine which parameters performed best.

Until then, have fun experimenting!

# **R) Python - Parameter Tuning Outcome**

Welcome back. The parameter tuning finished much faster than I expected—just two minutes and 30 seconds. That’s really quick! Now, we are going to check the outcome of the tuning.

First, we create a pandas DataFrame to store all the parameter combinations. This will help us organize the results in a tabular format. We start by including all the parameter combinations as the first step. Step two is to add the corresponding results—the RMSE values we computed for each combination—into this DataFrame. Once this is done, we can inspect the outcome interactively, which is one of the great features of Colab.

Looking at the results, even though it initially appeared that a multiplicative seasonality mode might perform best, the tuning shows that the additive seasonality actually gives the best results. There’s a significant difference: for the best multiplicative result, the RMSE was around 1.259, whereas for additive seasonality, it dropped to 0.981.

Examining the parameter values in detail, we see that the change point prior scale is consistently 0.05 for the best results, and the seasonality mode is always additive. The holidays prior scale and the seasonality prior scale have a smaller influence; their variation doesn’t drastically affect the outcome. This highlights that the seasonality mode and the change point prior scale are the parameters that matter most for improving performance.

We also briefly explored some of the recommended plots, but nothing striking stood out due to the large number of combinations. The key takeaway is clear: the additive seasonality mode combined with a change point prior scale of 0.05 gives the best performance.

Finally, we fetch the best parameter combination. To do this, we identify the index in the tuning results where the RMSE is minimal and select the corresponding parameters from all_params. We store this in a variable called best_params. After executing this step, we have the optimal parameter set ready for future predictions.

In the next video, we will use this tuned model to predict the future. While much of the process—exploring the data, building the model, performing cross-validation, and tuning parameters—seems repetitive, the challenge is to adapt it all for predictive analytics. Essentially, we will move from training and evaluating the model to actually using it to forecast future values.

# **S) Python - Predicting The Future Set Up**

Welcome back. In this video, we’re going to focus on building the script to predict the future. I’ll be using the full workflow that we’ve developed since the beginning. Essentially, we will copy all the necessary steps, check what is needed, remove what isn’t, and ensure everything works together smoothly.

This part of the process is split into four steps:

Preparing the data – which we are doing now.

Building and tuning the model – covered in the next video.

Predicting the future – the video after that.

Data visualization – the final step.

To start, I go back to the beginning where we loaded the data. I don’t need all the library imports, but I copy the data loading and preparation steps. I skip the EDA steps since we’ve already done them, but I make sure to include the holidays, as well as the feature engineering steps for lagged variables. Some parts, like correlation checks, aren’t needed for prediction and can be skipped.

Next, I set up the future regressors. I start with the training data, dataframe_train, and then load the future features (daily_bike_sharing_future). I concatenate the training and future datasets using pandas.concat. After concatenation, I reset the index with drop=True and inplace=True to ensure it’s sequential, which avoids any potential issues later. Checking the tail of the DataFrame shows that the indices are now in order.

After preparing the combined dataset, I inspect it for null values. The columns casual, registered, and count have nulls at the end, which is expected since these are the values we want to predict. I also ensure the date format is consistent and prepare the weather-related variables. At this stage, I remove unnecessary columns to reduce clutter—keeping the system simple prevents errors and makes the script more robust.

Next, I regenerate the holidays to include the complete set up until the end of 2012. After combining all relevant holidays, I create lagged variables for temperature (lag=1) and confirm the result using dataframe.head(). A single NaN at the very end is acceptable, as it won’t affect predictions.

At this point, the data is fully prepared. Everything needed for prediction is in place, and the workflow is clean and error-free. If anyone is struggling, it’s helpful to compare their script with mine to ensure all required steps are included.

In the next video, we will move on to building and tuning the model. Until then, have fun!

# **T) Python - Tuned Prophet Model**

Welcome back. In this video, we are going to build our tuned Prophet model with our training data.

First, we need to remove the names we have just created with the lagged variable. Therefore, removing the NaNs is important. Make sure that we have a clean dataset as well. We need to focus here on just having the training data — the training data.

The way that we fit our model is that we only use the training data. Therefore, we are going to build this train, and this is nothing but our DataFrame sliced up until the last 30 days. In our case, because we have been building this to predict the next 30 days, for the training set we remove the last 30 days which are for the future regressors and we just keep the rest.

So we have this part, and this is done. Then we need to work on something here: we need to include the best parameters. And it's crazy easy because we just do double asterisk **best_params. This was something that we have built already, and this is it.

Instead of fitting to the full DataFrame, we fit to the train DataFrame that we created. I do a shift-enter to run it.

Okay, we're getting an error. Let's see what we can do. The error says: "cannot perform P.O.W. with this index type date time array."

Ah, so one thing that we're missing here is a comma. Let's see if this works.

Okay, so this works. That was the issue — not an error really, just a small typo that was easy to decode. Fortunately, it was also easy to fix.

And this was actually it. Let me just check if I've done everything that I wanted. And yes, this is it.

In the next video, we are going to use the model to predict the future. This is really the goal here — to make sure that the models we built, every analytical tool we produce, has an actual value, that it has an impact. That will be covered in the next video.

# **U) Python - Forecasting**

Alrighty, let's kick it off. In this video, we are going to do some forecasting. That will be our main focus, and we will do it step by step.

First, we need to create a future DataFrame. Using our model, we generate a future DataFrame and specify the number of periods, which in this case is 30. We have established that we are predicting the next 30 days. Let’s take a look at the future DataFrame.

If you look at it here, you see that we only have the date stamps from the 2nd of January all the way until the 1st of December. Okay, I think we are missing something. Let's go back and check.

If I evaluate my training set and notice something missing, it’s because, ideally, we should have data all the way until the end of 2012. So something must be missing. Aha! I think it is because of this NaN issue. Let’s go back and run everything from when we reloaded the data and restarted the prediction process.

Now, if we check again, everything works. Looking at our DataFrame, it is working correctly until the 31st of December. The issue must have occurred when we dropped the NaNs earlier. To fix this, I am going to put this step into the train set and drop the NaNs only from the training data. This should work.

Let me run it. I build the model and make sure that everything is working. Now, let’s build the future DataFrame. It still wasn’t completely working before, but now we see that it goes until the 31st of December. Something changed along the way, but now it works.

This is part one. At the same time, I want to include future regressors. The future DataFrame should also contain the regressors. I create future_regressors by taking the original DataFrame and dropping the columns ds and y. Running this gives us the future regressors as expected.

Next, I drop any NaNs from the future regressors. This completes part one. However, our index currently runs from 1 to 730 in the DataFrame, and 0 to 729 in the future regressors, which is not ideal. To fix this, we reset the index in future_regressors with drop=True and inplace=True. This ensures the index runs correctly from 0 to 729 and the changes are stored.

Now, we concatenate the future DataFrame and the future regressors using pandas.concat. Yes, there are a lot of small steps here, but in the end, it works. I set access = 1 and run it.

Finally, we make the forecast. Using the model, we predict on the future DataFrame we have built. If we look at forecast.head(), we can see the output. This output is quite large, including not just the predicted values but also upper and lower confidence intervals for scenario forecasting. We also have the trend, additive components, and seasonal decompositions like working day effects and yearly patterns.

Now it’s about exploring the forecast and understanding what is happening. We can see the predictions and how the data behaves structurally. The key point is that we now have y_hat for the last 30 days. This is the prediction we wanted to make.

We will stop here in this video. In the next one, we will finish this Prophet tutorial with some data visualization and a proper conclusion.

# **V) Python - Prophet Data Visualization with ChatGPT**

Welcome back. In this final video of our practice tutorial, we are going to focus on data visualization. Along with that, I will also be using ChatGPT to demonstrate what we can do with it in practice. The idea here is to explore how ChatGPT can assist us in understanding and visualizing what is happening inside our Prophet model.

The overall goal is to create a mix-and-match approach, combining ChatGPT (or GenAI) with traditional programming. This combination is very relevant in real-world workflows. Of course, there are situations—especially when very new features or tools are released—where GenAI might not yet be fully up to date. However, it is improving rapidly, and it is extremely useful from a productivity standpoint. You should definitely take advantage of it.

When it comes to data visualization in particular, I really like using GenAI. Visualization code is often longer, requires more customization, and involves a lot of trial and error. In many cases, similar visualizations have already been built by others, so instead of starting from scratch, it is much easier to ask ChatGPT for help and iterate from there.

So, I open ChatGPT and start a new conversation. In the prompt, I ask something like: “Give me five ways to visualize my Prophet model in Python.” I specify Python because there is always the possibility of using R as well, but here we want to stay within Python. I also ask ChatGPT to provide the code. The plan is to try each visualization, see whether it fits our needs, keep the ones we like, and discard the ones we don’t.

I copy all the code that ChatGPT provides and start reviewing it. The first visualization is an evaluation metrics plot, but we don’t really need this anymore because we have already evaluated our metrics earlier. So we skip that one. The next visualization looks useful, so we keep it, even though we already notice there is an error in it that we will fix later.

Next, we keep the “plot the forecast” option, which is usually very useful. We also keep the “plot the components” visualization, which is always a good one. You will often hear people say that plotting components is important, and here we will actually see why that is the case. We also decide to keep a time-series visualization. Everything else that ChatGPT provided is not really needed at this point, so we remove it. All required libraries have already been imported earlier, so there is nothing else to add.

When we start running the code, we immediately get an error saying that data is not defined. This is because the correct variable name should be df. So we replace data with df everywhere it appears. We also make sure the forecast variable is correctly referenced. After fixing that and running the code again, it works.

The first plot we see is a comparison between the actual values and the forecasted values. We notice that there are certain periods—especially spikes—that the model does not predict very well. This is completely expected. Sharp spikes are always difficult for forecasting models to capture accurately. Despite that, the overall forecast looks reasonable, and we can move on.

Next, we plot the components, which is a very interesting visualization. Instead of calling the variable model, we correct it to m. Once we run it, we see the individual components of the Prophet model. First, we see the trend, which shows a clear upward movement from the beginning of the dataset to the end. This indicates long-term growth over time.

Then we look at the holiday effects, which are always interesting. We see recurring spikes that are consistent across years, representing the impact of generic holidays. Most of the time, holidays do not have a very large impact, but there are some cases where the impact is significant. One of the biggest spikes is very likely related to Christmas, and another positive spike is likely due to Easter.

After that, we examine the weekly seasonality. The values range roughly from –200 to +600, while our actual values are often in the range of 4,000 to 6,000. This tells us that weekly seasonality exists, but it is not particularly strong or impactful relative to the overall scale of the data.

We then look at the yearly seasonality, which is clearly stronger than the weekly seasonality. However, it is not perfectly smooth or well defined. There are ups and downs, suggesting that we may not have enough data to fully capture a stable yearly seasonal pattern.

Finally, we look at the regressors, and this is where the most significant impacts appear. The regressor effects range from about –2000 to +2000, which is substantial. This indicates that the regressors are driving a large part of the model’s dynamics. In fact, without these regressors, the model’s accuracy would likely be much worse. While there may be some seasonality in the regressors, the key takeaway is that this is where most of the explanatory power comes from.

We also plot another version of the forecast, which looks very similar to the first chart. The main difference is in the visualization style: we now have lines and dots. The dots represent the actual values, while the line represents the predicted values (or fitted values). This provides another useful way to visually assess model performance.

Lastly, we use a plotting function from Prophet combined with Plotly to create an interactive visualization. This type of plot may not be something we have seen before, but it is useful because Plotly allows interactive exploration of the data. Even though the information is similar to previous plots, the interactivity can be very helpful.

Overall, all these visualizations look quite good. You can definitely explore further and customize them even more, but this is a solid approach. Using ChatGPT to generate visualization ideas and code, and then refining them manually, is a very effective workflow.

# **W) Prophet Pros and Cons**

Alrighty, so Prophet is definitely one of my favorite approaches, and I really hope that by now it has become one of your favorites as well. I want to go through the pros and cons of Prophet in a way that is as unbiased and objective as possible—although, no promises. Let’s kick it off with the positives first.

The first major advantage of Prophet is that it is extremely flexible. There are loads of possibilities that you can include, and all of this complexity actually translates into flexibility. I hope you noticed how easy it is to program with Prophet. There are built-in functions for almost everything—visualization, cross-validation, forecasting, and more.

This ease of use makes Prophet especially beginner-friendly, but it is also valuable even if you are an experienced practitioner. You don’t need to write complex custom Python functions or reinvent the wheel. Instead, you can focus more on the outcomes and insights rather than spending time on low-level programming details. That shift in focus can be extremely powerful in real-world projects.

Another important positive is how Prophet handles events and holidays. You can explicitly define an impact window, with both upper and lower bounds, and this approach is both simple and intuitive. Even better, Prophet allows you to clearly see how these events affect the time series, which is very useful for interpretation and storytelling.

Last but not least on the positive side, Prophet is excellent at handling non-linearity. When working with coefficients, Prophet often operates in terms of percentages rather than fixed linear effects. Additionally—although this is not always stated explicitly—the way Prophet handles regressors helps avoid multicollinearity issues.

For example, if you have two regressors that are highly correlated, this would normally cause problems in a standard linear regression model. Prophet, however, uses a Stan-based modeling mechanism that deals with this internally. This means you don’t need to worry as much about correlated regressors, which is an important advantage to be aware of.

Now let’s move on to the negative side.

One downside is that Prophet really benefits from hyperparameter optimization. This does take additional time. While the extra effort is not excessive and is usually manageable, it is still something you need to account for. For that reason, I consider it a con, even if it’s not a major one.

The biggest downside of Prophet—for me personally—is its weakness in handling short-term dynamics. This becomes especially clear when comparing it to models like Silver Kite, which we will look at next. Prophet does not include an autoregressive component that captures short-term changes that cannot be explained by regressors.

As a result, when there is a sudden shift or abrupt change in the data, Prophet does not model it well. It takes time to adapt, which can lead to poor short-term accuracy. This limitation means that Prophet is not ideal when you care deeply about immediate or near-term forecasting performance.

Because of this, I primarily use Prophet for long-term forecasts and insight generation rather than for short-term, high-precision predictions. If short-term accuracy is critical, Prophet can be problematic.

# **XI) Section 11: Capstone Project: Prophet**

# **A) Project Introduction**

Transcript not available;

# **B) Python - Challenge Solutions Part 1**

Let’s solve this challenge together. You are provided with two files: a PDF and a CSV file. These serve as the starting point for the challenge. The first thing I do is download both of these files. Once that’s done, I head over to ChatGPT, where I want to specifically find a GPT that is designed for working with GitHub repositories.

Now, a quick note here: if you don’t have a premium subscription, unfortunately you won’t be able to follow this part exactly the same way I do. That said, you still have access to the final Python file, which I will include in the course materials. You can also use regular ChatGPT or simply rely on the provided template. The most important thing here is solving the challenge itself and sharing one possible way of doing it.

So I search for “GitHub” inside ChatGPT, and this is our first step. Immediately, the results are ordered by popularity. I click on the first option. It has a 3.8 rating, about 17 reviews, and is categorized under programming in English. Reading the description, it says it empowers the system for comprehensive repository interaction, from code contributions to read/write operations, reviews, and advanced task automation. This sounds promising.

I also check the second option, which has around 5K users and the same 3.8 rating. It provides both general and specific guidance on publicly accessible GitHub repositories and their contents. Both options seem quite similar, so I decide to simply pick the most popular one and move forward.

Once selected, I start the chat. Now, the way I approach this is by using a prompt-engineering technique called “chain of thought.” The idea is to start at a high level and then gradually go deeper, step by step. So the first thing I ask is whether it can access the Prophet GitHub repository, specifically clarifying that Prophet is a model used for time series forecasting. This helps narrow down the scope.

The response is encouraging, which is great. From there, I ask what is new in the recent Prophet releases. At the same time, I open a new Google Colab notebook. What I want to do here is check which version of Prophet is currently installed in Colab and compare it with the most recent releases to see if there are any new features that we should be using.

I run pip freeze, wait for the environment to connect, and then search for Prophet in the output. I see that the installed version is 1.1.5. I go back to ChatGPT and ask what’s new in Prophet since version 1.1.5. The response mentions newer versions and some added features, so I want to verify whether these versions actually exist.

At this point, I notice some confusion. Versions like 1.16 and 1.17 are mentioned, but when I double-check, it turns out that 1.1.5 is actually the most recent official release. This makes me wonder whether the GPT is hallucinating newer versions. So I decide to assume that we are already working with the most up-to-date version and that there’s nothing critical missing.

Now that this is settled, it’s time to actually solve the challenge. I upload both the PDF and the CSV file. I then give a clear instruction: read the challenge from the PDF, analyze the CSV file, outline the code, and include comments in the code. That should be enough to get started.

However, the response I get back tells me that while the files were uploaded, it needs more clarity about the challenge described in the PDF. So I refine the instruction further. I explicitly state that the task is to analyze the PDF, extract all the steps needed to complete the challenge, and then apply those steps to the CSV file.

This works better, but I immediately notice some issues. First, the generated code is using fbprophet, which indicates an old version of Prophet. That’s already a red flag. Second, the solution is overly simplistic: there are no regressors included, and the CSV file is largely ignored. While cross-validation is included, there is no parameter tuning.

This isn’t necessarily wrong, but it’s incomplete. One thing that really bothers me is the continued use of fbprophet. So I push back and tell the GPT that using fbprophet implies an outdated version. I ask it to inspect the Prophet repository and ensure that it is using the most recent functions. I also explicitly instruct it to analyze the CSV file and include things like holidays and regressors in the model.

At this point, I assess the GPT’s performance so far as “not amazing,” but I’m still curious to see what happens next. Then I hit another roadblock: I’m asked to sign in with GitHub, and I get an error saying my account is marked as pending and cannot authorize third-party applications.

This is interesting, but not ideal. I try signing out and signing back in. Eventually, I realize that for this specific GPT, you actually need a GitHub account in order to authorize access properly. That’s not a big issue, but it’s something worth pointing out.

Once I authorize the application, things finally start moving. The GPT begins to improve in quality. It starts outlining steps like loading and inspecting the CSV, initializing the Prophet model, and creating a future dataframe. However, I still see that holidays are effectively ignored, and assumptions are being made instead of actually reading the CSV.

At this point, we at least have a rough understanding of what needs to be done. However, the video is getting quite long. So I decide to stop here. In the next video, we will break everything down properly. We’ll take each step one by one, place it into the notebook, and carefully implement it in a much more structured, step-by-step way.

# **C) Python - Challenge Solutions Part 2**

Welcome back. In this video, we are going to kick things off by using some of the material we obtained from ChatGPT and the GitHub GPT we’ve been working with so far. The goal is to see how far this can take us and what kind of results we can realistically get from it.

The first thing we need to do is include the required libraries and load the data. For this, we need to connect to Google Drive, which is always an important step when working with Google Colaboratory. I navigate to my Drive and point it to the capstone project directory. Specifically, I go to My Drive → Python → Modern Time Series Forecasting Capstone Project, copy the path, and then use %cd to change the working directory. Once that’s done, everything is set up correctly and working as expected.

At this point, I jump between tabs and begin the actual work. The first step is to load the data. While that’s happening, I also ask ChatGPT to analyze the CSV file and adapt the code accordingly, hoping that it will actually read the data and do something meaningful with it. It starts analyzing, which is a good sign.

Next, I focus on importing the libraries and loading the dataset. I cut and paste pieces of code as needed and double-check how the data is structured. I verify column names, check whether parentheses are missing or duplicated, and quickly inspect the dataset using df.head(). This is the very first sanity check.

At the same time, I keep the challenge requirements in mind. The first major requirement is to prepare the dataframe properly. This means renaming columns and transforming the date column. Since we usually perform these operations in place, I do that immediately. After running the cell again, I confirm that the date column is now named ds.

However, I notice an issue: the code is replacing a column called demand, which doesn’t exist in our dataset. The actual column name is total individuals in shelter. So I correct this mistake, replace the correct column name, run the cell again, and confirm that this step is now properly completed.

Preparing the dataframe also involves handling holidays. Even though this wasn’t explicitly specified in the challenge, it is important. I look at how holidays are handled, particularly when Easter equals one, and how Thanksgiving and Christmas are incorporated. I extract that logic, run it, and confirm that holidays are correctly created with lower and upper windows. The values chosen are zero and one, which is acceptable for now, so I don’t change them.

The next step is initializing the Prophet model. This part is straightforward. We also see regressors being added, including holidays. However, this immediately raises a red flag. Holidays have already been added as holidays, so adding them again as regressors duplicates information. That’s a clear mistake, and I count this as a negative point for ChatGPT.

I also notice that daily and weekly seasonality are disabled, which is actually correct because we are working with weekly data. Since we don’t have daily granularity, weekly seasonality in this context doesn’t make sense. The future dataframe creation is not perfect, but it’s good enough for now.

At this point, one important realization should be forming: when things get complex or extensive, it’s not always easy to directly apply ChatGPT’s output. You need to understand what you’re trying to do and what is possible; otherwise, things can quickly go wrong. Once you have that understanding, ChatGPT becomes much faster and more useful.

Next, I decide to explicitly handle the training and test split. I ask ChatGPT to fetch the PDF and provide the code for steps two and three. Looking at our data, I check the tail of the dataframe and realize that we don’t actually have future dates available. Unlike our earlier projects, we don’t already have data extending into the future.

This means that to truly forecast forward, we would need to fetch external data, such as temperature or holiday data. While this is doable, it’s outside the scope of what we want right now. For visualization and evaluation, we still need a training and test split, which is mandatory.

ChatGPT suggests using the last 60 days as a test set, which is reasonable. I go with that. I include this logic in the Prophet model and fit it on the training dataframe. This part is important: the model must be fitted on train_df, not the full dataset.

From there, I create the future dataframe for the next 60 periods and add temperature as a regressor. This is one way of merging external regressors, and we’ll revisit this later. When I attempt to generate the forecast, I immediately run into an error.

After thinking about it, I realize the issue: the model is treating the data as weekly, but the future periods were defined in days. I fix this by switching to weekly frequency and setting the horizon to 13 weeks instead of 60 days. Once I make this correction, everything works as expected.

Now we move to training, testing, and accuracy assessment. I copy the evaluation code, calculate the forecast for the test set, compute the absolute error, and then calculate the mean absolute error (MAE). The MAE comes out to around 29, which is quite large.

Looking at the plot, it’s clear why. The blue line represents the model predictions, and the black dots represent actual observations. There is a significant gap between them, meaning the model is not performing particularly well. This tells us that the model needs improvement.

Next, I move on to step four. Even though we already have a template from earlier work, I still like this approach for two reasons. First, it feels different, so it doesn’t feel like repetitive practice. Second, templates age over time. Revisiting problems in a different way helps challenge assumptions and discover better approaches.

Step four focuses on visualization. I organize the plots step by step, separating residual plots from error metrics. I check the forecast plot, which looks similar to previous ones. Then I examine the component plots.

The components show a growing trend that stabilizes and then declines slightly. Holiday impacts are minimal—around one thousand compared to an overall level of four hundred thousand. Extra regressors also show minimal impact. Yearly seasonality exists but is still relatively small, and weekly seasonality effects from regressors are also low. Overall, it doesn’t look like there’s much happening that could significantly improve the model.

I also examine the residual plots, but I don’t find them particularly useful. The plots are messy, not very insightful, and visually unattractive. I decide to delete them. Additionally, comparing residuals during prediction versus fitting is not entirely fair, since the model is optimized for training data. So I remove those plots as well.

Finally, I move to step five: parameter tuning. I ask ChatGPT to provide code for this step. Initially, the output is not very good—it tries random adjustments without a clear structure. When I don’t like the output, I stop it and regenerate. This is one way of providing implicit feedback.

Eventually, ChatGPT starts doing what I want. It proposes tuning parameters such as changepoint_prior_scale and seasonality_prior_scale, and it uses itertools instead of sklearn’s parameter grid. I actually like this approach because it’s different from what we used before and forces us to think in new ways.

However, I notice another issue. ChatGPT attempts to split the data using the first 80% for training. This does not make sense in a time series context. We have data from 2014 to 2021, and what really matters is whether the model performs well in the most recent period. Context matters in time series forecasting. If the model works well in the first five years but poorly in the last year, that’s unacceptable.

So I ignore that logic and stick with our existing train-test split. I also notice that ChatGPT is still treating the data as daily instead of weekly, which causes additional confusion. I fix these issues manually.

Eventually, I run into another error related to frequency definitions. ChatGPT handles these errors fairly well, often fixing spelling or parameter mistakes quickly, and that’s something I genuinely appreciate. It makes experimentation much easier.

At this point, the video has gone on for quite a while. I still need to wrestle a bit more with ChatGPT to get everything exactly right. So I decide to take a break here. In the next video, I promise we’ll finish this properly and also talk more about the pros and cons of using ChatGPT in this way.

# **D) Python - Challenge Solutions Part 3**

So, the error message explains the issue we encountered earlier. If you remember, we were working with cross-validation, and the problem comes from the fact that pandas does not support time units like six weeks or thirteen weeks. That was something I personally wasn’t aware of before. Because of this limitation, we need to convert those periods into days instead. So instead of six weeks and thirteen weeks, we now use 42 days and 91 days, which is perfectly fine.

I update the period values accordingly and run the cell again using Control + Enter. Now the code is running, which is a good sign. At this point, though, we are more or less done with the main part.

What I’d like to do next is take a look at the best parameters. Let me find where that is. Okay, here it is—this is the section where we retrieve the best parameters. I copy that part and place it at the end. The thing is, I honestly don’t know how long this is going to take to run. I probably should have checked beforehand, but it doesn’t really matter. I’ll just leave it running and move on.

Before wrapping up, I want to briefly stress something that I think is extremely important: you really need to understand what a library can do and what it cannot do. As you saw, we struggled a bit along the way, and that’s completely normal. It’s part of the learning process. For something that’s relatively new, tools like this can be very helpful, and they can save you a lot of time—but they can also become an issue if you rely on them blindly.

You shouldn’t just use tools like ChatGPT without thinking. You always need to keep the documentation in mind. Throughout all my courses and in the way I build them, I continuously refer back to the documentation to understand what’s really happening. It’s not the most exciting job—it’s research—but that’s the reality. Whether you’re an analyst, a data scientist, or just someone trying to solve a real problem properly, you have to do this if you want to go deep and do a really good job.

ChatGPT still has a long way to go, especially for more niche or advanced use cases like this one. In particular, it can struggle with the most recent or up-to-date versions of libraries. Sometimes the code it generates isn’t the most elegant, sometimes it misses things, and sometimes you need to micromanage it quite heavily. You might even get outdated patterns.

For example, in our case, we saw a simple print statement with a variable. That really should have been written as an f-string, which is the cleaner and more modern way of doing it. It’s a small thing, but it shows the point.

At this stage, I’m going to stop here. I don’t think it really matters whether we inspect the best parameters right now. I’ll share the code so that you can look at it yourself, compare it with what you’ve done, and see how things differ depending on whether you used ChatGPT, another GPT model, or a predefined template.

I’d actually be very keen to hear how you handled it, what kind of results you got, and how your approach compared. Also, this type of content—where I take a challenge and try to use AI tools to fix it—I’d really love to know what you think. Do you like it? Do you not like it? Is this something you’d like to see more of?

Your feedback is genuinely the most important thing. If we want to make this course an absolute 11 out of 10, that feedback really matters. With that, we’ll stop here, and I’ll see you in the next section.

# **XII) Section 12: Intermittent Time Series**

# **A) Game Plan for Intermittent Forecasting**

I have to say that this section exists because of you. It has been the number one request I’ve received over the last year, and that’s exactly why I decided to act on it. It’s very important to me to keep you happy, because this is part of your journey, part of your learning, and, most importantly, it’s highly relevant for you.

Because of that, I really want to encourage suggestions. Please let me know what you’d like to see. I try to act on most of them. The only cases where I usually don’t act are when it’s a request coming from just one person, because that makes it a bit difficult to prioritize. But overall, I truly try to act on the majority of suggestions. And for that, I want to sincerely thank everyone who is committed, who takes the time to give feedback, suggestions, and requests. I genuinely appreciate it.

Now, let’s start with the problem itself.

The problem here is regular time series. This is what a usual time series looks like from a daily perspective: Monday to Sunday, 24/7. That’s also very much aligned with my own experience. I’ve worked in e-commerce for a very long time, and even my own online business runs 24/7. So this “always on” mindset is deeply embedded in how I think about data.

However, I fully understand that not all data is meant to be 24/7. Take retail stores, for example. Many of them are closed on Sundays, and retail is such a large part of commerce that we absolutely need to address this properly.

Let’s take a concrete example. The dataset we’re going to work with is from a store called Rossmann. It’s a large retail chain in Germany, and their stores are closed on Sundays because, in general, almost everything is closed on Sundays. On top of that, they’re also closed on holidays, which makes the situation even more complicated. At that point, you naturally start asking yourself: what can we actually do about this?

This is where things become quite troublesome.

To give you another example, here in Berlin, where I live—and most likely across all of Germany—there are about eight Sundays per year that are exceptions. Roughly one out of every six Sundays is an exception where stores are allowed to open. As a result, you can end up with weeks where stores are open for five, six, or even seven days. In rarer cases, you might only have four open days, for example when there are two holidays within the same week.

From a programmatic perspective, this is a nightmare.

So this is the framework we’re going to use. First, we need to identify which days the stores are closed. Second, we need an external variable to represent this information. Third, we need to apply a strong model to get good results.

We’ll start by defining the outer boundaries of what we want to cover. The challenge is that most models—especially advanced ones—cannot inherently handle business logic like “if the store is closed, then sales are zero.” These models are probabilistic and non-deterministic by nature. Because of that, we need to apply a flat, programmatic rule: if closed, then prediction equals zero. This logic has to be applied alongside the model predictions.

A very fair question to ask at this point is whether there are specific models designed for intermittent time series. The answer is yes. One of the most well-known ones is Croston’s method. It’s quite an old model, originally developed in the 1970s. There have been some improvements since then, but overall it hasn’t kept up with more modern approaches or deep learning models.

Because of this, intermittent time series forecasting is still a somewhat underdeveloped area. While new methods may appear in the future, for now it is generally better to rely on advanced, adaptive time series models. The key idea is that we already know in advance when a store is closed. Since we have that knowledge, we should simply enforce it programmatically by setting predictions to zero on those days.

The real challenge, therefore, is understanding how to keep this entire flow working smoothly from start to finish: understanding the logic behind the predictions, understanding the additional information that feeds into them, and clearly seeing how everything connects so that we can make this work in practice.

That’s it for this video. In this part, it’s really more about application than theory, because the model we’re going to use is already familiar to us.

# **B) Python - Intermittent Time Series Setup**

In this section, we’re beginning our work on intermittent time series, and for this, we’re going to use a MAX-type model. Alongside that, we’ll also be using a different library—my personal favorite—the Darts library. You’ll find the starter file for this lecture inside the intermittent time series folder, which itself lives under modern time series forecasting. The Darts library will stay with us throughout this section so that we can frequently refer to its documentation. This documentation will effectively be our guide, and it’s very important that we look at it carefully.

Originally, I used to introduce Darts later, around the LSTM section, which comes after this. But for now, I’m introducing it here. It’s simply a very good library, and I personally try to use it for almost everything related to time series. If you navigate through the documentation, you can see at the very end a long list of all the models that are available. For this specific use case, we’re going to rely on Auto ARIMA.

One important thing to understand about Darts is that it often retrieves models from other underlying libraries. That’s completely fine. What really helps us is that the overall modeling interface becomes very consistent. Even if the underlying implementation comes from different sources, the way we work with models remains very similar across the board.

If you look at Auto ARIMA in particular, it’s slightly different from the regular MAX model. We’ll talk about the pros and cons of using Auto ARIMA versus a standard MAX approach throughout this section. The main difference is that Auto ARIMA is much more “autopilot.” You give it the data, and it makes many decisions on its own. That means you have less control compared to a fully manual approach. But I don’t want to get ahead of myself just yet.

The main purpose of this video is very simple: to install the Darts library. You can feel free to install the latest version. It usually just works, and it works very well in Google Colab. I haven’t explicitly shared it here, but when you receive the course materials, you’ll also get a requirements file. So if you’re using Jupyter Notebook, VS Code, or any other environment, you can simply install the requirements, and everything should work smoothly.

I’ve also included a few helper functions here. This is meant to be an introduction, so I don’t want to go overboard. From the Darts library, we’ll only focus on a small subset of functionality for now, because the main focus is intermittent time series—by far the number one request I’ve received for this course. I’m genuinely very happy to finally bring this content to you. We’ll run all of this code and then move on to our dataset, which is actually quite cool.

The dataset we’re going to use comes from a store called Rossmann. By coincidence, it’s a very popular retail chain here in Germany. The data itself is in German, and although their website isn’t available in English, the concept is straightforward. Rossmann sells health-related products, cosmetics, pet care items, basic grocery products, and similar everyday goods. It’s a very German-specific type of retail store from my perspective, especially since we didn’t really have this exact concept back in Portugal.

That said, it’s an excellent dataset from a time series forecasting point of view. Retail forecasting is notoriously challenging. There’s high variability in sales, strong dependency on opening and closing days, location effects, promotions, and holidays. All of this makes retail data very tricky—and therefore very interesting—to model.

For now, we’re going to keep things simple and explore just one time series at a time, focusing on a single store. Later on, you’ll learn how to handle multiple time series, especially in the LSTM section. This same dataset could also be used with LSTM or other deep learning models, but we’ll get to that later.

At this point, we install our libraries. You might notice an error like “no such file or directory.” If that happens, it’s usually resolved by mounting your Google Drive, which is exactly what we do next. Once the drive is mounted, we can access our data correctly.

Looking briefly at the dataset, we have variables such as store ID, day of the week, date, sales, number of customers, whether the store is open, promotions, state holidays, and school holidays. One thing I’d like you to think about already is this: which of these variables actually make sense to include in a forecasting model, and which ones do not? That’s an important question to keep in mind.

Once the data is loaded, we immediately see a warning about mixed data types in one of the columns. We’re not going to cover that fix here, but it’s something we’ll need to address. When we inspect the dataset further, we see that it’s massive—over one million observations. Since there are so many stores, we’ll focus on just one to keep things manageable.

The good news is that the dataset has no missing values, which makes life much easier. That’s also what you’d generally expect from a dataset like this. From here, we’ll explore the data, decide which features to keep, which ones to remove, and how to prepare everything properly.

With that, the setup is complete. Data preparation is coming next.

# **C) Python - Data Prep**

In this video, we’re going to do quite a few things. The first step is to subset the data to just one store. The reason for this is that Auto ARIMA cannot handle multiple time series at the same time. We’ll deal with multiple time series later, using deep learning models. So for now, step one is to focus on a single store.

Step two is to take a closer look at the date variable. We need to understand how it’s currently structured, what’s wrong with it, and how to fix it. At some point, this date must become the index of our dataset, and that will be the main focus of this video. If we still have time, we’ll briefly look at some of the other variables, but for now the priorities are subsetting and fixing the index.

So let’s start by subsetting the data to include only store 1. Once we do that and preview the dataframe, this is our initial result. Immediately, one thing stands out: the dates look wrong. For example, the 31st appears before earlier dates. The order is clearly incorrect, and that’s something we definitely need to fix.

As you already know, one thing I really like to do is rename variables to make them cleaner and more consistent. In this case, we rename sales to y, and date to ds (for datestamp). We do this using a dictionary in the columns argument, mapping sales → y and date → ds. That’s our next step.

After that, we need to format the ds variable as a proper datetime object. This is how we explicitly tell Python that this column represents dates and how it should be interpreted. Once we do this, we can confirm it by checking the dataframe information, where we now see that ds is of type datetime. That’s progress.

However, if we preview the dataframe again, the dates still look off. So the next step is to set ds as the index. This is an important step, and the approach I personally prefer is using set_index with inplace=True. It’s clean and elegant.

Once we do that and preview the dataframe again, we can see that ds is now the index. But we’re not done yet. The order of the dates is still not usable. If we tried to feed this directly into a model, we would definitely run into issues.

To understand what’s happening, we look at the index itself. We can see that although the index is a datetime index, the frequency is still set to None. Python knows these are dates, but it doesn’t know how frequently they occur.

The fix for this is fortunately very simple. We explicitly set the index frequency to daily. Once we do that, everything falls into place. If we now inspect the index again, we see that it has a daily frequency, and the data is properly structured in time order.

At this point, the main goal of this video is achieved. We’ve successfully subset the data to one store, fixed the date column, converted it to a datetime format, set it as the index, and ensured the correct daily frequency.

I’m going to wrap up this video here.

In the next one, we’ll start focusing on the remaining variables: open, promo, state_holiday, and school_holiday. We’ll begin specifically with state holiday, because that’s the column that triggered the warning earlier (column seven). That’s where we’ll pick things up next.

# **D) Python - Feature Engineering**

In this video, we’re going to work a bit more on the dataset. We’ll do a small amount of feature engineering and then take a deeper dive into the state holiday variable. This is the variable that triggered the initial warning we saw earlier, so it deserves special attention.

If we look at the state_holiday column, we immediately notice something odd. We see a zero, then another apparent zero, but they’re not actually the same. This strongly suggests a data type issue. One of them is likely a string value, while the other is a numeric value. In addition to that, we also see values like A, B, and C.

To make this clearer, we can explicitly inspect the unique values in the state_holiday column. When we do that, the issue becomes obvious: we have '0' (a string), 0 (a number), and then the values A, B, and C. This confirms that the column has mixed data types, which explains the warning we received earlier.

Now, there’s another important observation here. The number of occurrences for A, B, and C combined is very small—less than 30 in total—while the zeros occur hundreds of times. Because of this imbalance, I don’t particularly like treating these categories separately.

Instead, what I prefer to do is map the state_holiday variable in a very simple and robust way. The idea is straightforward:

If the value is A, B, or C, we map it to 1.

Everything else becomes 0.

This approach is clean, easy to understand, and, most importantly, generalizable. If we later apply the same logic to other stores, we don’t have to worry about edge cases or specific category distributions. It’s an all-encompassing rule that will consistently work.

We implement this using an apply function with a lambda expression, and we make sure to store the result back into the state_holiday column. After doing this, if we preview the dataframe, we can see that state_holiday is now a clean binary variable with only ones and zeros.

Next, we briefly check the school_holiday variable to make sure there are no similar issues. When we inspect it, we see that it already consists of just zeros and ones, so it’s good to go and doesn’t need any additional processing.

The next stage is to remove variables that we don’t need.

Looking at the dataset, there are a few columns that clearly don’t add value for our current modeling setup:

store: we’re only working with one store, so this column carries no useful information.

day_of_week: since we’re using a seasonal model with a weekly (7-day) seasonality, this information is already implicitly captured.

customers: this variable is fundamentally intertwined with sales. A sale happens because there is a customer, and a customer is recorded because a sale happens. There’s no meaningful causal direction here—they’re essentially the same information expressed differently. Because of this strong dependency, we should remove customers.

The variables we want to keep are:

open (this is critical for the intermittent nature of the series),

promo,

state_holiday,

school_holiday,

and, of course, the target variable y.

We then drop the unnecessary columns using the drop method with columns=... and set inplace=True. After fixing a small execution issue and re-running the command correctly, we confirm that the dataframe now contains exactly the variables we want.

At this point, the core data preparation is complete.

In the next video, we’ll do a very brief exploratory data analysis. Feel free to follow along using the script. With autocomplete support available, we’ll be able to move through that part quite quickly.

# **E) Python - Time Series EDA**

All right, now we move on to EDA, and we need to be quick here. We already know how to do exploratory data analysis in Python, so let’s kick it off efficiently.

The first thing we do is plot the time series itself. We simply plot df['y'] to get a quick visual understanding of how the data looks. From this plot, we can see that the series is fairly stable overall, with a slightly decreasing trend. There are clearly some effects that suggest sales are decreasing over time, but nothing dramatic. What stands out immediately is the strong seasonality, especially visible through all the zeros on Sundays. Overall, this very much looks like a seasonal time series.

Given that observation, if we think ahead, we would expect this seasonality to show up clearly in the autocorrelation and partial autocorrelation plots. Since the trend is relatively weak, the series feels quite stable. When a series is stable like this, it usually means that the immediately previous values don’t carry an overwhelming amount of information, because there isn’t a strong upward or downward momentum driving the series.

With that intuition in mind, we go ahead and plot the ACF and PACF. We use around 50 lags, which is a reasonable and informative number for this kind of data.

Looking at the autocorrelation plot, what we see matches our expectations. The autocorrelation structure is very stable. The correlation at lag 49 is very similar to what we see at lag 7, or at least on the same scale. This tells us that the dominant signal is seasonal rather than driven by short-term dependencies. There isn’t a lot of unique information coming purely from the immediately previous days, which is something we more or less expected.

When we move to the partial autocorrelation plot, things get even more interesting. Here, we clearly see strong seasonal information extending across the previous four weeks. That’s an important insight. We also notice some negative values, which suggests that higher sales on one day can negatively affect sales on subsequent days. That’s an interesting behavioral pattern, especially when observed in the short-term lags.

At this point, there’s not much more we need to do from an EDA perspective. We’ve confirmed stability, strong seasonality, and some interesting short-term dynamics. That’s enough for our purposes here.

In the next video, we’ll move on to something quite important: data preparation for Darts. The way data is fed into Darts models is fairly consistent across different models, so what we learn there will carry forward into future sections as well. We’ll go through it briefly, using the documentation together, and this will form the foundation for what comes next.

# **F) Python - Darts Data Preparation**

In this video, we focus on data preparation, specifically preparing the data so that it can be used with the Darts models. The first step, which is very common across almost all modeling workflows, is to isolate the target variable. In our case, that target is y.

We start by extracting y from the dataframe. At this point, it’s still just a pandas object, so nothing special has happened yet. Before storing it permanently, it’s useful to walk through the transformation step by step so we clearly understand what’s going on.

What we have initially is a pandas dataframe. Now, to work with models in Darts, the data needs to be converted into a TimeSeries object. Looking at the documentation, we see that a TimeSeries can be created directly from a dataframe. If a time column is provided, it will be cast to a datetime index. If not, the dataframe index itself is used.

In our case, this is perfect, because our time information is already in the index. That means we don’t need to specify a time column at all. We can simply pass the dataframe to TimeSeries.from_dataframe, and that’s it.

So we take our isolated y and convert it into a Darts TimeSeries. Once we do this, the data is no longer a simple pandas structure. Instead, it becomes a specialized object that contains not only the values, but also metadata such as the number of components, coordinates, and time information. This format is unique to Darts, but the good news is that most things inside Darts follow this same structure. Once you understand it, working with it becomes very straightforward.

We then store this transformed object as our target. That completes step one.

Next, we move on to the covariates. To do this, we take the original dataframe and drop the target column y. Everything that remains becomes a covariate. We then convert this dataframe into a TimeSeries in exactly the same way as before.

This time, the resulting object has multiple components—one for each covariate. We can see their values, their names, and the associated metadata. Again, this is very easy to implement once you’re familiar with the pattern.

At this point, it’s worth mentioning static covariates. We’re not using them here, but they become very important when working with deep learning models and multiple time series. When we get to that stage later, static covariates will definitely come into play.

The next important step is scaling.

We do not need to scale the covariates in this case because they are binary variables—zeros and ones. You can already consider them scaled. However, our target variable y is not scaled, and scaling is important for many models, including Auto ARIMA in this setup.

So we create a scaler and apply it to the target time series. We use fit_transform, which first fits the scaler to the data and then transforms it. When we preview the scaled target, we can clearly see that the values now lie between 0 and 1. That confirms everything worked as expected.

And that’s it for this video.

At this point, we are fully ready to build our Auto ARIMA model. The scaler itself is very straightforward—there isn’t much more to it beyond fitting it to the data and transforming the values.

# **G) Python - AutoArima**

All righty, let’s do this—Auto ARIMA.

The first thing we do is open the documentation and look for Auto ARIMA. And here it is. One important thing to keep top of mind is that this implementation is based on the StatsForecast package. So before we go any further, it’s worth opening that reference and understanding where things are coming from.

Auto ARIMA in Darts builds on models provided by StatsForecast. That’s perfectly fine, and actually very helpful, because it gives us a consistent modeling interface while relying on a well-tested backend. There have been a few requests to add StatsForecast explicitly as a standalone section, and I’m honestly considering it, because it can be very interesting on its own—but that’s a side note for now.

Let’s focus on how Auto ARIMA works.

When we look at the documentation, we see that Auto ARIMA supports feature covariates, which is exactly what we need. That means we can pass our external regressors—like open, promo, state holiday, and school holiday—directly into the model, and it will just work.

Now, one important conceptual point about Auto ARIMA is this: although it’s “automatic,” we still need to understand how it chooses its parameters. Auto ARIMA selects the model orders using an information criterion—specifically the AIC (Akaike Information Criterion). We’re going to cover AIC in detail in the next video, but as a quick spoiler, AIC balances two things:

How well the model fits the data

How simple the model is

In practice, we want both: good fit and low complexity.

Up until now in this course, and generally in most applied forecasting workflows, we’ve been selecting models using metrics like RMSE or MAE. That is still the gold standard. If that approach is “100% correct,” then using AIC is maybe “80% correct.” But that’s okay. It’s fast, easy to implement, and gives us a very strong baseline—especially for something like intermittent time series.

This makes Auto ARIMA an excellent starting point. It’s quick to set up, supports covariates, works well with cross-validation, and gives us a solid benchmark. If later you want more control or more advanced tuning, you can always move to other models using a similar process.

Now let’s actually build the model.

First, we import Auto ARIMA. Model imports usually take a little longer, and that’s normal. Once it’s imported, we define the model itself. The setup is surprisingly simple. There are only a few parameters that really matter for us here:

seasonal=True, because we clearly have seasonality

seasonal_length=7, since our shortest and most important seasonal cycle is weekly

stepwise=True, which is optional but highly recommended

The stepwise option is important for performance. Instead of trying all possible combinations of ARIMA parameters, the model uses a stepwise search strategy. It starts with a small set of candidate models and then explores variations around the best one, adjusting parameters up or down by one step at a time. As soon as it can no longer find an improvement in AIC, it stops.

This stepwise approach comes from classic forecasting literature, most notably the work popularized in Forecasting: Principles and Practice. This book has been a foundational reference for ARIMA modeling for many years. Even though the original implementations were often in R, the same logic applies here in Python.

In practical terms, stepwise search often cuts computation time roughly in half, which is especially useful when you’re doing cross-validation. That’s why we enable it.

With that, the model definition is complete.

The next step is to fit the model. We fit it on the scaled target time series, and we pass the future covariates that we prepared earlier. That’s it—the model now has everything it needs.

There was a lot of explanation here, but conceptually the process is simple:

Define Auto ARIMA with seasonality and stepwise search

Fit it on the scaled target

Provide the future covariates

We’ll stop here for now.

In the next video, we’ll take a closer look at AIC and BIC, understand what they really mean, and why Auto ARIMA relies on them.

# **H) AIC and BIC**

In this video, I’ll cover AIC and BIC, and also explain how they connect to our Auto.ARIMA function. Let me start by clarifying what these terms actually mean.

AIC stands for Akaike Information Criterion, and BIC stands for Bayesian Information Criterion. You can think of both of them as KPIs that help us choose the best model. The way they do this is by assigning a score to each model, almost like judges in a talent show evaluating performances.

So what is really happening here? AIC and BIC give us a score, but how do they calculate that score? They take into account two main components. The first one is goodness of fit, which tells us how well the model fits the data. The second one is model complexity, which you can think of as the number of parameters the model is using.

The AIC is all about balance. It wants a model that fits the data well, but at the same time does not go overboard by using too many parameters. A good way to think about this is like baking a great cake using the minimum number of ingredients needed.

The BIC, on the other hand, works in a very similar way, but it applies a much harsher penalty when the model becomes complex. Because of this stronger penalty, BIC usually prefers simpler models. Compared to BIC, AIC is slightly more flexible and focuses more on finding the right balance rather than strongly discouraging complexity.

Now let’s connect this idea to Auto.ARIMA. For every model that Auto.ARIMA tries, it tests different combinations of the autoregressive (p), integrated (d), and moving average (q) components. For each of these combinations, Auto.ARIMA computes the AIC and BIC scores. The model that ends up with the lowest score is usually considered the best choice. This is very important to remember: lower is better.

There are several advantages to using a process like this. First of all, it is straightforward and objective. It allows us to compare different models using a single KPI, making the selection process more data-driven. At the same time, the penalty for complexity helps reduce overfitting, which increases the chances that the model will generalize well to new data.

Another important benefit is flexibility. AIC and BIC are not limited to ARIMA-based models like ARIMA, SARIMA, or ARIMAX. You can also use them in other areas such as segmentation and regression analysis. This makes them valuable tools to have in your overall modeling toolkit.

However, there are also some limitations to keep in mind. AIC and BIC do not give you an absolute “good” or “bad” score. Their values are only meaningful when you compare them across multiple models. To be fair, this limitation applies to most KPIs as well.

Another drawback is that they do not directly align with business-focused metrics. From a business perspective, companies usually care more about error metrics, such as how far the model’s predictions are from the actual values. AIC and BIC focus on goodness of fit and complexity, and businesses typically do not care much about model complexity itself.

Lastly, because these criteria penalize complexity, there is a risk of information loss. In some cases, more complex models may capture important nuances in the data, and AIC or BIC might dismiss them too early.

Now, if we zoom in on the process itself, the function simply runs through many combinations of p, d, and q, calculates the corresponding AIC and BIC values, and compares them. Once again, the model we choose is the one with the lowest value. This point is critical and worth repeating.

To sum it all up, AIC and BIC are extremely helpful because they allow us to automatically try different parameter combinations in a structured and objective way. You saw how easy this was with the ARIMA model, and the same applies to SARIMA and ARIMAX models as well.

Of course, these methods are not perfect. Eventually, we should focus more on error-based metrics such as MAE, RMSE, and MAPE, which we will do later. But for now, using AIC and BIC is absolutely fine, and they give us a solid way to assess and compare our models.

# **I) Python - Cross-Validation**

All right, we’re back. This step was actually super quick. It only took a few seconds at most. You can see that the process shows the attempts, and it didn’t take long before it found an optimal value.

Now, what we’re going to do next is cross-validation, which is always a very important step. Cross-validation helps us understand how well our model performs across different points in time, instead of just fitting well on a single training window.

The first thing we need to define is the forecast horizon. Personally, I usually like to use 30 days when we’re working with daily data. For me, this is generally the best approach. Forecasting just the next week often feels a bit unrealistic, because in most operational settings, not much actually changes within one week. Most real-world forecasts tend to focus on a 30-day window, and sometimes even up to 90 days, depending on business requirements and enterprise needs. For now, we’ll start with 30 days, and this is what we’re going to evaluate.

Because we are using auto.arima, parameter tuning does not apply here. The model already handles that internally by selecting the best parameters based on information criteria.

Next, we perform cross-validation using a rolling forecast approach. We define forecast_cv using the model’s historical_forecasts method. Let’s quickly look at the documentation to understand what this method expects. It’s not listed under attributes but under methods, specifically historical_forecasts.

This method requires the series, as well as the future covariates, which are the ones we already fitted earlier. We then specify the forecast horizon, which in our case is set to 30 days.

There are a few additional parameters available, such as train length and start, but those are not strictly required. One parameter that is often important is stride. There are multiple ways to use stride. For example, if you have a 30-day forecast horizon, you might decide that every week you forecast the next 30 days. In that case, you could set the stride to one-fourth of the forecast horizon. For now, we’ll keep it simple and set the stride equal to the forecast horizon itself. This means we forecast the next 30 days, then the next 30 days again, and so on.

Another important parameter is retrain, which we set to True. I always prefer to have retraining enabled. In our case, the goal is to evaluate auto.arima in a setup where the model continuously re-optimizes itself by finding the best parameters according to AIC. Since this is the behavior we want to evaluate, retraining should definitely be included.

There are a few more optional parameters, such as verbose, which controls how much information the function prints while running. Personally, I don’t care much about verbose output here, so I leave it disabled.

One parameter worth discussing is last_points_only. This method can return either a full sequence of forecasts or just the last predicted point. If we were to set last_points_only=True, and our forecast horizon is 30, then we would only get the 30th predicted value. For our use case, that doesn’t make much sense, so we explicitly set last_points_only=False.

Now let’s talk about the start parameter. This controls when the rolling forecast begins. Based on my experience over many years of forecasting, a good approach is to start far enough in the past to allow multiple evaluation windows. In this case, we look at the total length of the dataset and subtract the forecast horizon multiplied by 12. This gives us approximately 10 to 12 evaluation windows, which is usually a solid and reliable setup.

At this point, we encountered an error: an unexpected keyword argument called target_series. This is because the method expects the argument to be named series, not target_series. So we replace that with series=target_scaled.

We also need to include the future covariates, which we pass using future_covariates=covariates. Some parameters such as train length are removed, and we keep only the required ones: series, future covariates, start, forecast horizon, stride, retrain, and last_points_only.

After correcting these parameters, we run the function again. This time, it appears to be working correctly.

I’m going to stop here. The cross-validation is now running, and in the next video, we’ll explore the output in detail. I’ll see you there.

# **J) Python - Cross-Validation Results and the Zero issue**

What we’re going to do now is eventually build a loop, but before that, I really want us to go step by step and carefully inspect the output. The result of the cross-validation is stored in cv, so let’s start by looking at that.

At first glance, the output looks a bit odd. There are quite a few things going on here. The key thing to understand is that this output behaves like a list. Since we ran 12 cross-validation folds, we should expect 12 outputs. Each element in this list corresponds to the predictions from one cross-validation window.

So, for example, cv[0] gives us the first set of predictions. When we inspect those values, we immediately notice that they are scaled. That means the very first step we need to take is to apply the inverse transformation using the scaler. Once we do that, we get the predictions back in their original scale.

Now, looking at these values, it’s clear that something feels off. We can see negative values, which realistically should not exist in this context. These values simply don’t make sense for the problem we’re solving. For now, they are there, and we’ll deal with them properly in the next steps, but it’s important to acknowledge that this is an issue we need to address.

Before fixing that, let me show you how to retrieve and align everything properly. A very useful next step is to convert the predictions into a time series. Once we do that, we still have the same values, but now they are indexed by dates, which makes further analysis much easier.

What we can do next is retrieve the start and end index of the predictions and use those to extract the corresponding actual values. We take the minimum index from the predictions as the start date and the maximum index as the end date. Using these two dates, we slice the original dataset to get the actual values over the same period.

When we do this, we can confirm that everything lines up correctly. The predictions cover the period from the 6th of August to the 4th of September, and the actuals match that same window. At this point, we’re in a very good position because our predictions and actuals are properly aligned.

Now, this brings us to an important discussion around intermittent time series. In practice, there are very few models that can naturally handle intermittent behavior and explicitly predict zeros when, for example, a store is closed. Mathematically, you can’t just force a model to “slap a zero” into the prediction—it doesn’t work that way.

There are models that were specifically designed for intermittent time series, but these are generally older models. In my perspective, even a relatively simple model like auto.arima will often outperform those older approaches, especially when compared to modern deep learning models or when working on larger and more complex problems. Those older intermittent-demand models simply don’t scale or perform well enough.

Because of that, the simplest and most practical solution is to apply a rule after prediction: when the store is closed, we set the prediction to zero. That’s it. If the “open” indicator is zero, the prediction becomes zero as well. This approach is straightforward, effective, and widely used in practice.

Once we apply this rule, we can inspect the predictions again and confirm that the values correctly drop to zero when the store is closed.

Now we move on to evaluation. We calculate the RMSE (Root Mean Squared Error) by comparing the actuals with the predictions. For this particular fold, we get an RMSE value of 588. Of course, you could calculate additional error metrics if you want, but RMSE is sufficient for now.

What we want to do next is generalize this across all cross-validation folds. We create an empty list called rmse_cv. Then we loop over all cross-validation outputs using a for loop. For each fold, we repeat the same process: inverse transform the predictions, align them with the actuals, apply the zero rule when the store is closed, compute the RMSE, and append it to the list.

At the end, we print all RMSE values and compute the mean RMSE across folds using NumPy. After fixing a small indexing mistake and replacing a hardcoded zero with the loop variable, everything runs correctly. The final average RMSE comes out to 792.

That’s it for this video. In the next one, we’ll focus on visualization. We’ll explore how the model performs across different folds, see where it does well, and identify where it struggles. I honestly expected a bit more from some of the folds, so we’ll dig into that and see if there are any potential issues or opportunities for improvement.

# **K) Python - Plotting Cross Validation**

I really like to visualize things, especially when it comes to cross-validation. I find that visualization is usually very insightful, because it helps us understand whether there’s something odd going on—whether a specific residual stands out or if there’s something truly out of the ordinary. For this reason, we’re going to visualize CV[0], comparing it directly against the actual values.

We’ll start by gathering everything we need. This usually follows the same pattern as before. We focus on the first cross-validation fold by explicitly selecting CV[0]. From there, we take the predictions, apply the inverse transformation using the scaler, and then handle the zeroing logic. After that, we retrieve the first and last index of the predictions and use those to slice the actual values over the same time range. Finally, we plot both the actuals and the predictions using plt.plot, and we add a legend so we can clearly distinguish between them.

When we first plot this, the output looks pretty ghastly. The default figure size just doesn’t work well here. So we fix that by explicitly setting the figure size. A figsize of (10, 6) usually works very well for me, and once we apply that, the visualization already looks much better and easier to read.

The next issue is the legend placement. By default, the legend isn’t ideal—it feels a bit awkward and distracts from the plot. To fix this, we go into the Matplotlib documentation and adjust the legend parameters. By specifying the legend location explicitly, we can control where it appears. After trying a few options, it turns out that placing the legend in the lower right works best for this plot. It keeps the visualization clean and avoids overlapping with important parts of the data.

With the plot cleaned up, we can now actually interpret what we’re seeing. Overall, the model performs quite well in this fold. The predictions are relatively stable across most days, and they don’t fluctuate much. This makes sense when we connect it back to the underlying data and the autocorrelation analysis we did earlier.

If you recall, the autocorrelation showed very little information in the recent past—especially in the last six days. Because of that, we wouldn’t expect the autoregressive component to have a strong effect. As a result, the predictions remain fairly flat, which is exactly what we observe here. This behavior is expected and actually reassuring.

Visualization also allows us to spot areas that might deserve further investigation. For example, when looking at the period just before Christmas, we can clearly see strong seasonality in the actuals (the black line). This immediately raises a question: could we improve the model by adding additional variables that better capture this seasonal behavior? These are the kinds of insights that only really become obvious when you visualize the data.

This is where domain knowledge, data understanding, and visualization all come together. By knowing the industry, understanding the data, and carefully inspecting plots like this, you can often find opportunities to significantly improve your models.

That’s it for this video. I want to leave you with a challenge to think about how you could extend or improve this approach. I’ll see you in the next one.

# **L) Predict the Future + Challenge**

In most—if not all—sections of this course, we usually end up predicting the future. Interestingly, in this section we haven’t done that yet, and that’s very intentional. This is actually one of the challenges I want to leave you with: predicting the future using everything we’ve learned so far.

Once you start thinking about it, you’ll realize that there are a few non-trivial steps involved. The first one is that you need to create a future data frame. This alone can already be a bit tricky, because you’re no longer working with observed data—you’re working with assumptions and best guesses.

If we look at our existing data frame, we have variables such as Open, Promo, StateHoliday, and SchoolHoliday. Since this is Germany, you’ll need to go online and figure out whether there are state holidays or school holidays in the future period you want to forecast. You can assume that the store is located in Berlin (which is also a state), and then determine whether, for the next 30 days, there are any state holidays or school holidays. That’s challenge number one.

The Open variable is challenge number two. It’s related to holidays, but not only to state or school holidays. You also need to consider national holidays, Sundays, and similar effects. The store being open or closed directly affects sales, so getting this right—or at least making a reasonable assumption—is crucial.

The Promo variable is by far the most challenging one. Here, you essentially have two main options. One option is to simply set everything to zero. Personally, though, that’s not what I would do. Since you’re predicting the future, these inputs should represent your best possible guess. What I would do instead is look at the previous one or two years, focus specifically on the same period (for example, August), and see whether promotions were typically running during that time. If they were, I would replicate or copy that behavior into the future data frame. That would be my preferred approach.

So that’s challenge number one: predict the future by carefully constructing realistic future covariates.

Challenge number two is to use a MAX model with statsmodels, including proper parameter tuning. The goal here is very clear: can you beat the RMSE of 792 that we achieved earlier? Is that a good result? Is it acceptable? Or can you do better with a tuned model? If parameter tuning turns out to give you better performance, then that’s the approach you should go with.

These are the two main challenges I want to leave you with.

And finally, a promise from me. Doing the challenge should already be rewarding on its own, because you’re applying relevant concepts to a realistic forecasting problem. But beyond that, I promise that if you complete these challenges and post your results—whether in the Q&A section, on LinkedIn, or somewhere else—I will personally go through them and give you individual feedback. No AI, no automation—just me reviewing your work and sharing my thoughts.

For those of you who go the extra mile and do this challenge, that’s my commitment.

# **XIII) Section 13: Mid-Course Feedback**

# **A) Will you help me?**

You’ve been absolutely crushing it so far. From nailing the fundamentals to tackling real challenges, you’ve been showing the kind of curiosity and drive that turns learning into real skill. But we’re not slowing down yet — there’s a lot more power ahead.

Now, I need a small favor from you. Just a couple of minutes of your time, but it can make a huge difference.

In the next lecture, you’ll find a feedback form. I’d really appreciate it if you could fill it out. Tell me what’s working well, what’s getting you excited, and what you feel could be improved or is missing. Be honest — that’s exactly what helps me make this course better for you and everyone else.

Please do this as a personal favor to me. Your feedback directly shapes how this course evolves.

So go ahead: click the link, drop in your thoughts, and help me level this up.

Thanks for stepping up and making your voice heard. Let’s keep this momentum strong and blast through the second half of the course with even more focus, clarity, and firepower

# **XIV) Section 14: PART 3 - DEEP LEARNING FOR TIME SERIES FORECASTING**

# **A) Deep Learning for Time Series Forecasting Overview**

This is a deep learning world, and we’re just living in it.

If you’ve ever felt like you’re driving a horse and buggy in a world full of Teslas, it’s time to upgrade. In this section, we’re diving deep into neural networks that will make your data work for you like never before. So buckle up, because we’re turning this up to an eleven.

But why deep learning?

Because this is the state of the art. And that’s exactly why you’re here.

We start with LSTMs — Long Short-Term Memory networks — but not just a surface-level introduction. We get technical. We fine-tune LSTM parameters, experiment with activation functions, and squeeze out every possible drop of accuracy. We apply cross-validation to make sure your models aren’t just good, but truly exceptional. Think of it as turning every dial to maximize performance.

Next up: multiple time series forecasting.

This has been one of the most requested topics in the entire course, and it’s finally here. You’ll learn multiple ways to handle and forecast multiple time series, with practical, real-world approaches that actually scale.

Then we take things to the next level with Temporal Fusion Transformers.

This advanced architecture combines temporal patterns with covariates to deliver next-level forecasts. You’ll learn the key concepts, understand the model architecture, and implement everything step by step in Python. TFTs are like LSTMs on steroids — built to handle complex, multivariate time series data with confidence and precision.

And we’re not stopping there.

We’ll also explore — and beat — another groundbreaking deep learning model: N-BEATS. Known for its outstanding performance in forecasting competitions, N-BEATS is a powerhouse. You’ll see how to set it up, train it properly, and tune it for maximum accuracy. This model alone can take your forecasting skills to a whole new tier.

By the time we’re done, you won’t just understand deep learning — you’ll dominate it.

Armed with LSTMs, TFTs, N-BEATS, and every other acronym that matters, you’ll forecast like a pro.

# **XV) Section 15: RNN - LSTM**

# **A) Game Plan for LSTM**

Let’s kick off this journey into LSTMs, a core part of deep learning and neural networks.

Neural networks may sound like the fanciest technique out there, but the idea itself is actually one of the oldest concepts we’ll cover in this course, dating all the way back to the 1940s. The name — which, by the way, is incredibly memorable for humans — comes from the similarity between how these models work and how synapses function in our brains.

At a high level, neural networks receive inputs, transform them, and then produce outputs. That transformation step is where the real magic happens. One of the most important aspects of neural networks — and a key reason they fit so well with time series forecasting — is their ability to learn. Much like our own brains, neural networks adjust what they “know” based on new information. They can forget, discard, or update what they once considered facts when fresh data comes in, and they can arrive at different conclusions depending on the circumstances.

There is also an element of randomness involved. Every neural network has stochastic components, which means you may get slightly different results than mine — and that’s completely normal.

Now let’s talk about the specific model we’ll be working with.

Recurrent Neural Networks, or RNNs, are an advanced and specialized form of neural networks designed to handle sequential data. We’ll start with simple neural networks, move into recurrent neural networks, and finally arrive at LSTMs — Long Short-Term Memory networks. LSTMs are a specialized form of RNNs built specifically to overcome one of the major flaws of standard RNN architectures: their difficulty in learning long-term dependencies.

From a practical perspective, we’ll work with two datasets.

The first dataset is one you already know. We’ll use it to fully understand how LSTMs work from start to finish, breaking down complex concepts and applying them immediately. This dataset will also help us clearly understand how the Darts library operates under the hood.

The second dataset comes from the M4 competition — a well-known benchmark in time series forecasting. Here, our goal shifts to predicting multiple time series at once. This means we’re not just learning deep learning anymore; we’re tackling an entirely new forecasting challenge: multi-series forecasting.

And don’t worry — we’ll cover every concept step by step.

# **B) Simple Neural Network**

The mathematics behind neural networks can be pretty complex, but I do feel that the overall process can be quite intuitive. To illustrate the mechanics, I will start by explaining how multilinear regression works from a purely architectural or framework perspective.

Let’s visualize this first. You have an input layer and an output layer. The inputs represent the variables, and for each observation, you generate an output. In a nutshell, all input variables are combined to produce an outcome. This setup can actually be seen as a very simple neural network.

The process in neural networks is similar, but with a very specific added complexity. As before, we start with inputs and an output. However, now there is a hidden layer in the middle. What happens is that the inputs are transformed in this hidden layer by elements called nodes.

This hidden layer is absolutely fantastic for two main reasons. First, it enables non-linearity by transforming the inputs in whichever way the model finds best. Second, it combines the inputs and generates new dynamics that the original inputs do not explicitly have. For me, this is really the secret sauce that makes neural networks so powerful—the transformation of variables into better versions of themselves.

In the end, the hidden layer then feeds into the output layer. So how does this actually work? Let me briefly go through the mathematics behind it.

First, you need to know that the index j refers to the neurons in the hidden layer, and the index i refers to the inputs. Feel free to pause here and take a moment to look at everything before continuing.

Now, let’s talk about z. This is the value, or output, of the hidden layer neuron j. It is a combination of two things. The first is the bias parameter b of neuron j. The second is the sum of the inputs multiplied by their corresponding weights. Both the weights w and the bias b are learned during the training of the model.

The inputs themselves are represented by x. Each arrow in the network represents a weight, connecting an input to a node in the hidden layer. Each of these arrows corresponds to a weight 
𝑤
𝑖
𝑗
w
i
j
	​

, and together with the inputs 
𝑥
𝑖
x
i
	​

, they form the weighted sum for each neuron.

If you think about it, this is not very different from a multilinear regression equation. You have coefficients, inputs, and a constant term. The process is conceptually similar.

The next step is to transform the outputs of the hidden layer into the final output. This is a crucial step and is done using an activation function. The activation function helps prevent overfitting and enables the network to model complex relationships. We will cover activation functions in more detail later in the section, so there’s no need to worry about that for now.

So far, we have covered the one-sided process, where we go from the inputs all the way to the outputs. But there is more to it. While it is conceptually simple, the full process is slightly more complex.

Neural networks also have a backward process. Each time the model runs forward once, this is called an epoch. After each epoch, the model runs backward. During this backward pass, all the arrows—what we call the weights—are updated through a process known as backpropagation.

This back-and-forth process is where the learning happens. The model continuously learns and adapts by adjusting its weights based on the error. When building a model, one of the key parameters we choose is the number of epochs. This could be five, ten, twenty, or more, depending on what works best, and we determine this through experimentation.

Finally, so far we have visualized a simple neural network. But neural networks can be more complex. For example, we may have more than one hidden layer. The more hidden layers we add, the more complex the network becomes.

Additionally, we may have more than one output. This is particularly useful in cases like multivariate time series forecasting, where we want to predict multiple series at the same time.

To sum it all up, neural networks use hidden layers to transform inputs into neurons that better represent the problem. These representations are then converted into outputs, usually through activation functions.

# **C) Recurrent Neural Networks (RNN)**

In this video, we are going to go one level deeper into deep learning, and the main focus will be on recurrent neural networks, or RNNs. However, before we get to RNNs, I need to simplify the visualization we have been using so far.

The input layer will now be represented as one blue ball. Similarly, the hidden layer will be shown as one green ball, and the output layer will be shown as purple. What you need to keep in mind is that, for future visualizations, each of these layers can actually have multiple nodes. This simplification is only for visualization purposes. As we move forward into more complex architectures, we will visualize networks with many more neural components.

We still have an input layer, at least one hidden layer, and an output layer. However, it is not just one network anymore—we now have multiple networks. So we add another one. This raises an important question: how are these networks connected?

The connection happens through the outputs of the hidden layers. Each network corresponds to one unit in the sequence. Let’s imagine time steps n and n-1. The output at time n is influenced by the inputs at time n as well as the outputs from time n-1. Similarly, the output at time n-1 is influenced by its own inputs and by the outputs from the previous step.

This process continues forward in time. The output at time n was influenced by the inputs at time n and the outputs at time n-1. Using the same logic, the output at time n+2 is influenced by its own inputs and by the output at time n+1. This creates a chain of dependency across time steps.

This is where the term recurrent neural network comes from. We have this recurring or feedback effect, where past outputs influence future outputs. At the same time, this should feel oddly familiar. In time series forecasting, we use past information to predict the future.

Similarly, when we use regressors, we rely on input variables to help explain the independent variable. So even though recurrent neural networks are very different from the models we have seen so far, from a purely mathematical and intuitive perspective, the underlying logic is actually the same.

The last thing I want to cover is that RNNs are particularly well suited for sequence data. Time series is one type of sequence data, but there are many others—such as music, books, text, and language translation. All of these involve sequences where order and context matter.

Finally, there is one important flaw in recurrent neural networks that we need to discuss. But I won’t spoil that just yet—we’ll cover it in the next video.

# **D) LSTM**

Now that we understand the architecture of recurrent neural networks (RNNs), we also need to understand that they have a flaw. And spoiler alert—the solution to this flaw is the LSTM framework. But before jumping to the solution, let’s first understand what actually goes wrong when an RNN is trained.

We are already familiar with this type of chart: the model processes data as a sequence of data points. The most recent point always has more influence than the one before it, which in turn has more influence than the one before that. This behavior is exactly what we would expect from a regular time series. It makes sense intuitively, and we have seen this logic before, for example when working with exponential smoothing, where recent observations receive higher weights.

So far, there is no problem. This behavior is perfectly reasonable and expected. The real issue appears during backpropagation.

When we update the weights in a neural network, especially the weights connecting the input layer to the hidden layer, the model gains the ability to “unlearn” information. This is a very useful mechanism, because it allows the model to discard patterns that were once true but are no longer relevant. For most neural networks, this works well. However, for RNNs, this becomes problematic because of the sequential nature of the data.

What happens is that for each node update, the model loses a fraction of what it has learned. This loss is not a big issue for the most recent time steps, but as we move further back in time, the impact becomes severe. By the time the training process reaches the very first point in the sequence, the updates are extremely small. In other words, the earliest parts of the sequence barely get trained at all.

This is a serious issue. Why does it matter? Because we train the RNN from the past toward the future, but during backpropagation, the weight updates mainly affect the most recent time steps. This phenomenon is known as the vanishing gradient problem. As a result, a large part of the network—specifically the earlier time steps—remains undertrained.

Another way to look at this is to say that a standard RNN only has a very short-term memory. Only the recent past is effectively learned, while long-term dependencies are largely ignored. Clearly, this is something we need to overcome.

You might think, “Why not just remove the dropout or the learning mechanism altogether?” But that would severely limit the model’s ability to improve. Imagine if humans were unable to forget anything they had learned. That wouldn’t work—we would still believe that the Earth is flat or that the Earth is the center of the universe. Forgetting, or unlearning, is essential for improvement.

So what is the solution?

The LSTM introduces a free-flowing memory channel that exists alongside the usual RNN structure. This memory channel is not affected by the same dropout and gradient decay issues that affect standard RNNs. It is updated using the inputs and outputs at each time step and preserves this information for future reference.

This memory channel represents the long-term memory that is missing in standard RNNs. It is also updated with each training epoch, meaning that the memory itself improves over time as the model learns. This allows the network to retain important information from far back in the sequence.

In summary, recurrent neural networks are good at modeling short-term dependencies. However, in time series forecasting, events that happened one or even two years ago can still be highly relevant. If the model cannot remember them, that becomes a major limitation. This is why long-term memory is essential.

Of course, there is more to LSTM than this explanation, but from an intuition and practical application perspective, this is the core idea.

I’ll leave you with a link to the original LSTM paper. It’s not completely unreadable, but it is definitely challenging. You can find the paper in the course materials if you’d like to dive deeper.

I hope you enjoyed this explanation and that it’s clear how we arrived at LSTM—from simple neural networks, to sequence data, to recurrent neural networks, and finally to understanding their flaws and why LSTM is the solution.

Now, let’s stop with the theory and start applying these concepts in practice.

# **E) Python - LSTM Setup**

Now it’s time to put LSTM into practice. Inside the deep learning folder, you’ll find the section where we work with LSTMs. We’ll start with a single time series, just to kick things off properly. Later on, we’ll move to multiple series, but for now, focusing on one series will help us understand everything clearly. In that folder, you’ll also find a starter file that we’ll use as our base.

Since this is deep learning, it’s really important to have your GPU enabled. I personally use Colab Pro, but even with regular Google Colab, you’ll still be able to access a GPU that you can rent temporarily. If you don’t have Pro, the L4 GPU works fine—it’s slightly slower, but it will still work 100% for this tutorial. For me, time is valuable, so I pay the €10–€11 per month (roughly $12–$13) whenever I need to work on time series forecasting or anything that requires GPU acceleration.

In this session, I’m going to use the A100 GPU, which is one of the high-end GPUs from NVIDIA. It’s expensive, but it’s extremely fast and I highly recommend it if you have access. That said, the free GPU options will still be perfectly fine for this tutorial. After that, we’ll mount the drive. I already have the starter file ready here. Most of the steps should feel familiar—we install the required libraries and prepare the environment.

At this point, I’m going to pause for about a minute, because installing the libraries usually takes around one to one and a half minutes.

Alright, the installation is now complete and we’re importing the libraries. Usually, the RNN-related libraries take the longest to load. The dataset we’re going to use is one that I absolutely love—it’s about shelter demand in New York City. This is a dataset I originally found and then enriched myself. For example, I added real New York temperature data, and marketing-related variables as well.

Next, we preview the dataset. This step helps us check for null values, object types, or anything unexpected. The data is daily, so we explicitly set the frequency to daily. The demand variable is our target, and we rename it to Y.

Now let’s plot the data to see what’s going on. We can clearly observe a growing trend over time. As we approach 2020, the demand starts to decline, which aligns with what we might expect for shelter demand during that period.

Next, we look at the seasonality. There’s a very clear seasonal pattern: demand drops during the warmer months, reaching its lowest point around August, and then increases again with a peak in December. This makes perfect sense given that we’re modeling shelter demand.

When we look at quarterly seasonality, it also aligns with our expectations. Q1 and Q4 have the highest seasonal peaks, while Q3 shows the lowest demand. This fits well with colder versus warmer periods of the year.

At this point, we get a runtime warning indicating that the GPU is not being used. That’s fine for now. If I activate the GPU midway, it would require rerunning everything from the beginning, so I’ll leave it as is for the moment.

Moving on to decomposition, we start with a seasonal period of 365 days. This gives us a very interesting seasonal curve, which clearly confirms what we saw in the earlier plots. The trend increases and then declines over time. We can also try a seasonal period of 7 days, but that introduces too much noise in the visualization. Still, when we look at the seasonal component, the values are centered around one.

This is expected because we’re using multiplicative decomposition. The multiplicative formula is
Y = Trend × Seasonal × Residual.
Because of this, we expect both the seasonal and residual components to be centered around one. The trend component represents the baseline behavior of the series.

Next, we examine the autocorrelation plot. We see very clear and strong patterns, with spikes occurring every seven days. This indicates strong weekly seasonality. It also suggests that the data is non-stationary, which we already know due to the presence of a trend.

When we look at the partial autocorrelation (PACF), we can see that there’s a lot of information in the most recent seven days. We also see recurring information at lags of 14, 21, and so on. This confirms that the seasonal component plays a major role in the series.

Finally, we preview the full data frame. We have Y, Easter, Thanksgiving, Christmas, temperature, and marketing variables. The main series is the Y column, and the covariates include everything from Easter through marketing. We log-transform where needed, run the final checks, and at this point, everything is ready.

We’re now fully set up and ready to start modeling.

In the next video, we’ll focus on modeling the seasonality using LSTM.

# **F) Python - Time Variables**

Now let’s create the time variables. In our previous models, we had seasonality components that we could explicitly activate. With deep learning, things are a bit different. These models are more focused on sequence data, which means they can understand seasonality, but we need to explicitly help them by providing the right inputs. In other words, we need to spoon-feed the seasonality information.

There are multiple ways of doing this, and throughout the course I’ll show you several approaches. This is just one of them. In the end, it really comes down to which approach you find the easiest or most intuitive. Conceptually, they all achieve the same goal. There are many ways to solve the same problem—potato, potato—so it’s more about design preference than correctness.

Let’s start by creating the time variables. We begin with the year variable. As soon as we try this, we get a helpful suggestion from the environment. Let’s see if it works—and yes, it does. We immediately start getting useful outputs, which is exactly what we want.

To understand this better, let’s briefly explore the API reference and documentation. We’re using datetime attribute series, which returns a new time series indexed by time, with one or more dimensions that can be one-hot encoded. In this case, we want to extract the year, and this works perfectly for our needs.

Including the year variable helps us capture the trend in the data, which is very important. It also helps the model connect to seasonal patterns that may not be perfectly recurring year after year. Sometimes seasonality evolves over time, and having the year as an input allows the model to account for that change.

Next, we add the month variable, which further helps the model understand seasonal effects. After that, I always like to include the day of the week. This is especially useful for daily data, where weekday effects often play a major role.

We can implement this using the weekday option, which is what I have in my notes. Once we include it, we see values ranging from 1 through 7, covering the full weekly cycle from start to finish. We can also inspect the output to confirm that everything looks correct.

And that’s actually it for this video. This is how you include time-based variables for deep learning models. In the next step, we’ll bring everything together—scaling the data and merging these time variables with our existing covariates, which will also be scaled.

# **G) Python - Scaling**

The next step is to scale our data, which is a very important part of working with deep learning models. We’re going to start by preparing our scalers. It’s important to note that we are not using any shortcuts here. We will use two separate scalers: one for the target series and one for the covariates.

So first, we create a scaler for the target variable, which we’ll call scaler_y. Then we create another scaler for the covariates. This separation is important because the target and the explanatory variables serve different purposes and should be handled independently.

Next, we apply the scaler to the target series. We use fit_transform on the series and store the result as the scaled version. If we preview the output, we can clearly see that the values are now scaled between 0 and 1. This confirms that the scaling step worked correctly, and at this point the target series is ready.

Before moving on, it’s important to recall what our data looks like. Earlier, we confirmed that the time variables—year, month, and weekday—are not one-hot encoded. Instead, they appear as numeric values such as 1, 3, 4, 5, and so on. Because of this, we now need to stack these time variables together with our existing covariates.

This is exactly what we do next. We take the original covariates—Easter, Thanksgiving, Christmas, temperature, and marketing—and stack them together with the year, month, and weekday variables. When we inspect the result, we can see that all components are now combined into a single covariates structure. This confirms that the stacking step worked as expected.

Once all covariates are combined, we apply the second scaler. We use fit_transform on the full covariates dataset, producing scaled covariates that are ready for modeling. At this point, all variables—both the target and the covariates—are properly scaled.

And that means we’re absolutely good to go.

We are now fully prepared to build our LSTM model. To get started, we look for the RNN-based model, specifically the one labeled RNNModel. This is the model we’ll be working with, and we’ll keep the documentation open alongside it as we explore its parameters and configuration.

# **H) LSTM parameters**

We are almost there and ready to build our LSTM model.

But before we actually do that, I want to briefly walk you through the key parameters that we will be setting and tuning throughout this section. Understanding these will make the modeling choices much clearer as we move forward.

We’ll start with dropout, which is a mechanism designed to prevent overfitting. Dropout represents the share of neurons that will be randomly excluded during each epoch. To quickly recap, an epoch is one complete pass through the training dataset. If you remember the feature sampling mechanism we used in gradient boosting—where we only look at a subset of features at a time—this follows the same idea. Each epoch uses a random subset of neurons, which helps the model generalize better.

Next, we have the number of hidden layers. This parameter controls how deep the network is. The more hidden layers you add, the more complex the model becomes. However, increased complexity also means a higher risk of overfitting and longer training times.

Closely related to that is the hidden dimension parameter. This specifies how many feature maps or units each hidden layer contains. Again, higher values increase model complexity and expressive power, but they also increase the risk of overfitting if not handled carefully.

The fourth parameter is the number of epochs. This simply defines how many complete iterations the model will make over the training data. You can think of this as how many times the network gets the opportunity to learn and refine its weights.

Following that, we have the learning rate, which controls how fast the model learns. A higher learning rate means the model updates its weights more aggressively, while a lower learning rate results in slower, more cautious learning. In practice, lower learning rates are often preferable because they help prevent overfitting and unstable training.

Parameter number six is the training length. This defines how much historical data is used for training in each iteration of the model. It’s important to note that this value must be larger than the seventh parameter, which is the input chunk length.

The input chunk length specifies the number of past time steps that are fed into the model at once. This parameter is particularly important for time series forecasting, as it determines how much historical context the model can use to make predictions.

From my perspective, most of these parameters are simply technical levers. They don’t really have a direct business interpretation. The main exceptions are the input chunk length and the training length, as these are closely connected to the forecasting horizon. In our case, since we’re working with a 31-day testing and forecasting horizon, we’ll set these values in a way that makes sense for that time frame.

And that’s really it.

In the next video, we’ll see how all of this comes together in practice and how we actually configure and train the model. I also want to reinforce one last point from the slide I forgot to show earlier: these parameters are just cogs in the machine. They’re tools we use to improve results, nothing more.

What really matters is that we stay outcome-driven and focus on performance.

# **I) Activation Functions**

In this video, I’m going to cover activation functions.

Activation functions are one of the core foundations of neural networks, as they determine how neurons process inputs and transmit information forward through the network. We’ll take a closer look at three key activation functions: sigmoid, hyperbolic tangent (tanh), and the rectified linear unit (ReLU). The goal here is to understand their individual characteristics and the specific roles they play within neural network architectures.

Let’s start with the sigmoid activation function.

The sigmoid function is defined by the formula:
one divided by one plus e raised to the power of minus t.
What this function does is map any input value into a range between 0 and 1. This “squashing” effect makes sigmoid especially useful when outputs resemble probabilities.

The sigmoid has a distinctive S-shaped curve, and because of this shape, it can lead to the vanishing gradient problem, particularly when input values are very large—either very positive or very negative. In those regions, the gradients become extremely small, which can significantly slow down training or even cause it to stall altogether.

If you visualize the sigmoid function, you’ll notice that all outputs remain between 0 and 1. Extreme input values barely change the output, which limits the impact that individual variables can have. This behavior is partially intentional, as it helps prevent overfitting by ensuring that no single variable dominates the model excessively.

Next, we move on to the hyperbolic tangent function, commonly referred to as tanh.

The tanh function has a very similar mathematical structure to the sigmoid, but with one important difference: its output range spans from -1 to 1. This gives it a broader range and, crucially, makes it zero-centered, which is often desirable in neural networks.

Like sigmoid, tanh also has an S-shaped curve and can suffer from the vanishing gradient problem. However, because it is centered around zero, it is generally preferred over sigmoid for hidden layers, as it allows gradients to flow more symmetrically during training.

If you look at the tanh curve, you’ll again see that values are capped—this time at -1 and 1. This means that while variables can influence the output, their impact is still bounded, preventing extreme values from overwhelming the network.

Finally, we have the rectified linear unit, or ReLU.

The ReLU function is defined as the maximum between zero and the input value x. Its output range therefore goes from 0 to positive infinity. From a computational efficiency perspective, ReLU is extremely fast and simple because it is a piecewise linear function. This simplicity often results in faster training compared to sigmoid or tanh.

ReLU also helps mitigate the vanishing gradient problem, but it introduces a different issue known as dead neurons. This happens when neurons consistently receive inputs less than zero, causing them to output zero every time. Once a neuron becomes “dead,” it no longer contributes to learning and effectively becomes useless.

If you visualize ReLU, you’ll see that all negative inputs collapse to zero—these are the dead neurons. On the positive side, the function grows linearly and, in theory, extends to infinity. While that may seem odd at first—why would infinite values make sense?—in practice, ReLU performs extremely well from an empirical standpoint.

In fact, it’s often surprising how frequently ReLU turns out to be the best choice during parameter tuning. Despite its simplicity and quirks, it consistently delivers strong results in real-world deep learning models.

That wraps up our deep dive into activation functions.

While this might not be the most exciting topic on the surface, activation functions are a fundamental part of neural networks and deep learning. Understanding them is essential if you want to truly grasp how these models work and how to tune them effectively.

# **J) Python - LSTM Model**

We’re now getting into Recurrent Neural Networks. RNNs come in three main variants: vanilla RNN, LSTM, and GRU. In this course, we’re going to use LSTM, which is generally considered the better and more robust option for most time series problems. Because of that, we’ll stick with LSTM throughout.

Now, we’re going to leave this cell here because we’ll be using it later, but let’s go ahead and build the LSTM model. We define the model using the RNN module and specify that we want an LSTM architecture. Let me organize everything so it’s clear and readable. Once that’s done, the model definition is good to go.

Next, let’s talk about input chunk length.
This represents the number of past time steps that are fed into the forecasting module at prediction time. Before defining it, we’ll set up some configurations. We define the forecasting horizon, which in this case corresponds to our future CSV ranging from January 1st to January 31st, so that’s 31 days.

Now, the input chunk length must be greater than the forecasting horizon. You can tune this value, but the general rule is that input chunk length should always be larger than the forecast horizon. Another way to think about it is this: it’s the number of past time steps we use to predict the future. If we want to predict the next 31 days, we should be looking at more than 31 days in the past.

If we only look at, say, 30 days to predict the next 31 days, that’s not very strong—especially because time series data is sequential. Today influences tomorrow, tomorrow influences the next day, and so on. When we’re predicting the 31st day, we still want the model to be influenced by today and the days before it. That’s why we should avoid using too few past observations.

This is technically a soft rule, but I strongly encourage treating it as a hard rule. For now, we’ll use an input chunk length of 46. This is a tunable parameter, and we’ll explore it further later. There’s also another parameter called training length, which we’ll get to shortly.

With that, we define input_chunk_length using the value we just discussed and move forward.

Now let’s talk about the hidden dimension.
The hidden dimension refers to the number of units inside each RNN layer. There’s a recommended default value of 25, and that’s perfectly fine to stick with. You’ll notice that most parameters have sensible defaults, except for input chunk length, which we must define explicitly.

Next is the number of RNN layers. This controls how many stacked LSTM layers we use in the network. Using just one layer is usually too shallow, so we’ll go with two layers. There’s also an important detail here: if you only use one RNN layer, dropout is automatically set to zero. Personally, that feels a bit odd because dropout is one of the learning and regularization mechanisms. Since we want dropout, we’ll stick with two layers.

We set dropout to 0.1, which is a reasonable and commonly used value.

Now let’s move to training length.
Training length defines the length of both the input and output series used during training. This value must be larger than the input chunk length; otherwise, the RNN is never run for enough iterations to learn properly. Since both input and output are involved, the standard approach is:

training length = input chunk length + forecasting horizon

That’s exactly what we’ll use here.

Next, we define the number of epochs. There’s no strict recommended value, but 10 epochs is usually a fair starting point, so we’ll go with that.

We also set a random state. This doesn’t have a huge impact on performance, but it ensures that the initial weights are more or less the same across runs, which helps with reproducibility.

Now let’s look at optimizer parameters.
We can pass arguments directly from PyTorch, such as the learning rate. Personally, I like to specify this explicitly. The recommended learning rate here is around 1e-3, which is 0.003, so that’s what we’ll use.

Finally, we configure the PL trainer arguments.
This is where we specify that we want to use the GPU. If you’re using a CPU, I strongly recommend switching to a GPU—even if your machine is slow and you’re not on Colab. LSTM training can take quite a bit of time on a CPU. We set the accelerator to "gpu" and specify the device index as an integer, which in this case is 0.

With all of that in place, we run the cell.
If you see an error related to hidden dimensions, it’s usually just a spelling mistake—once corrected, everything works as expected.

Now we fit the model to the data.
We train it using the scaled series and scaled future covariates. I’m not entirely sure whether the verbose flag is supported here, but we’ll find out as it runs.

And there we go—the model is training.
You can see that the A100 GPU just breezes through this. It runs incredibly fast, and everything works smoothly.

Just to wrap up this video, I originally had a really nice video here showing why GPUs are so much better than CPUs. It featured the CEO of NVIDIA demonstrating the difference using a painting analogy. Unfortunately, that video is no longer available, so I’ll have to remove it from the materials. That’s a shame, because it was very relevant.

If I do find a good replacement, I’ll be sure to share it with you. It’s absolutely worth watching.

# **K) Python - Cross-Validation**

Let’s move on to cross-validation. I’m going to search for it first—it should be somewhere around here. As usual, we’ll be working with historical forecasts, and you’ll notice that the process stays very similar as we do more and more deep learning in this framework.

I’ll still take this step by step for now. Of course, in later sections I’ll move a bit faster so we don’t become overly repetitive. But for now, let’s do this carefully and properly.

So, let’s set up cross-validation.
The way this works is that we define our cross-validation parameters, then we pass our trained model into the historical forecast function. This is where the actual rolling evaluation happens.

We start by specifying the series, which in our case is the scaled series. For past covariates, we pass None, and for future covariates, we provide the scaled covariates.

Before proceeding, let me double-check something important. If we go back to the documentation homepage, there’s a very nice overview of how LSTM—and actually all models—work with covariates. What we’re interested in here is the RNN model.

From the documentation, we can see that this model uses future covariates, not past covariates and not static covariates. This is very similar to what we saw earlier with models like SARIMA and Prophet. You’ll notice that Prophet appears in the same category, and Auto-ARIMA does as well.

What this means conceptually is simple but important:
If we use covariates to explain the past, we must also provide those covariates for the future. We cannot have one without the other. That’s why we pass future covariates during both training and forecasting.

Later on, we’ll also learn about models that only rely on past values and don’t require future covariates, as well as models that support static covariates. Those are extremely useful in certain scenarios.

For now, we proceed with scaled future covariates.

Next, we define our cross-validation configuration.
We need to specify a start point. One way I like to do this is by looking at the total length of the dataset—for example using the dataframe shape—and then choosing a point six months or twelve months before the end as the starting position for cross-validation.

Now let’s talk about the stride.
The stride defines the number of time steps between two consecutive forecasts. Another way to think about it is how frequently we generate new predictions.

If we followed the documentation recommendation exactly, the number of forecasts would be massive. Conceptually, it would mean predicting the next 31 days today, then again tomorrow, then again the day after, and so on. From a practical perspective, that’s not very realistic.

An alternative approach is to forecast at a lower frequency. For example, if we’re forecasting 31 days ahead, we could generate forecasts every week—day 7, day 14, day 21, and so on.

To keep things simple here, we’ll divide the forecasting horizon by two. That gives us 15.5, but if we check the documentation, the stride must be an integer. So 15.5 won’t work. We round it down to 15, which works perfectly for our case.

Now we include all the remaining parameters.

We set start to the value we defined earlier.
We set stride to 15.
We set retrain=True, which means the model is retrained at each step. From my perspective, this makes total sense, because we want the model to adapt as new data becomes available.

There’s another parameter called last_points_only.
By default, this is set to True, which means that for each forecast window, we only keep the last predicted value. Personally, I find this a bit strange. In practice, I’ve never been in a situation where I only cared about the very last point of a forecast.

Typically, I want to look at the entire forecast window—or at least a meaningful subset of it—especially when evaluating model performance. So we explicitly set last_points_only=False.

Next, we make sure we include the forecast horizon parameter. Note that the correct parameter name is forecast_horizon, not forecasting_horizon. Once that’s corrected, everything lines up properly.

We remove any unnecessary parameters like verbose, fix the parentheses, and run the cell.

At this point, cross-validation starts running.
In the next video, I’ll come back and show you how long this actually took, and we’ll also look at the results and interpretation.

# **L) Python - Cross-Validation Performance**

The cross-validation took roughly one minute, which is great for us and very reasonable given the complexity of the model.

Now that it’s finished, the next step is to retrieve and evaluate the results. For evaluation, we are going to compute the RMSE (Root Mean Squared Error). This is the metric I personally prefer for time series cross-validation because it penalizes larger errors more strongly and is easy to interpret in the original scale of the data.

We start by initializing an empty list that will store the RMSE values from each cross-validation fold. So we create something like rmse_cv = [].

If we inspect the output of our cross-validation object, we see that it contains multiple elements, and each of these elements represents a prediction window. Each prediction corresponds to one fold, and each fold contains 31 predicted days, which matches our forecasting horizon.

Now, an important thing to keep in mind is that these predictions are still scaled. Because of that, we cannot directly compute meaningful errors yet. To properly evaluate the model, we need to inverse-transform the predictions back to their original scale.

To do this, we use the scaler that was applied to the target series. In our case, this is scaler_y. So we call scaler_y.inverse_transform() on the cross-validation predictions. Once we do this, we obtain the actual predicted values, which is exactly what we need.

To simplify manipulation and comparison, we convert the predictions into a time series format and then into a structure that is easy to work with using pandas. This allows us to align predictions with actual values based on their timestamps.

At this point, we have our predictions, but we still need the actual observed values for the same time range. To do that, we extract the start and end timestamps from the prediction index. Using those timestamps, we slice the original dataset and retrieve the corresponding actual values from the target column.

So now we have two aligned series:

the predicted values, and

the actual values for the same dates.

With both of these in hand, we can compute the error. We use Root Mean Squared Error, either by taking the square root of the mean squared error or by directly using a dedicated RMSE function. Conceptually, both approaches are equivalent—it’s really a potato–potahto situation.

We first compute the RMSE for the first fold to make sure everything works correctly. Once we confirm that, we generalize the process by building a loop.

Inside the loop, for each cross-validation fold:

we inverse-transform the predictions,

extract the corresponding actual values,

compute the RMSE,

append the result to the rmse_cv list, and

print the RMSE value using an f-string so we can track progress.

After iterating through all folds, we compute the mean RMSE across all folds using numpy.mean. We format the output to two decimal places, which makes the result cleaner and easier to read.

In our case, the final RMSE comes out to approximately 62.21, which looks reasonable given the scale of the data.

And that’s it.
This is how we compute cross-validation performance for an LSTM-based time series model—fairly straightforward once everything is aligned correctly.

Now that we have a baseline evaluation, we’re ready to move on to the next stage: parameter tuning. I’ll show you one parameter tuning technique first. This approach is particularly useful when you have many parameters and each model run takes a significant amount of time.

This will be parameter tuning – round one.

# **M) Python - Parameter Grid**

Welcome to parameter tuning – round one.

We’re going to start by defining the parameter tuning grid. The first thing we do is create our parameter grid as a dictionary. The values that we include here are all coming from the parameters we discussed earlier — these are essentially our baseline or initial values.

Now, an important thing to understand is that we are not going to include all possible parameters and combinations. If we did that, the number of combinations would grow exponentially, which quickly becomes impractical for deep learning models.

In total, there are about seven tunable parameters, but instead of tuning all of them at once, we’re going to narrow this down. What we’ll do is select four parameters, each with two possible values. One of the parameters — the number of epochs — will appear across multiple combinations.

The parameters we include in this first round are:

Number of RNN layers

Hidden dimension

Dropout

Learning rate

Number of epochs

Training length

Input chunk length

For dropout, we’ll test values of 0.05 and 0.1.

For the optimizer, we focus only on the learning rate (LR). While multiple values are possible, for now we’ll keep it simple and include 0.003 as the primary value.

For the number of epochs, we include 5 and 10. I have a strong preference, especially at the beginning of any parameter tuning process, to always start simple. The goal is to see whether a simpler model can perform just as well as a more complex one. From a modeling and framework perspective, if we can achieve similar results with fewer epochs and fewer parameters, that’s usually better. Simpler models tend to generalize better to unseen data.

Next, we define the training length. In our case, the training length is the sum of:

input chunk length (46), and

forecasting horizon (31).

That gives us a total training length of 77.

Finally, the input chunk length itself is set to 46, which is consistent with what we used earlier.

Once all these values are defined, we generate the parameter combinations from the grid. We then compute the length of the resulting grid, which gives us the total number of combinations.

In this setup, we end up with 16 parameter combinations.

Now, this is where the exponential nature of grid search becomes very clear. If we were to add just two more values to one parameter, or add another parameter with two values, the number of combinations would jump rapidly — from 16 to 32, then to 64, then to 128, and so on. This is why parameter tuning for deep learning models must be done carefully and strategically.

This approach is one viable option for tuning deep learning models, and in future sections, I’ll also show you alternative tuning strategies that scale better when models become more complex or training time increases.

And that’s it for this video.
We’ve now completed the parameter grid setup.

In the next video, we’ll actually run the parameter tuning. Since we have 16 combinations and each cross-validation run took about one minute, we should expect the tuning process to take roughly 10 to 20 minutes.

# **N) Python - Parameter Tuning Round 1**

We start by initializing our RMSE list. This list will store the final evaluation score for each parameter combination. At the beginning, the list is empty, because we haven’t evaluated anything yet.

Everything we’re about to do now is something we’ve already seen before. Conceptually, nothing here is new. What we’re doing is simply putting all the pieces together. We iterate through the parameter grid, build a model for each parameter combination, perform cross-validation, retrieve the predictions, calculate the error, and finally store the results. In a sense, this entire section has been preparing us for this exact moment.

We begin by looping through the grid.
For each set of parameters in the grid, we create a new LSTM-based RNN model. The model configuration is built dynamically using the current parameter values. This includes setting the input chunk length, the number of RNN layers, the hidden dimension, the dropout, the random state, and the learning rate, which comes from the optimizer parameters. We also include the trainer configuration to ensure the model runs on the GPU.

Once the model is created, we perform cross-validation. We reuse the same configuration we defined earlier: the start point, the stride, retraining enabled, last points only set to false, and the forecasting horizon set to our predefined horizon. This ensures consistency across all parameter combinations, which is crucial for a fair comparison.

After cross-validation completes, we move on to retrieving and saving the results. For each fold in the cross-validation output, we extract the predictions. Since these predictions are still in the scaled space, we inverse-transform them using the Y scaler so that they are back in the original units.

Next, we retrieve the corresponding actual values for the same time window. With both predictions and actuals aligned, we calculate the RMSE for that fold. Each fold’s RMSE is appended to a temporary list that holds the errors for the current parameter combination.

Once all folds are processed, we compute the mean RMSE across folds. This average RMSE represents the overall performance of the model for that specific parameter configuration. We then append this mean RMSE value to our main RMSE list, which tracks performance across all parameter combinations.

At this point, the loop continues until every parameter set in the grid has been evaluated.

Now we execute the cell.
This will kick off the full parameter tuning process.

I’d recommend coming back in the next video so you can see how long the process actually took. That’s usually helpful for building intuition around training time. Of course, you don’t need to sit and watch it — you can simply let it run. Based on earlier timings, this should take somewhere between 10 and 20 minutes, depending on the hardware and load.

In the next video, we’ll take a close look at the results and identify which parameter combination performed best.

# **O) Python - Parameter Tuning Round 1 Part 2**

It took 14 minutes, so that’s definitely within expectations. And now, let’s continue.

So we’re going to create a DataFrame and see the best outcomes. And it’s going to be like this, right? We create a DataFrame from our grid, we add the RMSE there, and we actually don’t need to sort the values. I mean, we can—but we don’t have to. So let’s have a go.

Personally, I like these tables. I just like to click on stuff, basically. And when we look at this, we can see that it’s actually very similar to our initial result. I think what we had before was around 0.125462. So we definitely improved slightly.

At the same time, you can see that overall it actually went worse. Which tells us that we already had a pretty good outcome. And that outcome came from, let’s say, more complex models—so more RNN layers and a bit of a lower dropout. And that’s exactly what we’re going to keep building on.

Now personally, this is one step that I would recommend. We’re going to export the best parameters here. So here we go.

First, we identify them. We go to the tuning results—what was the name? It was just called results. And when the results are messy, we look for the minimum value, right? These are the best parameters.

Then what we do is we extract them. So we have this part here, and if we just do this—what we’re going to do is .iloc[0, :]. Here we go. This makes it easier for us to extract later on.

And then we do to_csv, and we call it something like best_params_round_1. And that’s it.

So now we’re absolutely ready to do parameter tuning round two. Our first round was already very quick because we only had 16 options. At the same time, the coding was also very quick because Gemini already had a lot of input from us.

So this next one should be even quicker. I’m definitely very excited to do this with you.

# **P) Python - Parameter Tuning Round 2**

It can happen that what you’re doing now is something you come back to a few hours later. And that’s exactly why I like to store our best parameters. So what we’re going to do now is load the best parameters from round one.

Let’s do this step by step. We start with a Pandas read_csv on best_params_round_1. I’m not going to include the index column right away, because I like to look at it first. We can see here that we have two columns. What we then do is set the index column to zero, and then we can clearly see the values.

Now, one issue here is that we know some values need to be integers, and some do not. So let’s get this into a variable first. We define params_round_1 equal to this, and we also display it so that we always see what we’re working with.

Here we go. We are going to isolate the best parameters from round one—just the ones that we have already used. Let me show this to you.

Now, we run this, and immediately we get an error. Let me check the number of RNN layers. Okay, regardless, this wasn’t quite right, so I’m going to try using .values. Here we go.

If I do .values, it returns 1.0. Wait—does it return 1.0? Let me double-check. The number of RNN layers should be two or three. So we have three here… but wait, it went to one. That was unexpected.

Let me triple-check this. best_params_round_1. Let me triple-check. This can happen because I run a lot of experiments before showing this to you, so it’s possible that these values came from a previous experiment. But definitely, an unexpected number of RNN layers being one would be incorrect.

Okay, now it’s three here, and it’s three here as well. So this must be an integer. Here we go. We now have the number of RNN layers set correctly.

Now, as input to Gemini, we’re going to add the hidden dimensions and the dropout. Dropout is indeed a float, hidden dimension is an integer, and the number of epochs is something we are going to repeat as well.

Here we go.

Now let’s define the parameter grid. The number of RNN layers is set to the number of RNN layers. I honestly have no clue where that earlier suggestion came from Gemini. The hidden dimension is the hidden dimension, and the dropout is the dropout. So we start there.

Then we move on to the number of epochs—five and ten. Learning rate is set to 0.01 and 0.03. Training length, I’m going to include 77 and 100.

By the way, this is a very fair question that you might have. Including these specific values doesn’t mean it has to be exactly this. Of course, we can include more training length. Personally, I think this is a fair starting point: we include what we’re predicting plus some value that’s larger than the forecasting horizon. But it doesn’t need to be this exact setup.

The hard rule is simply that the training length must be bigger than the input chunk length.

So let’s continue. We put 77 and 100. Input chunk lengths are 46 and 60. Then we generate the parameter combinations. The grid is the parameter grid, and once more, we end up with 16 combinations.

Now, I guess we still have time, right? So I’m just going to let this run.

We perform parameter tuning. We initialize our list, and now we iterate through everything: input chunk length, dropout, training length, number of epochs, random state, optimizer, and the PyTorch Lightning trainer kwargs.

Then we perform cross-validation using historical forecasts. The series is the scaled series, future covariates, the start, the stride, retrain, last points only, and the forecast horizon.

Next, we calculate the cross-validation results. We create a list just for the cross-validation output. For each iteration in the cross-validation, we transform the predictions back using the scaler, get the start, get the end, get the actuals, calculate the root mean squared error, and append it.

Finally, we compute the mean RMSE.

And that’s it. This is insanely quick once you already have everything set up. Once you have something like this, you’re really just breezing through. This is quite good.

Let me do Ctrl + Enter. This appears to be running, which is great.

So I’m going to stop here for this video. In the next one, we’ll come back to look at the results and start predicting the future.

# **Q) Python - Parameter Tuning Final Results**

We could have easily doubled the number of options from 16 to 32 and it would still have been okay. Even more than that, for a lot of cases, we could simply leave things running. For example, just renting out a GPU is often totally reasonable. It doesn’t cost that much per hour—maybe around 2 or 3 euros per hour. Especially if you’re in an enterprise setup, you can really just go for it and push performance as far as possible.

Now let’s check the performance.

We’re doing the same thing as before. Let’s come here and inspect the results. We can already see that we did improve a tiny bit by increasing the input chunk length. At the same time, you can clearly see some trends emerging.

For instance, the learning rate is clearly leaning toward 0.003, and the number of epochs is clearly ten. There are some parameters here that show a very clear trend. If we were to continue tuning, we might try something like a learning rate of 0.005, or increase the number of epochs to 15, and explore values around those ranges.

If you look a bit deeper, you can also spot other trends. Epochs are consistently at ten. The number of RNN layers shows consistency. Dropout is more geared toward 0.05, and there are some tendencies in the hidden dimension as well. Overall, the strongest signals are coming from the number of epochs and the dropout values.

Now, what are we going to do next?

Of course, we are going to export the best parameters. We start by identifying them from the results and exporting the best ones from this round. Here we go. Shift + Enter. Now that’s done.

At this point, everything is about predicting the future.

I’m going to approach this as if we were retrieving the best parameters from disk. In practice, we wouldn’t do this parameter tuning every single week. We would do it from time to time, more like a refresher for the model. Most of the time, we would simply build on top of the results from the last tuning run.

So we load the best parameters from round two. We can quickly preview them, and then isolate the relevant values. These include the number of RNN layers, the hidden dimension, the dropout, and the learning rate, which is a float. The training length is an integer, the input chunk length is also an integer, and we’re missing the number of epochs, which again should be an integer.

Here we go. Now we have all the parameters we need.

Next, we need the data.

We load the future CSV file and call it future_df. We can quickly preview it to see how it looks. Of course, the demand values are not known yet, but we do have features like Easter, Thanksgiving, Christmas, temperature, and marketing.

What we’re going to do next is extract the X values from the future data frame. For simplicity, I’m going to call this x_train. We take all rows and start from index position one. We can preview both the original future data frame and x_train side by side to confirm everything looks correct.

This gives us the five covariate columns, from index one to the end.

Then we repeat the same process for the future features. We concatenate x_train and x_future. This allows us to properly build and explore our covariates across both historical and future periods.

Here we go. Let me just fix that small syntax issue. Parentheses in place—and now we’re good to go.

Next, we create the time series covariates. Our covariates are built from the combined X data frame.

Now we add the time variables. We create features for the year, the month, and the weekday. With this, we’re definitely good to go and can continue.

The next thing we do is import the scaler classes. Just to clarify, the scalers might already exist earlier in the notebook, but here I’m working as if we were starting from scratch.

We transform the target time series to get series_scaled. Then we stack the covariates—year, month, and weekday—and scale them as well using fit_transform.

Let me run this. Shift + Enter. I accidentally hit Caps Lock, but that’s fine.

And that’s it for this part.

In the next video, we are going to actually build our LSTM model using these best parameters, and then truly start predicting the future.

# **R) Python - Tuned LSTM Model and Predicting the Future**

Let’s build the tuned LSTM model.

We start by defining our tuned model using the RNN LSTM architecture. We pass in the input chunk length, the hidden dimension, the number of RNN layers, and the dropout value. The dropout parameter is taken directly from the tuned results.

Next, we configure the optimizer settings. We specify the learning rate, which again comes from our tuned parameters. We also include the training length, which controls how much historical data the model uses during training.

At this point, we check if anything is missing. We still need to include the random state, which we set to 1502, and the PyTorch Lightning trainer kwargs, where we enable GPU usage. We also realize that the number of epochs is missing, so we add that as well. Finally, we confirm that input chunk length, hidden dimension, number of RNN layers, dropout, learning rate, training length, and number of epochs are all correctly defined.

There was a small syntax issue caused by a stray character, but once that is removed, everything is good to go.

Now we fit the model to the data.

We train the tuned LSTM model using the scaled target series along with the scaled future covariates. Once the model is trained, we generate predictions. We specify n as the forecasting horizon, which in our case is 31 days. We also pass the scaled covariates to ensure the model has access to the future information it needs.

Next, we inspect the predictions. As expected, the output is still in the scaled space. To make the predictions interpretable, we convert them back to the original scale by transforming them using the inverse transformation of the target scaler.

Now we can see the actual predicted values. If we want, we can also rename the prediction series—for example, calling it “LSTM”—to make plotting and comparison easier later on.

At this point, the core prediction step is complete.

A good practice is always to visualize the results. We plot both the historical training data and the predicted values on the same chart. We start by creating a figure with a size of 10 by 6. Then we plot the original demand series and label it as the training data. We overlay the predictions on the same plot and add a legend for clarity.

We also add a title to the chart, such as “Demand Forecast.” After fixing a small missing parenthesis, the plot renders correctly.

Initially, the visualization looks a bit crowded, so instead of plotting the full history, we focus on a more recent window. We zoom in on data from 2020 onward, which makes the forecast much easier to interpret. If desired, we can narrow this down even further—such as looking only from December onward—to closely inspect the short-term forecast behavior.

And that’s it for the LSTM model.

I hope you enjoyed this walkthrough. If you have any questions, feel free to ask—I’m here to help. Let’s keep building, and I’ll see you in the next video

# **S) LSTM Pros and Cons**

Okay, so this video is going to be about the pros and cons of LSTM.

But first—congratulations on getting here. This was an extremely long section, and we faced a lot of hurdles along the way. We really had to work to make LSTM function in a way that was simple, manageable, and practical. There were ups and downs, moments where things broke, and times when we had to step back and fix what wasn’t working. That’s completely normal.

What makes this even more impressive is that you tackled two very different challenges at the same time. On one hand, you were learning deep learning concepts—specifically LSTMs—and on the other hand, you were leveling up to working with multiple time series. That combination is not trivial at all. So genuinely, congratulations on making it this far.

Now let’s talk about why LSTM is great.

First and foremost, LSTMs are robust to outliers. Those sudden spikes we often see in time series data are generally handled quite well. If you recall the activation functions we discussed earlier, they play a key role here by limiting the impact of extreme values.

Another positive is usability. Once we built the first model, moving from a single time series to multiple time series wasn’t that difficult. At least from my perspective, the transition felt fairly smooth, which is a big win when scaling models.

Additionally, LSTMs—and deep learning models in general—are excellent at handling non-linearity. They allow regressors to capture complex patterns that traditional models often struggle with, making them very powerful in many real-world scenarios.

Now let’s look at the downsides.

One major drawback is the lack of interpretability. We don’t get much insight into why the model makes certain predictions. This lack of explanatory power becomes a real issue if you rely solely on neural networks. In practice, this often means you need to combine LSTMs with other models that provide better explanations.

Another big challenge is tuning. This was probably the hardest part of the entire section. You almost absolutely need a GPU unless you don’t care about time at all. Without one, training and tuning quickly become impractical. This is also why I pushed so hard to make everything work efficiently—we needed to manage computation realistically, and in the end, I think we did.

Putting an LSTM model into production also requires effort. There’s training time, tuning time, and experimentation time. Even though we only had 16 parameter combinations per round—which really isn’t a lot—it still took time and compute resources. So this is something you always need to factor in.

Finally, here’s an important watch-out.

LSTMs—and neural networks in general—are not particularly good at dealing with strong trends. They handle sequence data well, but when a time series has a powerful upward or downward trend—especially explosive growth or sharp declines—performance can suffer. If your data exhibits strong trends, you need to be very careful. Make sure the model is well trained, thoroughly tested, and validated before relying on it.

And that’s it for LSTMs.

**XVI) Section 16: LSTM - Multiple Time Series Forecasting**

**A) Multiple Time Series Forecasting**

All right. Welcome to multiple time series. To illustrate this, imagine that you have a coffee shop chain. Dealing with just one store is already complicated, but let's say you want to predict the next month's coffee sales. Along with that, you would also need to plan for how many coffee beans you are going to need, the milk, and so on. For now, let's focus solely on predicting sales.

If you need to predict sales not just for one store but for multiple stores, then you need a method that can handle this complexity. This is where multiple time series forecasting comes in. Each store might have its own trend, but there is also interconnectedness among them. You don’t want to maintain many separate models predicting the same thing when a single model could do it. By using multiple time series, we can leverage information from one series to help predict another. So it’s not just about using the past to predict the future; we also use different series that have valuable information to improve our predictions.

From a modeling perspective, this is very exciting. Essentially, you are looking at many things at once to try to forecast what will happen. For humans, this can be quite challenging, but deep learning is actually designed for this kind of task, making it very suitable.

Up until now, we have been using past data to predict the future and have incorporated regressors to aid our models. Now, we need to take it a step further by using multiple sequences together. This requires analyzing all series collectively and understanding how they interact.

Let’s look at the pros of multiple time series forecasting. First, it provides a more comprehensive view of what is happening. Instead of having separate models working in isolation, we measure error in one unified way. This gives us a bird’s-eye view of all sales across all cities. Second, it is highly scalable. What works for a few series works similarly for ten or more, both technically and in terms of time.

On the other hand, there are some cons. Multiple time series forecasting is inherently more complex. Additionally, because we are looking at a bird’s-eye view, we lose some granularity in understanding individual series.

Now, let me outline a step-by-step approach for performing multiple time series forecasting. The first step is collecting data. This is always crucial. In a real-world scenario, you would gather sales figures, external factors, and operational data such as seasonality or other factors that could influence KPIs.

Next, we build a spreadsheet to organize the data. This includes cleaning the data, handling missing values, scaling it appropriately, and performing feature engineering to best represent the problem. For example, we could include coffee sales, pastry sales, merchandise sales, weather, holidays, and average temperature as features. Feature engineering might also create new variables to enhance predictive power.

Once the data is prepared, we move to model building. We will continue using LSTM because we are already familiar with it. Traditional models struggle with multiple series because they are not designed for it, but deep learning models like LSTM can handle both single and multiple series. Everything learned for one series can be applied to multiple series. LSTM, a type of recurrent neural network, was initially created for sequence data and is particularly effective for time series due to its long-term memory capabilities.

With multiple time series, we can link different series together. For instance, coffee sales could be linked to pastry sales, or iced coffee sales could relate to lighter pastry sales. This linkage is only possible with deep learning models.

Our approach will involve preparing the data, building an LSTM model, making predictions, and evaluating the results. The goal is to take a complex problem and apply a solution that is simple and natural, without overengineering.

That’s the plan. I hope this overview makes sense. Let me know if you have any questions, and I’ll be happy to help. See you in the next video.

**B) M4 Data set**

Welcome back. Let me share with you the M4 dataset. This dataset was part of the M4 competition, one of the most prestigious and comprehensive forecasting competitions ever held. It is an excellent resource for learning about multiple time series forecasting.

Let me take you back to 2018, when the M4 competition was organized by Professor Spyros Makridakis and his team. Apologies if I mispronounced his name. This was the fourth in a series of competitions aimed at advancing the field of time series forecasting. The competition featured 100,000 time series from diverse domains, including finance, economics, demographics, and more.

The goal of the competition was simple yet ambitious: to evaluate the performance of various forecasting methods and to encourage the development of new techniques. Participants were challenged to forecast future values of these time series using any method—or combination of methods—they preferred.

The datasets were split into different frequencies, such as yearly, quarterly, monthly, weekly, daily, and hourly. Among these, the hourly dataset stands out due to its granularity and complexity, making it one of the most challenging datasets. This is why we are going to use it.

The hourly dataset consists of 414 time series, each representing hourly data points. These series come from various real-world applications, such as electricity consumption, weather conditions, and traffic flows. The dataset covers different periods, with each series containing varying lengths of historical data. This variety provides a rich learning environment for us.

So why is this dataset particularly great for multiple time series forecasting? First, the granularity is key. The hourly frequency provides a high level of detail and captures subtle patterns and short-term fluctuations that might be missed with lower-frequency datasets. This granularity is crucial for understanding the dynamics of time series data and sharpening your forecasting skills.

Next, the complexity of the dataset is significant. It includes a wide range of time series with different characteristics. Some series exhibit clear seasonal patterns, while others show irregular trends or sudden spikes. This diversity presents a unique challenge and an excellent opportunity to learn how to manage such data and see how models like LSTM handle it.

Moreover, the dataset’s real-world relevance is another important factor. The time series are derived from real-world applications, meaning your work will reflect challenges that businesses and organizations face daily.

In conclusion, the M4 dataset offers a highly challenging environment for learning multiple time series forecasting. Its granularity, complexity, and real-world relevance make it an invaluable resource for anyone looking to deepen their understanding of forecasting techniques.

So let’s get started. My challenge is to make this material both interesting and digestible for you, and I’m very much looking forward to it. Till the next video, have fun!

**C) Python - Preparing Script and Loading Data**

Welcome to multiple time series. You are going to find a starter file in the multiple series folder, which is also inside the deep learning folder. I’ve already run the code for the first video because some parts, like working with DART, can be time-consuming. Feel free to pause here, connect to your drive if you’re using Google Colab, and follow along with the folder and the DART library.

Let’s start with the initial step and the decisions I made. First, we have our data, and I really want to show it to you. We have a pandas DataFrame with columns labeled as V’s and H’s. It’s important to note that the V’s represent hours, while the H’s represent the time series. I know it seems a bit confusing, but this is how the dataset is structured.

To make sense of this data and transform it for easier use, we use numpy. Specifically, we do .values.squeeze() to get just the numerical values and tidy the structure. After applying this, our data shape is 415 by 961. Next, we remove the V and H labels, as they don’t have meaningful information for our analysis. If we need the hours later, we can create them ourselves. After excluding the first row and first column, we have a shape of 414 by 916.

Since the data is in an unusual format, we need to transpose it. This switches the dimensions from 916 by 414 to 414 by 916, which is easier to work with for our multiple time series analysis.

One thing to address is the presence of nine extra values. There’s no explanation for them in the documentation, and dealing with them would add unnecessary complexity. My goal here is to focus on multiple time series rather than handling these unknown values. Therefore, I made an executive decision to remove the rows containing these nine values rather than the columns. Specifically, rows 700 to 959 have no meaningful values, so we remove them while keeping all columns. After this, our dataset contains 700 hours and 414 time series, which is what we’ll use for our analysis.

I understand that some of you may have questions about why we are not handling missing or unknown values, but the main purpose here is to focus on learning and applying multiple time series forecasting. If you have any questions about this preparation or anything else related to multiple time series, feel free to post in the Q&A, and I’ll be happy to discuss it.

This concludes the preparation, and I’ll see you in the next video.

**D) Python - Preprocessing Time**

Welcome back. In this video, we are going to create a time series object. Spoiler alert: it’s going to be here, and we’ll use a specific Darts function to do this.

First, we start by creating a pandas DataFrame. Why are we doing this? Over the course of this module, it’s very helpful to have the data in a pandas structure for easier manipulation. Let’s call it dataframe_pandas so it’s clear what we are referring to. We can do .head() to inspect the first few rows.

Currently, we don’t have dates in the dataset, so we’re going to create a dummy starting date. This is important for visualization purposes—it’s easier to understand the data when we see dates rather than just hour numbers. For example, after the 300th hour, it’s hard to interpret without actual timestamps. We create the start date using pandas:

start_date = pd.Timestamp("2000-01-01 00:00:00")


Next, we’ll create the time series object using Darts. The function we use is from_times_and_values. Here, we provide both times and values.

To define the time axis, we generate a date range using pandas:

time_index = pd.date_range(
    start=start_date,
    periods=dataframe_pandas.shape[0],
    freq='h'  # lowercase 'h' since uppercase 'H' is deprecated
)


This gives us a continuous time index with no holes, representing each hour. After running this, we have our index from 0 to 413, which matches our 414 columns of time series data.

Now we can create the series object:

series = TimeSeries.from_times_and_values(times=time_index, values=dataframe_pandas.values)


And with that, we are ready to move on.

In the next video, we’ll focus on data exploration. Normally, I would have this pre-prepared, but since we are working with multiple series, I want to go step by step so you can see the reasoning behind the visualizations and understand why they make sense.

**E) Python - Visualization + Hourly Seasonality**

Welcome back. In this video, we’re going to focus on data visualization and exploratory data analysis (EDA) for our multiple time series.

To start, we can attempt a simple plot using dataframe.plot(). However, with 400+ series, this quickly becomes overwhelming and doesn’t provide any meaningful insight. Even trying plt.plot() or minor tweaks doesn’t really help—the sheer number of series makes the plot hard to interpret.

The key question here is: what do we actually want to understand from our data? One interesting thing to explore is whether the series connect to each other. For example, do they exhibit similar seasonalities? The purpose of this video is to focus on daily seasonalities and how we can visualize them effectively.

First, we transform our Darts series back into a pandas DataFrame:

dataframe_with_time = series.pd_dataframe()


Now that we have the DataFrame with time as the index, we can compute hourly means to start uncovering patterns. Instead of resampling, which may not work as expected, we can use groupby on the hour component of the index:

hourly_means = dataframe_with_time.groupby(dataframe_with_time.index.hour).mean()


This gives us the average values for each hour across all series. Now it’s ready to plot:

plt.figure(figsize=(12,6))
plt.plot(hourly_means, marker='o')
plt.title("Hourly Means Across Series")
plt.xlabel("Hour of the Day")
plt.ylabel("Average Value")
plt.grid(True)
plt.xticks(range(24))
plt.show()


Even after this, the plot can still be cluttered because of the large number of series. One approach is to select a subset of the series, for example the first ten:

plt.plot(hourly_means.iloc[:, 0:10], marker='o')


This provides a clearer view, but another challenge remains: the series have different magnitudes. Some appear almost flat at the bottom, which makes it harder to interpret their seasonality.

For now, this gives us an initial draft of how to visualize daily patterns. The next step will be to standardize the data so that we can clearly see seasonality across series.

That’s it for this video. In the next one, we’ll focus on standardization and improving our visualizations to better capture patterns.

**F) Python - Daily Seasonality**

All right, let’s continue. The next step is to standardize the hourly means. We can start by computing the hourly means and then standardizing them. There are a few ways to do this—one approach is using pandas, another is with NumPy, which is slightly simpler:

hourly_means = np.mean(dataframe_with_time.values, axis=0)
hourly_std = np.std(hourly_means, axis=0)
standardized_hourly_means = (hourly_means - np.mean(hourly_means)) / np.std(hourly_means)


Once standardized, we can plot the first ten series to inspect the patterns. Adding gridlines, titles, and labels makes the visualization easier to interpret. Most of the series now follow a fairly consistent pattern, except for a few outliers.

Expanding the plot to include the first 100 series confirms that most series share similar information and patterns. This is important because it means our model can leverage one series to help predict others, taking advantage of these shared trends. Even the slightly different series are not completely unique—they still have some common structure. At a larger scale, say 50,000 series, this shared information becomes even more valuable for the model, enabling high predictive accuracy.

Next, we extend this analysis to weekly seasonality. We compute daily means grouped by the day of the week and then standardize these values in the same way. Plotting the first ten series gives us an initial view of weekly patterns. While these patterns are less specific than the hourly patterns, we can still observe consistent trends—some series rise, stabilize, or fall at similar points during the week. Plotting the first 100 series further confirms that there is still valuable information in the weekly trends.

Overall, these visualizations show that multiple time series share significant structure, both daily and weekly. This validates the idea that deep learning models can leverage information across series to improve predictions.

In the next video, we will focus on building time series covariates, which will be very similar to what we have done in the past, but adapted for multiple series. This will set the stage for modeling and forecasting.

**G) Python - Time Covariates, Scaling and Time Steps**

All right, let’s move on to the next step. In this video, we are going to focus on creating the time series covariates. Initially, these covariates are quite simple—they represent hours and weekdays. Since we have around 700 observations, having these two features is enough to start.

First, we create a time series instance for the hour. We can use the pandas date range from our dataset to define this. We also apply one-hot encoding to represent the hours as categorical features, which helps the model better understand cyclical patterns.

Similarly, we create the weekday series. Again, we use one-hot encoding so that each day of the week is represented as a separate binary feature. This allows the model to capture weekly seasonal patterns.

Next, we import a scaler to normalize our covariates. We use fit_transform to scale the series, which maps the values between 0 and 1. This step ensures that all covariates are on the same scale, making it easier for the LSTM model to learn effectively.

Finally, we stack the covariates together. This combines the hour and weekday series into a single input structure that can be fed into the model. At this point, we have our time series covariates fully prepared.

The next step will be to use these covariates to build the LSTM model for forecasting multiple time series.

**H) Python - LSTM Model and Cross-Validation with Multiple Series**

Welcome back. In this video, we are going to build the LSTM model for forecasting our multiple time series. We start with the model configuration, which involves defining the input chunk length, forecast horizon, and training length. For this example, the forecast horizon is set to 24 hours, the input chunk length is 48, and the training length is the sum of the chunk length and forecast horizon. These parameters define how much past data the model sees and how far ahead it predicts.

Next, we construct the LSTM model. Using the Darts library, we specify parameters such as the hidden dimension, number of RNN layers, dropout, epochs, and random state. For this example, the hidden dimension is 188, we use two RNN layers, a dropout of 0.1, 10 epochs, and a random state of 1502. We also set the input chunk length and optimizer learning rate at 0.003. Device selection and other trainer configurations are handled as well.

Once the model is built, we fit it using our scaled series and the time series covariates (hour and weekday series). The covariates are passed as future covariates to allow the model to use known external features for prediction.

During training, you might encounter a user warning regarding dropout and the number of RNN layers. Dropout only applies when the number of layers is greater than one, which is why we increased our RNN layers from one to two. After this adjustment, the model runs smoothly.

With the model trained, we move on to cross-validation and historical forecasting. Using the historical_forecast function, we specify the series, future covariates, starting point, forecast horizon, stride, retrain flag, and whether to consider only the last points. One important detail is ensuring the stride is an integer, as forecast_horizon / 2 may return a float (e.g., 12.0), which causes errors. Adjusting this to an integer resolves the issue.

We also clarify the output chunk length, which is set to 1 to predict one time step at a time. Even though the model predicts one step at a time, we can still aggregate predictions to cover the entire forecast horizon of 24 hours.

After making these adjustments, the model runs successfully, performing historical forecasts over the multiple time series using both past observations and covariates.

In the next video, we will analyze the model output and interpret the predictions. This will help us understand how well the LSTM model captures daily and weekly patterns across multiple series.

**I) Python - CV Results**

All right, welcome back. The cross-validation process has completed, and it was quite fast—about 39 seconds. Now, let’s check the performance of our model using cross-validation. Since this is multiple time series, we don’t necessarily know what each series represents. The actual values themselves are abstract, so it’s difficult to label them as “good” or “bad.” However, evaluating performance is an essential part of parameter tuning and ensures that we have a robust version of the model.

To begin, we initialize an empty list for RMSE values and iterate through the cross-validation results. For each CV fold, we first get the predictions and convert them from a time series object to a pandas DataFrame. Using the scaler, we inverse transform the predictions so that they are on the original scale. We then determine the start and end indices of each fold using the predictions’ index, and extract the corresponding actual values from our original DataFrame. With the predicted and actual values aligned, we compute the RMSE for each fold and append it to our RMSE list.

After processing all CV folds, we summarize the results by calculating the mean RMSE across folds and display it using an f-string. One common issue that arises here is ensuring that the iteration over CV folds uses integers. Using range(len(CV)) instead of range(CV) avoids the “time series object cannot be interpreted as an integer” error.

For example, after correcting this, the mean RMSE might show a value like 479.9. This value is not about interpreting whether the prediction is “good” or “bad”—it is simply part of model evaluation and optimization. The cross-validation RMSE informs our parameter tuning, where we can experiment with different hyperparameters to improve performance.

Finally, the next step will be visualizing the cross-validation results, which provides a clear, graphical representation of model performance across folds. This visualization is very helpful before proceeding to full parameter tuning.

**J) Python - CV Visualization**

Welcome back. In this video, we are going to visualize the cross-validation results. The process is straightforward. We start by iterating over the cross-validation folds using for i in range(len(CV)). For each fold, we create a figure with a size of (15, 5), which usually works well for this type of plot.

Next, we plot the predicted values from the current fold. Since the predictions were scaled, we first inverse transform them using the scaler. Then, we plot the actual values for the same fold using series.plot(label="Actuals"). We also add a legend and a title to the plot, labeling each fold appropriately with Fold i+1.

Sometimes, a warning appears: “number of series components is larger than the maximum number of components to plot.” By default, the maximum number of components to plot is ten, but this can be adjusted with the max number of components parameter. Even with adjustments, you may still get a minor warning, but the plot becomes much easier to interpret.

One important detail: make sure the last_points_only option is set correctly. If it is True, you might see only one point plotted for the end, which can look odd. Setting it to False ensures that the full series is displayed, making the visualization meaningful.

Once the visualization runs, you can clearly see the complete series along with the corresponding cross-validation predictions. This allows you to observe specifically where the cross-validation predictions fall within the series. The visualization naturally progresses from left to right along the time axis, showing how predictions evolve as more data becomes available.

This method is especially useful if you want to highlight a specific portion of the series. You can subset the components and compare the actual values versus predicted values directly for clarity. Overall, this visualization is a powerful and intuitive way to see the performance of your model before moving on to parameter tuning.

In the next video, we will start parameter tuning and work on improving the model further.

**K) Python - Parameter Tuning**

In this section, we move on to parameter tuning for our multiple time series setup. As we already know, the first thing we need to do is define a parameter grid. Unlike the earlier single-series example where we explicitly split the series, here we will not do any manual splitting. Instead, we rely fully on cross-validation to evaluate performance. Of course, you can choose to run one round, two rounds, or even three rounds of tuning depending on how exhaustive you want the search to be.

For this example, we start with a Temporal Fusion Transformer (TFT) model. This is not just to “spice things up,” but also to show that there are multiple equally valid ways to perform tuning beyond the models we used earlier. The logic remains the same; only the model configuration changes.

We begin by defining the parameter grid. For dropout, we use values of 0.05 and 0.1. For the number of epochs, we select 10 and 15. Next, we configure the learning rate, adding values such as 0.03 and 0.005, especially since we observed earlier that training with these settings runs relatively fast.

For the training length, we verify the calculation carefully. Since the input chunk length plus the forecast horizon was initially 48 + 24, that gives us 72. Based on this, we test training lengths of 72 and 96. For the input chunk length, we confirm that 48 was used earlier, and we extend it by also testing 72. With all these combinations defined, we create the full parameter grid.

Once the grid is ready, we estimate how many combinations we have. With all parameters combined, the grid size becomes fairly large, and we can expect the run time to be long—easily around an hour or more. This is why parameter tuning is often left running while you take a break.

Next, we initialize an empty list to store results, typically something like an RMSE list. We then iterate through the parameter grid using a loop such as for params in grid. Inside this loop, we create the model using the current parameter combination. This includes configuring dropout, number of epochs, learning rate, training length, input chunk length, optimizer arguments, and trainer settings.

After initializing the model, we run cross-validation. We pass in the scaled time series, future covariates, forecast horizon, and specify that last_points_only is set to False so that we retain the full prediction windows. This produces a list of cross-validation predictions.

For each fold in the cross-validation output, we process the predictions step by step. First, we convert the predictions from a time series object into a DataFrame. Then, we apply the scaler’s inverse transformation so the values are back in their original scale. We determine the start and end indices of the prediction window using the minimum and maximum timestamps of the predictions.

Using these indices, we extract the corresponding actual values from the original Pandas DataFrame. Once we have both the predictions and the actuals aligned over the same time range, we compute the RMSE for that fold. Each fold’s RMSE is appended to a temporary list for that parameter configuration.

After iterating through all folds, we compute the mean RMSE across folds and append it to the main results list. This value represents the overall performance of the current parameter combination. The loop then continues to the next set of parameters in the grid.

At this point, everything is running as expected. Because the tuning process is computationally expensive, it’s common to leave it running and return later to inspect the results. Once the process completes, you’ll have a list of RMSE scores corresponding to each parameter configuration, allowing you to identify the best-performing setup.

This is where we stop this video. In the next video, we’ll come back, look at how long the tuning took, and analyze the results to decide which parameter combination performs best.

**L) Python - LSTM Parameter Tuning Results**

Very eager to wrap this up because this run took a long time. It says around three hours, but in reality it was almost four. I literally played a bit, did some exercise, took a shower, came back—and only then it was done. So yes, I am very happy that it finally finished.

Now, of course, you might ask: “Okay Diogo, is there really a point to all of this? Why are we suffering through all this pain?”
And yes, there is a point. I did not expect it to be this painful—almost four hours. Even two hours would already have been painful enough. But there is a point, and we are going to get to it now.

What we do next is attach the RMSE results to the corresponding parameter grid. We then display the results. Initially, creating a Pandas DataFrame with the grid and results looks quite messy, and honestly, we don’t really need all that raw formatting. Instead, we just keep the results clean and transform them into an interactive table, which is much easier to explore visually.

Once we sort the results, we start seeing clear patterns. Most of the best-performing configurations have a hidden dimension of 25. The input chunk length appears consistently favorable, the learning rate of 0.005 dominates, although 0.03 does show up occasionally. The number of epochs is almost always 15, and the number of RNN layers is consistently two. Training lengths vary slightly, but 72 seems preferable overall.

This naturally raises an important question:
Do we really need 128 parameter combinations and nearly four hours of computation to figure out that the number of epochs should be 15, hidden dimensions around 25, and RNN layers equal to two?

Because what this is really telling us is more exploratory insight than absolute truth. Maybe we should have tested 20 epochs. Maybe one RNN layer would have been sufficient without dropout. Could we try hidden dimensions of 30? Could a slightly higher learning rate work better?

The key takeaway is that this approach is inefficient. This is the most basic form of parameter tuning—the fundamentals—but it’s computationally expensive and slow.

From here on out, and throughout the rest of the course, we will move toward more efficient parameter tuning strategies. Approaches that do not require this level of brute-force computation, and that could easily land us in the top five or top ten configurations within five or ten minutes, rather than hours. That is the real goal.

Now, since this run took four hours, we definitely want to export the best parameters. We extract the best-performing configuration and save it—not as a dictionary, but as a CSV file, named something like best_params_lstm.csv.

It’s also worth remembering that in a previous section, we worked with a single-series LSTM, where we did tuning in round one and round two. That approach is more efficient in practice, but it also means we optimize parts independently and never see the full picture. Here, we do see the full picture—but it’s inefficient. Ideally, we want a balance between both.

With the best parameters exported, we move on to predicting the future. We start by importing the CSV file using Pandas and setting the index column correctly. At this point, we need to be careful with data types. Some parameters are floats, others should be integers—such as hidden dimensions, input chunk length, training length, and number of epochs.

Instead of transforming everything at once, it’s cleaner to explicitly extract each parameter and cast it to the correct type. We assign integers to the RNN layers, hidden dimensions, epochs, training length, and input chunk length, and floats to dropout and learning rate. Once everything is properly typed, we are good to go.

During this process, we notice a small issue when accessing parameter values. The fix is to use .iloc[0] to correctly retrieve values from the DataFrame row. This also helps avoid future warnings and makes the code more robust. Once corrected, everything works as expected.

At this point, the best LSTM parameters are loaded, cleaned, and ready to be used.

I’m going to stop this video here.
In the next one, we’ll add future covariates, which are currently missing. Since we are predicting the future, we also need future values for the time-based covariates such as hours and weekdays. Once those are added, we’ll be able to fully wrap up the LSTM for multiple time series.

**M) Python - Predicting the future**

At this stage, we need to generate future values for our time series dates. Specifically, we want to create a future time index that starts one hour after the last timestamp in our data. This is important because if we start at the same timestamp, we would create an overlap between historical data and future predictions. Since predictions begin one hour ahead, the future index must reflect that.

To do this, we use pandas.date_range. The start is set to the last timestamp in the dataset plus one hour. We define the frequency as hourly and specify the number of periods based on the forecast horizon. This gives us a clean future datetime index that begins exactly where predictions should start. This completes step one.

In step two, we create time series objects for the future covariates, specifically the hour of day and weekday. This follows the exact same structure used for the historical covariates. Using the future time index, we generate a time series object and then extract datetime attributes such as hour and weekday. These attributes are one-hot encoded in the same way as before, ensuring consistency between historical and future covariates.

Next, we stack the future covariates together. First, we stack the future hour and future weekday features. Then, we append these future covariates to the existing historical covariates. We do not replace the old covariates; instead, we extend them forward in time. This ensures that the model has covariate values both for the training period and for the forecast horizon.

With future covariates prepared, we move on to building the tuned LSTM model. We initialize the RNN model with model="LSTM". We pass in the best parameters obtained from tuning: hidden dimension, number of RNN layers, dropout, number of epochs, training length, and input chunk length. We also set the random state to 1502 for reproducibility, include the optimizer parameters, and define the PyTorch Lightning trainer arguments.

Once the model is defined, we fit it on the scaled series, this time explicitly providing the future covariates. The training runs for the tuned number of epochs, which in this case is 15. Training completes successfully.

After training, we move to the final step: making predictions. We call model.predict() with the forecast horizon and pass in the future covariates. However, at this point, we encounter an error. The error message indicates that the provided future covariates do not extend far enough. Specifically, the future covariates must start at or before a certain time step, but the model is detecting a mismatch.

This suggests that something is off with the date alignment. To debug this, we start inspecting the future weekday covariates. We notice that they begin on January 30th. We then check the scaled series and confirm that it also ends on January 30th. At first glance, this seems correct.

However, when we inspect the original covariates, we see that they extend up to January 31st. This is unexpected and indicates a misalignment between the series and covariates. The covariates appear to go further in time than the target series, which can confuse the model when it tries to align past and future inputs.

We then check the lengths of the objects. The scaled series has a length of 700, while the covariates have a length of 724. Although this might seem acceptable since covariates can extend further, the index alignment is what really matters. Something in how the covariates were stacked or appended has shifted the time index.

At this point, despite checking the stacking logic, inspecting indices, and verifying lengths, the issue is still not immediately obvious. This is one of those subtle time-index alignment problems that can be tricky to debug live.

Rather than dragging this on further, I’m going to stop this video here. I’ve done as much live debugging as reasonably possible. In the next and final video, I’ll show you the exact fix, explain what was wrong, and demonstrate the correct way to align future covariates for prediction.

**N) Python - LSTM Debugging**

This one turned out to be fairly easy once identified. The issue was simply in how the covariates were passed during prediction. Instead of passing only the future covariates, I should have passed the full covariates object, which already contains both historical and future covariates. When I used the shortcut, I mistakenly provided only the future portion, which caused the misalignment error.

So the fix was straightforward: during prediction, use covariates instead of future_covariates. This ensures the model has access to the entire covariate timeline, covering both the past (needed for context) and the future (needed for forecasting). Once this change is made, the prediction step runs correctly without any date or alignment issues.

The final step is to transform the predictions back to their original scale. The model outputs predictions as a time series object in scaled form, so we first convert the predictions into a DataFrame using time_series_to_dataframe. After that, we apply the scaler’s inverse_transform method to bring the predictions back to their original units. This gives us the final, interpretable forecast values.

At this point, this output is exactly what we want. You could optionally add some visualization to plot the predictions against historical data, but functionally, this DataFrame is the final result. From here, it can be stored, exported, or directly written into a database or downstream system.

And that’s it for us. I hope you enjoyed this walkthrough. I know it had its painful moments, but it also covered a complete, real-world workflow—from feature engineering and modeling to tuning, debugging, and final prediction. If you have any questions, feel free to ask. I’m happy to help, and I’ll see you in the next video.

**XVII) Section 17: Temporal Fusion Transformers (TFT)**

**A) Game Plan for TFT**

Imagine you’re running demand forecasting for a large retail chain, but your current models just can’t keep up. Sales are influenced by intricate seasonality, promotions, holidays, special events, and even sudden market shifts. Traditional approaches like ARIMA or even standard LSTMs start to fall short in these situations. They struggle to dynamically adapt to all these interacting factors, especially when the forecasting horizon extends beyond just the next few steps.

This is where DFT (Temporal Fusion Transformers) truly stands out. DFT shines precisely in scenarios where other models struggle. It is designed to deliver accurate multi-horizon forecasts, while explicitly accounting for complex temporal patterns and multiple influencing variables. Whether it’s promotions, calendar effects, or changing market dynamics, DFT can incorporate all of this information into a single, coherent forecasting framework.

By the end of this section, you’ll have a clear understanding of the DFT architecture. We’ll break down its unique components step by step, including how attention mechanisms work and how the model is structured to handle multi-horizon forecasting. The goal is not just to use the model, but to truly understand what is happening under the hood and why it works so well for complex forecasting problems.

You’ll also gain hands-on experience building a DFT model from scratch using the Darts library in Python. We’ll go deep into the implementation details, exploring every important configuration and design choice. This means you won’t just copy-paste code—you’ll understand how each part contributes to the model’s performance.

Beyond building the model, we’ll focus on optimizing DFT performance. You’ll learn practical techniques to fine-tune hyperparameters, improve accuracy, and make the model more robust. Optimization is a critical step, especially for real-world applications where both performance and efficiency matter.

What truly sets DFT apart is its ability to perform multi-horizon forecasting, predicting multiple future time steps at once. This is incredibly valuable for operational and strategic planning, where decisions often depend on forecasts across several future periods rather than a single point estimate.

Another key strength is its attention mechanism, which dynamically prioritizes the most relevant features at each point in time. This allows the model to focus on what truly matters for the forecast, significantly improving accuracy in complex environments.

DFT also excels in interpretability. Unlike many black-box models, it provides insights into which features have the greatest impact on the forecast. This transparency makes the results more trustworthy and easier to explain to stakeholders.

Finally, DFT offers remarkable flexibility with inputs. It can seamlessly handle a wide variety of data types, including static features, known future covariates, and observed past variables. This flexibility makes forecasting more robust, comprehensive, and adaptable to real-world business needs.

This is an exciting section. DFT is a powerful and modern forecasting technique, but it is also complex. That’s why we’ll approach it step by step, ensuring clarity at every stage. Let’s get started.

**B) Introduction to Temporal Fusion Transformers**

Let me start with a story about one of the tech giants: Uber.

Back in 2018, Uber faced major challenges when it came to accurately forecasting ride demand. Urban environments are highly dynamic—demand shifts due to time of day, weather, special events, and sudden market changes. Traditional forecasting approaches such as ARIMA and even LSTM-based models were not sufficient. They struggled to cope with the complexity, variability, and multi-scale nature of the data.

To overcome these limitations, Uber’s data science team decided to implement Temporal Fusion Transformers (TFT). The motivation behind this choice was clear: TFT can handle multiple forecasting horizons within a single model and dynamically adapt to a wide range of influencing factors. This made it especially suitable for Uber’s use case, where demand patterns vary not just across time, but also across regions and external conditions.

Using TFT, Uber was able to forecast ride demand across different locations and time periods while explicitly accounting for special events such as concerts and sports games, as well as weather conditions. The results were significant. The TFT-based approach improved forecast accuracy by approximately 15% compared to previous models. This improvement allowed Uber to better align driver availability with rider demand.

As a direct business impact, more accurate forecasts reduced passenger wait times by about 10% and increased the number of rides per driver by roughly 12%. Additionally, improved demand predictions helped Uber optimize its driver incentive programs, leading to an overall operational cost reduction of around 5%. This real-world success highlights why TFT has gained so much attention in modern time series forecasting.

Now, enough with the story—let’s focus on what really matters for this lecture.

In this section, we will cover the core principles of Temporal Fusion Transformers. We’ll break down the key components that make TFT so powerful, explain how it excels at multi-horizon forecasting, and explore how it dynamically prioritizes relevant features over time. We’ll also discuss why interpretability and flexibility are central strengths of this model.

At its core, the Temporal Fusion Transformer is an advanced deep learning architecture designed specifically for time series forecasting. It integrates multiple data sources and adapts dynamically to changing temporal patterns, which significantly improves forecasting accuracy in complex, real-world scenarios.

One of the most important components of TFT is its attention mechanism. Attention allows the model to focus on the most relevant parts of the data at each time step, ensuring that critical signals are emphasized while less relevant information is down-weighted. This selective focus plays a key role in improving prediction quality.

TFT also incorporates LSTM-based encoders and decoders, which are responsible for capturing long-term dependencies in time series data. In addition, it supports static covariates—features that do not change over time but strongly influence outcomes, such as city, region, or store type. We’ll dedicate a separate lecture to static covariates, as they are crucial for understanding how TFT personalizes forecasts.

When it comes to multi-horizon forecasting, TFT truly sets itself apart. Traditional models like ARIMA or standard LSTMs can predict future values, but they often require separate models or complex configurations for different horizons—for example, one model for hourly forecasts and another for daily forecasts. TFT, on the other hand, is designed to predict multiple future time steps simultaneously within a single framework.

This means a single TFT model can produce hourly, daily, and weekly forecasts in one go. For companies like Uber, this is extremely valuable, since operational decisions often depend on forecasts across multiple time scales—different times of day, days of the week, and even longer planning horizons. Having one unified model makes the forecasting pipeline simpler, more consistent, and easier to maintain.

Another defining feature of TFT is dynamic feature prioritization. Through its attention mechanisms, the model continuously adjusts the importance of different features over time. This allows it to adapt in real time to changing conditions. For example, during peak travel hours or major events, factors such as weather or local concerts may become far more influential than usual.

Conceptually, you can think of this as a heat map of feature importance over time. Darker regions indicate higher attention weights, showing which features the model considers most relevant at each moment. While these visualizations are often conceptual, they reflect how TFT internally adapts to shifting patterns in the data.

Interpretability is another major strength of TFT. Because the model explicitly computes attention weights and feature importance, users can understand why a certain prediction was made. You can see which features mattered most at a given time and how their influence changed. This level of transparency is especially valuable in business settings, where trust, explainability, and actionable insights are just as important as raw accuracy.

Finally, TFT is highly flexible in terms of inputs. It can handle static features such as city or holiday indicators, known future inputs such as scheduled events, and observed historical data like past demand. This flexibility allows the model to seamlessly combine diverse data sources and capture the full complexity of real-world forecasting problems.

To summarize, Temporal Fusion Transformers represent a major leap forward in time series forecasting. They are not just more complex models—they are more complete models, capable of handling multiple horizons, diverse inputs, dynamic feature importance, and interpretability all at once. Just as Uber successfully leveraged TFT to improve operations, you can apply the same principles to your own forecasting challenges.

In the next video, we’ll go deeper and start exploring TFT in practice.

**C) Case Study Briefing - Electricity Pricing**

In this video, we’re going to walk through a very cool case study where we bring Temporal Fusion Transformers (TFT) to life using a real-world electricity consumption dataset. This dataset captures electricity usage over time and is rich with patterns, trends, and seasonality. It gives us the perfect foundation to understand how TFT works in practice and how it can be used to forecast future electricity demand accurately.

One of the key reasons this dataset is so well suited for TFT is the presence of dynamic temporal patterns. Electricity consumption is not static—it varies significantly by hour of the day, day of the week, season, and even special circumstances. These natural fluctuations make the dataset an excellent playground for advanced forecasting techniques, especially models like TFT that are designed to adapt to changing temporal dynamics.

There is also strong real-world impact behind this use case. Accurate electricity demand forecasting is absolutely critical for energy providers. These forecasts help balance supply and demand, ensuring grid stability and preventing outages. Poor forecasting can lead to inefficiencies such as overproduction, wasted energy, or even situations where electricity prices become negative because supply far exceeds demand. This makes the problem not just interesting, but genuinely important.

From a modeling perspective, this case study represents a step into advanced forecasting territory. We are moving beyond simple univariate examples and starting to incorporate multiple features and richer structures. In this section—and especially in the challenge that follows—we will explore how TFT handles complexity, multiple inputs, and long-term dependencies within the data.

Our primary goal here is to build a large, expressive TFT model capable of handling time series with depth and structure. We’ll start by focusing on a single series to keep things manageable, but this is just the first step. TFT is particularly strong at handling multivariate time series, and we’ll gradually move in that direction as well.

Another important focus is on long-term dependencies. We want the model to distinguish between short-term noise and meaningful long-term trends. Just as importantly, we want to extract interpretable insights from the model. This means not only making accurate predictions, but also understanding why the model predicts what it does. We’ll work toward visualizing and interpreting the outputs so that the reasoning behind the forecasts becomes clear.

To wrap it up, by the end of this case study, you won’t just have built a state-of-the-art forecasting model. You’ll also understand the logic behind its predictions. This combination of strong predictive power and interpretability is exactly what makes Temporal Fusion Transformers such a powerful and exciting tool.

**D) Python - TFT Starter File**

Welcome to TF. This is going to be, I think, the most complex topic that we’ve covered so far, especially from a more theoretical perspective. However, from a practical point of view, it’s not going to be very different, because we have already learned quite a bit—especially when working with the Darts library. So from that angle, this should be a breeze for us.

You’re going to find the TF folder inside the deep learning folder, and inside that, you’ll find a starter file. I’ve already taken care of mounting Google Drive, setting up the path folders, and installing the required library. You just need to change whatever is necessary in the path; otherwise, everything more or less stays the same.

Next, we’re going to import the libraries and start exploring the data. At the same time, I’ll also import the TF model. Alright, the TFT model is imported, and now we can take a look at the dataset itself.

This is actually quite an interesting dataset. We have electricity data along with a few exogenous regressors. What we’re computing here are electricity prices. What makes this particularly interesting is that electricity prices can actually be negative, which feels very strange at first—but it does happen. Prices become negative when the electrical network has too much electricity and not enough consumption. In those cases, prices keep decreasing to the point where they become negative, essentially signaling, “We can’t hold this anymore—please take it away from us.” So it’s a very interesting phenomenon to model.

Now, looking at the data, we have electricity prices along with a few countries included. For now, we’re going to stick to just one country to keep things simple. We’ll expand to others later as part of a project. Specifically, we’re going to focus on “D,” which represents Germany—Deutschland in its original format.

So we’ll focus only on D for now. When we inspect the information, we can see that we have a unique ID, which is D, along with the target variable y and two exogenous variables. We’ll start building from there. The data is hourly, so the first thing we do is set the frequency to hourly.

Let’s take a look at the price series. This is where you can clearly see what I meant earlier about negative prices—there are points where prices drop below zero. Overall, the series feels deeply seasonal. We see spikes, a more or less stable trend, and strong seasonal patterns throughout.

Let’s decompose the series. We have a couple of options here. For example, we can use 168, which is the number of hours in a week—24 times 7. If we run this, we get a quick preview. Looking at the target variable y, you can clearly see how deeply seasonal it is. There’s a very well-defined seasonal component, and the trend looks relatively stable, though it does fluctuate up and down.

Part of this behavior is due to the fact that hourly data is extremely volatile. Hourly time series are among the hardest to predict because the law of large numbers has the least effect at this level of granularity. The more granular the data, the less smoothing effect you get from statistical aggregation.

If instead we use 24 as the seasonal period, you can see that the trend becomes even stranger. The seasonal component, however, remains very well defined. You can experiment with different values and really explore the structure of the data.

You might also consider switching to a multiplicative decomposition to see the difference. However, we can’t use multiplicative seasonality here because the data includes zero or negative values. That makes total sense—once you start multiplying by negative values, the results become meaningless. So for our case, we’ll stick with additive seasonality.

Looking at the autocorrelation plot, you can see that there’s a lot of information present. There are clear spikes around 24 hours and 48 hours, which is exactly what we would expect given the hourly nature of the data. This confirms strong daily and multi-day dependencies.

If we look at the partial autocorrelation plot, we also see a lot of information. There are strong signals at one, two, and three hours before, as well as around 23, 24, and 25 hours before—capturing both short-term dependencies and seasonal effects. We also see signals around 48 hours, meaning there’s information coming from two days ago as well.

Finally, we move on to creating the time series object. I’ve added a comment here about static covariates. This becomes relevant when you build a TFT model with multiple static covariates—such as multiple countries. In that case, the unique ID (representing each country) would be included as a static covariate. This is how the TFT model would be informed about the multiple-entity perspective.

That’s it for this part. In the next video, we’re going to focus on something called additive encoders for time variables, and we’ll explore the documentation related to that. I’ll see you in the next video.

**E) TFT Model Architecture**

First and foremost, a warm welcome. This is going to be a fun video, and in this session we’re going to cover the Temporal Fusion Transformer (TFT) model architecture. So take a deep breath, close your eyes… and ta-da! Here comes a big, shiny equation. Don’t worry—we’re going to go through it step by step.

Let’s break it down. We start with y at time t, which represents the predicted value for the i-th time series at time t, for a given quantile q, at a forecast horizon τ. This can feel tricky at first, but the key idea is that TFT predicts values in terms of quantiles. Quantile forecasting is not mandatory in TFT, but it allows the model to express uncertainty and probability in its predictions.

Next, we have the forecast horizon τ. As discussed earlier, TFT supports multi-horizon forecasting, meaning the model can predict multiple future time steps at once.

Then we have f(q), which represents the function used to compute the quantile forecast. This function takes several inputs to generate predictions. One of these inputs is τ, which indicates how far into the future we are forecasting.

Another input is Y, which represents the historical target values for the i-th time series. This includes the past observations of the target variable and forms the backbone of the model’s understanding of historical behavior.

We then introduce unknown inputs, also called past covariates, for the i-th time series from time t−k to t. These are variables that are only known up to the current time step, such as past weather conditions, historical sales, or other observations that occurred in the past but are unknown in the future.

In contrast, we also have known inputs, also referred to as future covariates, spanning from time t−k to t+τ. These inputs are known in advance and include features such as holidays, day of the week, or scheduled events—anything that we can confidently project into the future.

Finally, we include static covariates for the i-th time series. These are features that do not change over time, such as location, product type, or category. Static covariates provide context that remains constant across all time steps.

Putting this together, the TFT model predicts across multiple forecast horizons τ, produces quantile-based forecasts, and leverages historical inputs, known future inputs, unknown past inputs, and static covariates. All of these components are bundled together to produce probabilistic forecasts.

Now, if you thought that equation was a whammy, here comes a double whammy. And just as a fun note, according to the dictionary, a whammy is an event with a powerful and unpleasant effect—a blow. So here we go.

What you’re now looking at is the actual TFT model diagram, and the main goal of this lecture is to help you master this framework. This is not easy—it’s big and complex—so we’ll go step by step, explaining each component and how everything builds on top of each other. If anything doesn’t make sense, feel free to reach out—I’m here to help.

We start by focusing on xₜ₊₁. Everything begins with variable selection. Just like in any model, this is where the inputs are chosen and weighted based on importance.

From there, the output of variable selection flows into the LSTM decoder. The LSTM decoder processes information from past data and known future inputs to generate predictions for future time steps. It operates sequentially, using its memory to capture both short-term and long-term dependencies while predicting step by step into the future.

After the LSTM decoder, we encounter the Add & Norm gate, which plays a crucial role in maintaining the stability and efficiency of the network. This gate combines the decoder output with residual connections from previous layers, helping preserve information and gradients. The output is then normalized to keep values within a manageable range, preventing exploding or vanishing gradients and improving training stability.

While the LSTM decoder excels at capturing sequential dependencies, some variables are better preserved without sequential processing. By allowing certain signals to bypass the LSTM, the model ensures that critical features have a direct influence on the final prediction. This hybrid approach strengthens forecasting accuracy by combining sequential modeling with direct feature influence.

This entire process is repeated for each future step—xₜ₊₁, xₜ₊₂, xₜ₊₃, and so on—up to xₜ₊τ. Variable selection, the LSTM decoder, and the Add & Norm gate work together to continuously refine predictions using both historical and known future inputs. At this stage, we are still dealing with known future inputs, such as day-of-week effects or holidays.

So far, we’ve focused on the future—but we also need to understand the past. This is where the LSTM encoder comes into play. The LSTM encoder processes historical inputs to capture temporal dependencies and encodes them into a fixed-length context vector. This vector summarizes past behavior and provides essential information to downstream components.

In short, the LSTM encoder captures past data, variable selection identifies the most important features, and the LSTM decoder predicts future values, with Add & Norm gates ensuring stability. Together, these components form a strong foundation for time series forecasting.

Up to this point, the architecture may feel similar to a standard LSTM setup. However, this is where things start to diverge.

The next major component is the Gated Residual Network (GRN). The GRN enhances learning by managing the flow of information using gating mechanisms. These gates allow important signals to pass through while filtering out noise.

GRNs also use residual connections, which add the input of a layer back to its output—similar to the Add & Norm gate—helping preserve information and gradients. Finally, normalization ensures consistent output ranges and stabilizes training. By integrating GRNs into the TFT, the model efficiently focuses on the most relevant features while maintaining stability.

Next, we arrive at one of the most important components: the Masked Interpretable Multi-Head Attention (MIMHA) mechanism. This powerful component allows the model to focus on different parts of the input data simultaneously.

The “multi-head” aspect means that multiple attention heads operate in parallel, each focusing on different patterns or dependencies. In the diagram, this is represented by multiple arrows, with each arrow corresponding to a different attention head.

Attention works by assigning weights to different time steps, emphasizing relevant information while downplaying less important signals. This selective focus allows the model to capture complex relationships and long-range dependencies, improving both accuracy and interpretability.

After attention is applied, the output flows through another Add & Norm gate, combining attention-enhanced data with residual connections and normalizing the result to maintain stability.

In some cases, the output of the GRN may bypass the MIMHA layer and go directly to the next Add & Norm gate. This happens when the GRN has already sufficiently refined the important signals. Bypassing attention reduces computational cost, speeds up processing, and helps prevent overfitting by avoiding unnecessary transformations.

The GRN continues refining the information, filtering out noise and preserving essential features. The refined output then moves through subsequent Add & Norm gates, continuing the cycle of processing and stabilization.

At the final stage of the architecture, the processed data reaches a dense layer. This layer applies the final transformation—similar to an activation function—integrating all learned patterns and dependencies to produce predictions.

The output consists of quantile forecasts, which provide a range of possible future values rather than a single point estimate. This is extremely valuable for understanding uncertainty and making risk-aware decisions.

Another critical component is the static covariate encoders. These encode features that do not change over time, such as location or product type. Static covariates are connected to multiple layers in the model because they provide consistent context that influences predictions at various stages.

Within the Temporal Fusion Decoder, information from past inputs, future inputs, and static covariates is synthesized using attention mechanisms and GRNs. The decoder emphasizes the most relevant signals and generates the final predictions while accounting for dependencies and patterns.

The position-wise feed-forward layer applies linear transformations and activation functions independently to each time step, capturing non-linear relationships without altering temporal order.

The temporal self-attention mechanism allows the model to attend to different time steps simultaneously. Multiple attention heads focus on different temporal patterns, helping capture long-range dependencies and improving forecast accuracy.

Finally, static covariates are integrated into the Temporal Fusion Decoder, enriching the model’s understanding by combining stable and dynamic information. This ensures that both constant and time-varying aspects of the data are considered.

The architecture also allows the output from the first Add & Norm gate to bypass the Temporal Fusion Decoder entirely and move directly to the final Add & Norm gate. This happens automatically when the initial processing is already sufficient, reducing computational complexity, speeding up inference, and preventing overfitting. The Darts library handles this internally.

To summarize, the TFT architecture combines variable selection, LSTM encoders and decoders, gated residual networks, attention mechanisms, dense layers, and static covariate encoders to produce accurate, probabilistic forecasts. It synthesizes past data, known future inputs, unknown past inputs, and static context while maintaining stability and interpretability.

And… oof. That was a long one. But I truly hope this deep dive helped clarify how the TFT architecture works and how all the pieces fit together.

Thanks for sticking with it, and I’ll see you in the next video.

**F) Covariates: Past, Future and Static**

In this video, we’re going to dive into the concept of covariates in the context of the Temporal Fusion Transformer (TFT). This is a topic I really want you to have completely nailed down. While understanding the architecture is extremely important, covariates play a huge role in the practical implementation of TFT, especially when we start working in Python. Mastering this concept is essential, so let’s get started.

First, let’s talk about past covariates, which are also known as historical features. These are variables from past time steps that help the model understand the context leading up to the present moment. Past covariates are crucial because they allow the model to capture trends, seasonality, and recurring patterns in the data. Without this historical context, forecasting would be far less accurate.

Next, we have future covariates. These are inputs that can be used to predict future values because they are known ahead of time. Future covariates are especially useful in scenarios where certain conditions are planned or scheduled in advance. Examples include promotions, holidays, planned events, or calendar-based features such as day of the week or month. The key rule here is simple: if you know it in advance, you can use it.

This concept may sound familiar. In earlier models like Prophet or similar traditional approaches, we were already using future covariates. Those models relied on future information to explain past behavior and predict future outcomes. However, what’s new here is that TFT allows us to combine future covariates with past covariates at the same time, which gives the model a much richer understanding of the data.

Another important addition in TFT is static covariates. These are features that do not change over time but still provide essential context for forecasting. Examples include location, product type, customer segment, or any categorical or grouping variable that remains constant. Static covariates help the model differentiate between different entities and understand structural differences across time series.

These static features influence forecasting because they provide broader context. For example, two time series may look similar historically, but if they belong to different regions or product categories, their future behavior could be very different. Incorporating static covariates ensures that the model accounts for these underlying characteristics when making predictions.

To conclude, understanding and effectively using past covariates, future covariates, and static covariates is absolutely crucial for getting the most out of the TFT model. By incorporating all three types, the model can leverage information from multiple angles—historical patterns, known future events, and stable contextual features.

It’s this multi-dimensional perspective that makes TFT such a powerful and flexible forecasting model.

That’s it for this video. If you have any questions, feel free to reach out—I’m here to help. I’ll see you in the next video.

**G) Python - Series, Time and Static Covariates**

In this video, I’m going to walk you through the documentation for the Temporal Fusion Transformer, focusing on a very important component that appears across essentially every model: the encoders. You might have already noticed this section in the documentation, or maybe I gave you a few spoilers earlier, but this part is absolutely key for how we handle time-based features.

What’s happening here is that, up to now, we’ve often been manually creating cyclical covariates—things like hour of day, day of week, or month—by writing custom code. The good news is that Darts already provides built-in encoder utilities that can generate these features for us automatically. This makes our workflow much cleaner and less error-prone.

So what I’m going to do is take this encoder block from the documentation, copy it, and paste it directly into our code. Think of this as an initial setup. From here, we can easily customize and extend it based on our data.

Since our dataset is hourly, it makes complete sense to include all relevant time-based encodings. At a minimum, we definitely want hour, day, and day of week. These capture strong cyclical patterns that are almost always present in hourly time series data. We can also include month, which helps the model learn longer-term seasonal effects.

You might be wondering whether adding even more features—like week of year—is necessary. The answer is: it doesn’t hurt. In practice, adding week-level information can sometimes help the model capture mid-term seasonality, so we’ll include that as well. This encoder setup works just as well for daily data, weekly data, or other frequencies—you simply adjust which encodings you include.

In addition to cyclical encoders, we also keep the positional encodings for past and future relative time steps. These remain unchanged and help the model understand where each time step sits relative to the prediction window.

We also add an encoded year feature. This is a simple but effective transformation where the year is standardized—for example, subtracting 1950 and dividing by 50. This scaling ensures the year feature remains numerically stable and easy for the model to learn from. While encoding the year for future values doesn’t always make a big difference, there’s no downside to including it, so we add it here as well.

All of this together forms a compact but powerful block of code that automatically generates time-based covariates. Once defined, we simply feed these encoders directly into the model, and Darts handles the rest internally.

And that’s really it for this video. We now have a clean, reusable way to generate all the time-based covariates we need, without manually engineering them every time.

**H) Python - Past Covariates**

In this video, we’re focusing again on the Temporal Fusion Transformer (TFT) model. One thing that really stands out when you look at its architecture is that everything is “green”—meaning TFT is designed to accept almost every type of input you can think of. Unlike earlier models we’ve worked with, this is the first one that truly takes everything at once.

What this means in practice is that TFT is extremely flexible. We can train it using only past data, or past and future data together, or even only future covariates if that makes sense for the use case. On top of that, the model supports univariate and multivariate time series, which opens up a huge number of possibilities. Essentially, if something can be done in time series forecasting, TFT can handle it—and I’m going to show you how.

In this specific video, we’re going to isolate past covariates and work only with those. The idea is to take things step by step so we clearly understand what each covariate type does and how to pass it into the model correctly.

We start by working with the dataframe. For now, we simply include all the relevant columns so we can build from there. The dataset contains a unique ID, the target variable y, and two additional features: exogenous_1 and exogenous_2. In this example, we’ll focus only on those two exogenous variables.

Next, we define a variable—let’s call it x_past—which contains exogenous_1 and exogenous_2. These will serve as our past covariates, meaning they are known only up to the current time step and not into the future.

Once that’s done, we create the past covariates time series using TimeSeries.from_dataframe. We pass in the dataframe and specify x_past as the value columns. That’s all we need—nothing fancy here. This single line converts our past features into a proper Darts TimeSeries object.

After creating the past covariates, we quickly verify them to make sure everything looks correct. We check that the series exists, that the values are there, and that the structure is as expected. Once confirmed, we’re good to go.

And that’s it for this video.

In the next one, we’ll move on to future covariates and see how they differ from past covariates and how they plug into the TFT model. I’ll see you there.

**I) Python - Future Covariates**

Up until now, we’ve been working with future covariates. And when we talk about future covariates, it’s important to clarify what that actually means in practice. Even though we call them future covariates, they usually cover both past and future time periods. That’s simply how the model expects them to be structured.

In contrast, past covariates truly belong only to the past. They are used during training to help the model learn patterns, but they are not required during the prediction step. You don’t need to feed them into the model when generating forecasts. Future covariates, on the other hand, must be available for the future time steps you want to predict. This has been the case so far, and it will remain the case in this video as well.

There is one very important note I want to emphasize here. If you include a variable as a future covariate, you do not need to include the same variable as a past covariate. You can choose one or the other. In this walkthrough, I am deliberately including both past and future covariates—not because it’s required, but to demonstrate how both can be wired into the TFT model. From a modeling perspective, including both does not provide any extra benefit here. In most real-world scenarios, you would typically choose future covariates if you have them, and only fall back to past covariates when future information is unavailable. This flexibility is one of the key strengths of TFT compared to many other models.

Now let’s move into the implementation.

We begin by loading the future covariate data. This dataset contains the same structure as before, but extended into the future. Once loaded, we take a quick look at it to confirm that the data is present and correctly formatted.

Next, we subset the data to include only the relevant unique ID. For simplicity, we are still working with a single entity. This keeps the example clean and focused.

At this stage, we isolate the exogenous variables—specifically exogenous_1 and exogenous_2. These are the variables we want to treat as future covariates. We verify that the target variable y is not included here, because future covariates should not contain the target itself.

After that, we concatenate the past and future exogenous data. This ensures we have a continuous sequence of covariates covering both historical and future time periods. When doing this, we need to be careful with the concatenation axis. Initially, things may look off if the wrong axis is used, but once corrected, the combined dataset aligns properly.

Once the concatenation is correct, the final step is to convert this pandas DataFrame into a TimeSeries object. We use TimeSeries.from_dataframe, passing in the combined dataframe and explicitly setting the frequency to hourly, since the frequency hasn’t been defined earlier. This step is crucial because TFT relies on correctly defined time indices.

After this conversion, we now have a properly formatted future covariates TimeSeries, ready to be used by the model.

And that’s it for this video.

In the next one, we’ll move on to scaling, which is an essential preparation step before fitting the TFT model. I’ll see you there.

**J) Python - Scaling**

Now let’s move on to scaling.

This is a very important step, especially when working with deep learning models. Scaling puts everything on a similar numerical scale, which helps the model train faster and more efficiently. Without scaling, the model can struggle with variables that have very different magnitudes, which can slow down convergence or even confuse the learning process. At the end of the day, this step is all about performance and stability.

We start by importing the scalers. I usually like to keep things clean and organized, so I separate them. One scaler is used for the target time series, and another scaler is used for the covariates. This makes it easier to manage transformations and avoids accidental mixing of data.

First, we apply scaling to the target series. We create a scaled version of the series by calling fit_transform on the scaler. This is very straightforward: the scaler learns the distribution of the data and then transforms it accordingly. Once this step is done, our target series is now properly scaled and ready for modeling.

Next, we move on to the covariates. Here, we scale both the past covariates and the future covariates. Just like with the target series, we apply fit_transform so that the scaler learns the appropriate scaling parameters and transforms the data. After this step, all covariates—both historical and future—are now on a consistent scale.

At this point, everything is properly scaled, and we are fully prepared to move forward with the TFT model. This is always a good moment to pause and double-check that all components—series, past covariates, and future covariates—are aligned and scaled correctly.

Before stopping, there’s one more important thing to define: the forecasting horizon. We explicitly set the forecasting horizon to 24. This means we are predicting the next 24 time steps.

Why 24? That’s a very good question. If you look at the future covariates, they span from index 0 to 23. This directly corresponds to 24 hours. After that, the next entity or sequence begins. So, in simple terms, we are forecasting the next 24 hours, which aligns perfectly with the structure of our future covariates.

And with that, we’re done for this video.

In the next one, we’ll finally start building the TFT model itself. I’ll see you there.

**K) TFT Model Parameters**

Welcome to this lecture, where I’ll walk you through the key parameters of the Temporal Fusion Transformer (TFT) model. We’ll focus on the most important parameters, explain what each of them means, and understand how they influence the model’s behavior. Let’s get started.

The first parameter is the input chunk length. This defines the length of the historical data window that the model uses as input. In our example, we set this value to 96, which means the model looks at the past 96 time steps when making predictions. This historical window is crucial because it allows the model to capture patterns and dependencies over a meaningful period of time. Having sufficient historical context helps the model generate more accurate forecasts.

Next, we have the output chunk length. This parameter determines how many future time steps the model predicts in a single forward pass. In our case, it is set to 24, which represents our forecasting horizon. This is one of the most important parameters because it directly defines what we are trying to predict. If your goal is to forecast the next 24 time steps, the model must be explicitly configured to do so. In practice, the model is tuned around this value.

Moving on, we have the hidden size. This refers to the number of hidden units in the LSTM layers of the model. We start with a default value of 16, meaning that each LSTM layer contains 16 hidden units. The hidden size determines the model’s capacity to learn and represent complex patterns in the data. While a larger hidden size can capture more intricate relationships, it also increases computational cost and training time.

The next parameter is the number of LSTM layers. In our setup, we use two LSTM layers. Having multiple layers allows the model to learn hierarchical representations of the data, which can improve its ability to capture long-term dependencies. However, increasing the number of layers also increases model complexity, training time, and the risk of overfitting. This makes the number of LSTM layers an important parameter to tune carefully.

After that, we define the number of attention heads. This refers to the multi-head attention mechanism used in the model. We set this value to four, which means the model can focus on different parts of the time series simultaneously. Multiple attention heads allow the model to attend to various temporal patterns at the same time, improving its understanding of dependencies and relationships within the data.

Next is the dropout rate, which is a regularization technique used to prevent overfitting. During training, dropout randomly disables a fraction of the model’s units. We use a standard value of 0.1, meaning that 10% of the units are dropped during each training iteration. This encourages the model to generalize better by reducing its reliance on specific neurons.

We then specify the batch size, which determines how many samples the model processes before updating its weights. With a batch size of 64, the model processes 64 samples at a time. This setting strikes a balance between training speed and memory usage, enabling efficient training without overwhelming computational resources.

Another important parameter is the number of epochs. This represents the number of complete passes through the entire training dataset. We set this to 10, meaning the model sees the full dataset ten times during training. Increasing the number of epochs can improve performance, but it also increases the risk of overfitting, so this parameter should be chosen carefully.

Next, we have static covariates. Although we won’t use them in this example, we still set this parameter to true. Static covariates allow the model to incorporate features that do not change over time, helping it better understand both the inputs and their broader context. This is a very important aspect of the Temporal Fusion Transformer. In fact, TFT is particularly well-suited for multivariate time series, and static covariates are one of the key reasons for its strength. Keep this in mind, especially for the upcoming challenge.

That’s it for the main parameters. By understanding what each of these settings does, we can later optimize and tune them to ensure the model achieves the best possible accuracy. I’ll see you in the next video.

**L) Python - TFT Model**

Welcome back. In this section, we’ll build our Temporal Fusion Transformer (TFT) model step by step. I have the documentation open alongside us so we can go through the setup together and clearly understand each configuration choice. Let’s get started.

We begin by defining the input chunk length. This parameter represents the number of past time steps that the model uses as input. According to the documentation, it applies to the target series past values and also to future covariates, if the model supports them. While the documentation example uses a value of 168, we will set it to 96. One important recommendation here is that the input chunk length should always be greater than the forecast horizon.

Next, we define the output chunk length. This parameter specifies the number of time steps the model predicts at once per internal chunk. It’s important to note that this is not the same as the forecast horizon, and the documentation explicitly highlights this distinction. The output chunk length controls how many steps the model predicts in a single forward pass, while the forecast horizon determines how far into the future we ultimately want predictions.

Another important point mentioned in the documentation is that if the forecast horizon n is set to be smaller than the output chunk length, it can prevent autoregression. To avoid this, we set our forecast horizon to be the same as the output chunk length. In our case, both are set to 24. An interesting feature here is that even if future covariates do not extend far enough into the future, the model can still generate longer predictions. For example, even if we only have future covariates for 24 steps, the model could still predict 48 steps ahead. This makes the TFT quite flexible. For now, however, we’ll stick to matching the output chunk length with our forecast horizon.

We do not use the output chunk shift, so we leave it unchanged. Moving on to the hidden size, this parameter defines the number of hidden units in the model. A value of 16 works well as a starting point, and we’ll keep it at that.

Next, we configure the number of LSTM layers. TFT can be thought of as an LSTM on steroids, given how much complexity is layered on top of it. We’ll start with two LSTM layers, which is a reasonable default. Along with this, we also define the number of attention heads, which we set to four. This allows the attention mechanism to focus on different parts of the time series simultaneously.

For regularization, we include dropout, which we set to 0.1. This helps reduce overfitting by randomly dropping units during training. Other parameters are left at their default values, as they are not critical to adjust at this stage.

We also set use static covariates to true. While we don’t actually have static covariates in this example, this parameter allows you to enable or disable them easily depending on your dataset. It’s good practice to be aware of this option, especially for more complex, multivariate problems.

Next, we configure the trainer arguments to ensure that we can leverage the GPU during training. If you thought training an LSTM took time, imagine training a model with multiple LSTM layers combined with attention mechanisms. Proper trainer configuration is essential here to keep training feasible.

We also set a random state of 1502, which we always include for reproducibility. This completes our model configuration.

With everything set up, we now fit the model to the data using the fit method, passing in the target series, scaled past covariates, and scaled future covariates. We remove the verbose option and execute the training.

As you can see, the model does not train as quickly as a standard LSTM. This is expected. TFT is a highly complex model, and with that complexity comes longer training times. In practice, I reserve TFT for truly complex problems. Complex models should be used for complex tasks—if your problem is simple, a simpler model will often perform just as well and be far more efficient.

I’ll leave the model running for now, and we’ll return in the next video to focus on cross-validation. I’ll see you there.

**M) Python - Cross-Validation**

Welcome back. In this section, we’ll move on to cross-validation, which is the next step after training our TFT model. This step is crucial because it helps us understand how well the model generalizes over time rather than just fitting a single train–test split.

We start by creating a new cell and defining our cross-validation object. For this, we use the model’s historical_forecasts method. This allows us to generate rolling forecasts over different points in time and evaluate the model’s performance more robustly.

We first make sure that the inputs are correctly specified. The target series remains the same, and both past covariates and future covariates are passed exactly as before. These stay unchanged throughout the cross-validation process.

Next, we define the start point for cross-validation. To determine this, we check the size of the dataset using the dataframe shape, which gives us a total length of 1680 time steps. A common approach is to start cross-validation sufficiently far from the beginning of the series. I usually subtract a multiple of the forecast horizon from the total length. While ten times the forecast horizon is a typical choice, the TFT model takes longer to run, so here we reduce it to five times the forecast horizon to keep execution time manageable.

We then specify the forecast horizon, which is set to 24. This means that at each cross-validation step, the model predicts the next 24 time steps. The stride is also set to the forecast horizon, meaning we generate a new forecast every 24 steps. In practice, this controls how frequently forecasts are made. For example, you could choose a stride of 12 to generate forecasts every 12 hours, but here we keep it aligned with the forecast horizon.

We set retrain to True, which means the model is retrained at each cross-validation step. This is more computationally expensive but provides a more realistic evaluation, especially for time series that evolve over time. We don’t care about verbosity in this case, and we explicitly set last points only to False so that we retain the full set of historical forecasts rather than just the final prediction.

At this point, we review all the parameters—start, forecast horizon, stride, retrain—and everything looks correct. We then execute the cell and let the cross-validation run.

This is a good moment to wrap up the video, as cross-validation with TFT can take some time. Based on previous runs, I would expect this to take around five minutes. For reference, the normal model took about 53 seconds, and since we’re effectively repeating this process multiple times, roughly four minutes sounds reasonable. That said, I’m often wrong with time estimates, so let’s see how it actually turns out.

Come back in the next video, where we’ll review the results.

**N) Python - Cross-Validation Results**

All right, we’re back. Let’s check how long the cross-validation actually took. Was the estimate correct or not? It turns out it took about four minutes, so the estimate was pretty accurate. That means everything ran as expected, and we’re good to move forward.

Now we’re going to do what I usually call the “same dance,” which is a pattern you’ll see often and can very easily automate. The idea is to evaluate each cross-validation fold by extracting predictions and actual values, computing an error metric, and storing the results. To begin, we initialize an empty list that will store our error values. In this case, we create an empty list called rmse_cv.

Next, we iterate over each fold in the cross-validation results. We do this with a loop that goes over the length of the cv object. Inside the loop, the first step is to extract the predictions for the current fold. These predictions are stored as a time series, so we convert them back to their original scale by applying the inverse transformation using the series scaler. Initially, there was an error because the loop index was missing, but once we correctly used for i in range(len(cv)), everything worked as expected.

After retrieving the predictions, we need to extract the corresponding actual values. To do this, we first determine the time window covered by the predictions. We take the minimum and maximum timestamps from the prediction index to define the start date and end date. Using these dates, we slice the original dataframe and retrieve the actual target values (y) for the same time period. This ensures that predictions and actuals are perfectly aligned in time.

Once we have both predictions and actual values, we compute the error metric. Instead of using mean squared error directly, we import and use the root mean squared error (RMSE), which is often easier to interpret because it is on the same scale as the target variable. For each fold, we calculate the RMSE between the actuals and the predictions and append the result to the rmse_cv list.

After iterating through all folds, we compute the overall performance by taking the mean of the RMSE values stored in the list. We then print the final result in a clean and readable format. The final RMSE comes out to around 25.2.

This error feels quite large. When you look at the actual values, which tend to hover around 25, an RMSE of roughly the same magnitude suggests that the model is significantly off in its predictions. This indicates that, while the model is working technically, its predictive performance may not be satisfactory.

To better understand what’s going on, the next step is visualization. In the next video, we’ll plot the predictions against the actual values so we can visually inspect where the model is going wrong and how the forecasts compare to reality. I’ll see you there.

**O) Python - Visualizing the Cross-Validation Results**

Welcome back. Let’s move on to visualization. In this step, we want to visualize the cross-validation folds by plotting the model’s predictions against the actual values. This will help us understand where the model is performing well and where it is struggling.

We start by creating a figure and axes using Matplotlib. A figure size of 10 by 6 works well for this kind of comparison. Once the plotting area is ready, we iterate over each fold in the cross-validation results. For each fold, we retrieve the predictions and the corresponding actual values in exactly the same way we did during error computation. This ensures consistency between evaluation and visualization.

Inside the loop, we plot the actual values first and label them as “Actuals.” Then we plot the predictions on the same axes and label them as “Predictions.” To make the comparison clearer, we apply a dashed line style to the prediction plot. This visual distinction makes it easier to separate predicted values from the true values at a glance. After plotting both lines, we add a legend so the plots are clearly identified.

Next, we set a title for the plot, something like “TFT Model Cross Validation,” to clearly indicate what we are visualizing. Once everything is set up, we render the plot.

When we look at the resulting visualization, we can clearly see the predictions versus the actual values. At first glance, the model appears to be quite off. One of the most noticeable issues is the presence of sharp spikes in the predictions. These spikes look unnatural and are especially concerning because they don’t seem to correspond to similar movements in the actual data.

Outside of these spikes, the predictions don’t look entirely unreasonable. In fact, in periods where the spikes are absent, the model seems to roughly follow the overall trend. However, the spike behavior is problematic and suggests that the model is learning something unstable or incorrect.

It’s also worth noting that this particular time period is especially challenging to predict, as it falls toward the end of the year. End-of-year periods often have unique seasonal effects and irregular patterns, which can make forecasting more difficult. Still, even accounting for that, these extreme prediction spikes should not be present.

At this point, it’s clear that we have some issues with the model. The next logical step is parameter tuning. We need to explore whether adjusting the model’s hyperparameters can reduce these spikes and improve overall performance. That’s exactly what we’ll move on to next. I’ll see you in the following video.

**P) Random vs Complete Parameter Tuning**

Welcome to this lecture on random search for parameter tuning. Let me start by asking you a question.

How many people do you need in a room to have a 50% chance that at least two people share the same birthday? Think about it for a moment. There are 365 days in a year, so intuitively you might expect that you need a very large number of people. Can you take a guess, even roughly? Could you calculate it?

The answer might surprise you. You only need 23 people. With just 23 people in a room, there is already a 50% chance that at least two of them share the same birthday. This is a classic probability problem known as the birthday paradox, and it highlights just how unintuitive randomness and probability can be for humans.

So why start a lecture on parameter tuning with this example? The birthday paradox illustrates the power of randomness and its unexpected efficiency. Just as it takes far fewer people than expected to get a shared birthday, randomness can also be surprisingly effective when searching for good parameter combinations in machine learning models.

When it comes to complete or exhaustive parameter tuning, the idea is to systematically test every possible combination of parameters. While this approach is thorough, it is also extremely computationally expensive and time-consuming. If we look at our current example, we have parameters such as input chunk length and several others that we could tune. The output chunk length is fixed at 24 because that is our forecasting horizon, but even then, the total number of possible combinations comes out to 2,187.

This is massive, especially given how complex the Temporal Fusion Transformer is. If you thought LSTM models were slow to train, TFT models are even slower. Testing thousands of combinations quickly becomes impractical, both in terms of time and computational resources.

This is where random parameter tuning comes in. Random search provides a very efficient alternative to exhaustive search. Instead of testing every possible combination, we randomly sample parameter configurations that are likely to perform well. With far fewer evaluations, we can still arrive at strong model performance.

There is also solid research showing that random search is more than good enough in practice. One important concept here is the balance between exploration and exploitation. Random search explores the hyperparameter space broadly, while still discovering and exploiting combinations that work well.

Another key idea is diminishing returns. Beyond a certain point, evaluating more and more parameter combinations leads to very small performance improvements. Random search tends to identify high-performing configurations early, without needing to exhaustively test everything. At that stage, it often becomes more valuable to focus on improving the input features or the data itself rather than continuing to fine-tune parameters.

In conclusion, random parameter tuning offers a practical and efficient alternative to grid search. Multiple studies have shown that it can match the performance of exhaustive methods while requiring far fewer evaluations. This makes it especially valuable for optimizing complex models like the Temporal Fusion Transformer.

Just like the birthday paradox challenges our intuition about probability, random search challenges the assumption that exhaustive testing is necessary for optimal results. It isn’t. Randomness, when used wisely, is more than enough.

**Q) Python - Parameter Grid**

As discussed earlier, we are now going to use a random sampling approach. From what we have learned, this method works very well in practice. There is a high likelihood that we can find very good parameter configurations simply by sampling randomly, without having to try every possible combination.

At the same time, we don’t really need to test everything. Especially as a starting point, randomly exploring a wider range of options is actually more effective. It allows us to quickly identify promising regions in the parameter space and then refine our search later if needed.

To begin, we define a parameter grid. This is simply a dictionary that contains the parameters we want to tune and the values we want to sample from.

We start with the input chunk length. Here, we’ll try values such as 48 and 96, and we’ll also include 168 since we know that weekly seasonality can be important for hourly data. Next, the output chunk length will remain fixed at 24, because this corresponds to our forecasting horizon and we don’t want to change that.

For the hidden size, let’s double-check our initial configuration. We originally used a hidden size of 16. Since we want to explore in both directions, we’ll include smaller and larger values as well. So we’ll try 8, 16, and 24.

Next are the LSTM layers. We’ll test 1, 2, and 3 layers. This allows us to explore simpler as well as deeper architectures and see how they affect performance.

For the number of attention heads, we originally used 4. We’ll expand this to include 2, 4, and 6 attention heads. This gives the model flexibility to attend to different temporal patterns at varying levels of complexity.

Then we define the dropout rate. We’ll try 0.05, 0.10, and 0.15. This helps us evaluate how much regularization is beneficial for preventing overfitting in this model.

We’re also missing the number of epochs, which is another important parameter. For this, we’ll test 5, 10, and 15 epochs. This gives us a balance between undertraining and overtraining.

With these choices, we now have a complete parameter grid. At this point, the total number of possible combinations is quite large, but we are not going to test them all.

Instead, we define how many combinations we want to try. We’ll set the number of iterations to 5. Since each run takes roughly four minutes, this means the total tuning time will be around 20 minutes. This is a reasonable compromise. If you’re short on time, you can reduce this number. If you’re more curious or have more computational resources, you can increase it.

Next, we generate a list of parameter combinations using a parameter sampler. This comes from scikit-learn. We pass in the parameter grid, the number of iterations, and a random state. Setting the random state to 1502 ensures reproducibility.

Once this is done, we obtain a list of sampled parameter configurations. If you use the same random state and the same number of iterations, you should see similar results.

Now, we define a set of fixed parameters. These are parameters that we do not want to tune, such as encoder settings, the use of covariates, and trainer-related arguments. These stay constant across all experiments.

For each parameter set in our sampled list, we update it with these fixed parameters. This ensures that every model configuration includes the necessary base settings, while still varying the parameters we want to explore.

Finally, we can print or inspect the parameter list to quickly verify that everything looks correct. This completes the setup phase for random parameter tuning.

In the next video, we’ll actually start training models using these parameter configurations and evaluate their performance. I’ll see you then.

**R) Python - Parameter Tuning**

At this point, we know exactly what needs to be done. If this is not your first section working with NAS or parameter tuning with me, then this should feel familiar. You already know the flow and the logic behind it. And if you’re using Google Colab, ideally you’ll have Gemini available. While notebooks are not always perfectly connected, things keep improving over time, especially when working with these kinds of deep learning libraries.

We start by iterating through the parameter list. This time, we loop directly over params. Unlike earlier sections, we deliberately included everything inside the parameter list during our pre-work. Because of that, our loop becomes much cleaner and simpler.

Inside the loop, we pass the scaled series, the scaled past covariates, and the scaled future covariates directly into the model. Since all preprocessing has already been handled, there’s no extra logic required here.

Next, we define the cross-validation step. We use historical forecasts with the forecast horizon set explicitly. The stride is also set to the forecast horizon, retraining is enabled, and last_points_only is set to false so that we retain all predictions across folds.

Once cross-validation is complete, we iterate over each fold. For each fold, we extract the predictions and inverse-transform them using the series scaler. This brings the predictions back to their original scale so that they can be meaningfully compared to the actual values.

We then retrieve the corresponding start and end dates from the prediction index. Using those dates, we slice the original dataframe to extract the true observed values for the same time range.

With predictions and actuals aligned, we compute the root mean squared error for that fold. Each fold’s RMSE is appended to a list so that we can later compute an average error across all folds.

At this point, it’s important to note that we need to initialize the RMSE list before entering the fold loop. This was missing initially, which would cause an error. Once the RMSE list is properly initialized, everything runs as expected.

After iterating through all folds, we compute the average RMSE for the current parameter combination and append it to our results. Optionally, we print the parameter configuration along with its average RMSE using an f-string. This makes it easy to track performance as the tuning progresses.

Finally, we execute the cell. Based on our earlier estimates, this process should take around 20 minutes, though it’s safer to be prepared for closer to 30 minutes depending on how complex some of the sampled models are.

That’s it for this video. Let the tuning run, keep an eye on the runtime, and in the next video we’ll review how long it actually took and analyze the results.

**S) Python - Parameter Tuning Results and Storing**

The full run took 34 minutes, which honestly isn’t bad at all. I wasn’t too far off with the estimate, and that’s always good to know when working with complex models like TFT. This gives us a realistic expectation for future tuning runs.

Now that the tuning is complete, the next step is to attach the RMSE values to the parameter list so that we can properly analyze the results. We start by converting the parameter list into a Pandas DataFrame. Once we display it, we can immediately see all the sampled configurations along with their corresponding RMSE values.

At first glance, the visualization isn’t perfect. The encoders column makes the table a bit messy and hard to read, but that’s fine for now. What really matters is the performance. And the results are impressive. We can clearly see RMSE values around 12, 13, 14, and 15, which is a massive improvement compared to the original RMSE of 25. Even with just five random configurations, we managed to cut the error by more than half, which is absolutely stunning.

Of course, it’s worth emphasizing that five iterations is still very small. If we increased this to ten or twenty random samples, we could likely squeeze out even better performance. But even at this early stage, the gains already demonstrate why random search is such a powerful approach for tuning complex models.

Next, we want to extract and store the best-performing parameters. To do that, we sort the DataFrame by RMSE and identify the configuration with the minimum value. Instead of directly exporting the sorted head, we explicitly filter the row where the RMSE is equal to the minimum RMSE in the DataFrame. This gives us a clean and unambiguous selection of the best parameter set.

Once we isolate the best parameters, we export them as a CSV file. This is useful both for reproducibility and for later reuse without having to rerun the entire tuning process.

Now we move on to preparing these best parameters for building the final model. When we extract them from the DataFrame, we need to be careful with data types. Some values come back as NumPy integers, so we explicitly cast parameters like input_chunk_length, output_chunk_length, hidden_size, num_attention_heads, lstm_layers, and n_epochs to standard Python integers. The dropout value remains a float, which is exactly what we want.

For the encoders, we simply reuse the encoder configuration we defined earlier. These components don’t need to be isolated again because they remain static across models. The same applies to most of the fixed parameters such as trainer arguments and covariate usage settings.

At this point, everything is ready. We now have a clean, well-defined set of optimized parameters that significantly outperform our initial model configuration.

I’ll stop the video here. In the next one, we’ll build the final TFT model using these best parameters and move on to generating future predictions. That’s where all this work really pays off.

**T) Python - Tuned TFT Model**

The issue was just here, with the future covariates. What I should have used was simply the covariates, because I wanted everything included—both the future covariates and the past covariates together. When I used the shortcut earlier, I accidentally included only the future covariates, which is why the problem occurred.

The last thing I wanted to do here was to transform the predictions. First, we convert the predictions time series into a DataFrame. After that, we take the scaler and apply the inverse transform to the predictions so that they are back in the original scale.

So now we have the predictions here, and this would be the final output. At this point, we could also add some visualization if needed, but essentially, this is the output we are looking for.

From here, you simply take this output and feed it into a database, and that’s it for us.

I hope you enjoyed this. I know that I had a lot of fun. If you have any questions, let me know—I’m here to help, and I’ll see you in the next video.

**U) Python - Interpretability**

Okay, let’s move on to interpretability, which is a very cool feature. What I like the most about this is that it’s super easy to use directly from Darts explainability. We start by importing the explainer.

Once that’s done, we build the explainer around the model. So we define the explainer by passing the model into it. After that, we generate the explainability results by calling the explain method on the explainer. That’s it—very easy and very quick.

Now that we have the explainability results, we can visualize them. First, we plot the variable importance by calling the variable selection plot. We pass in the explainability results and set the figure size, initially to something like 10 by 6. After looking at it, we can slightly adjust the size. The width of 10 was fine, but increasing it to 15 makes the visualization clearer and easier to interpret.

When we look at the results, we notice that there are two main components: the encoder and the decoder. The encoder is responsible for taking in and understanding the information, while the decoder uses that information to generate the output. From the plot, we can see that one of the exogenous covariates is the most relevant variable. Along with that, the month feature is also highly relevant for the decoder. Overall, the exogenous variables play the most important role.

From a business perspective, there may not be a huge difference in how you interpret every single variable, but one thing becomes very clear: the exogenous variables really take center stage. They are extremely important for the predictions, and the model clearly acknowledges this. You typically see high relevance both in how the information is understood by the encoder and how the output is produced by the decoder.

Next, we look at one final interpretability feature: plotting the attention mechanism. This is a big part of the Transformer architecture. We use the explainer again and call the method to plot attention, passing in the explainability results and specifying the plot type as time.

In this visualization, we are essentially looking at the average attention head. This may differ slightly from run to run, but it shows where most of the model’s attention is coming from. In this case, most of the attention is coming from future values. You can clearly see that the future appears repeatedly, while only the first part corresponds to the past. This indicates that future values hold a lot of information for the model.

There are different plot types available. If we choose time, it plots the mean attention over all horizons. If we choose all, it plots the attention per horizon, where the maximum horizon corresponds to the output chunk length used during training. We can also use a heatmap, which visually highlights where the attention is strongest—the brightest areas show the most relevant information.

Personally, the time-based plot is the clearest. It makes it very obvious where the attention is coming from, and you can see a clear spike showing where the model finds the most relevant information. This gives a strong indication of what the model is relying on when making predictions.

And that’s it. I hope you enjoyed this—I know that I did. I think this kind of interpretability is super relevant, and I really hope you take this and apply it to your own projects. If you do, please share it with me, because I love hearing your stories. I’ll see you in the next video.

**V) TFT - Pros and Cons**

Now that we’re done with forecasting, let’s evaluate the strengths and weaknesses of the TFT model, starting with the pros.

One of the biggest advantages is its high forecasting accuracy. TFT is a very complete technique that brings together many powerful ideas into a single framework. It is built on the Transformer architecture, which is the foundation behind most modern generative AI and state-of-the-art modeling approaches today.

Another major strength is how well it handles multiple time series. While models like LSTMs can technically do this, TFT is specifically designed for multi-series forecasting and works naturally with static covariates. On top of that, it provides built-in interpretability. Through attention mechanisms and gating layers, it clearly shows which features influence the forecast, making it much more transparent than most deep learning models.

The architecture is also extremely flexible. It supports static, past, and future covariates, allowing you to adapt it to almost any forecasting requirement. Additionally, it is robust to missing data and can handle irregular time series, which is very valuable in real-world business scenarios where data is rarely perfect.

Now, let’s look at the cons.

First, there is the issue of computational complexity. TFT typically requires a GPU and significant processing power. Training times can be long, and for real-time applications where immediate forecasts are required, this model may not be the best option.

Data preparation is another challenge. Preparing the data for TFT can be time-consuming, as you need to carefully isolate and structure static, past, and future covariates. This setup effort is much higher compared to simpler models.

The model is also prone to overfitting, which is a common issue with deep learning approaches. If you are working with a small dataset, TFT may not be the right choice. The architecture includes multiple hidden layers, attention mechanisms, and masks, which can make the model too complex for limited data.

Finally, there is the interpretation complexity. While TFT does provide interpretability tools, understanding and explaining the results can still be difficult for non-experts.

To conclude, TFT is an extremely powerful model and offers an excellent solution for complex forecasting problems. However, you should think of it like using a big missile or a shotgun—it’s best used when the target is large. Hopefully, that metaphor sticks.

Always consider the computational demands, data requirements, and overfitting risks before choosing this model. That said, when used in the right context, it’s a very strong tool and definitely something you can confidently showcase.

**W) TFT Key Takeaways**

Let’s take a moment to look back at everything we’ve covered. We started with an introduction to TFT, understanding how it works and why it is particularly well suited for multiple time series forecasting, which is a key challenge in many real-world problems.

We also explored the electricity pricing use case, which was a great way to see how TFT behaves in practice. While the example itself was interesting, the real value came from breaking down the architecture, where we went into significant depth.

We began with the Variable Selection Network, which is responsible for identifying the most important features for forecasting. Then we looked at the LSTM encoder, which helps the model understand and analyze historical data. After that, we introduced the Temporal Fusion Decoder, which combines all inputs to generate predictions.

We also discussed the attention mechanisms, which allow the model to focus on what truly matters at different points in time. Finally, we covered the Gated Residual Networks, which help information flow through the model efficiently and ensure stable and effective learning.

This architecture is complex, but you went through it step by step, and I strongly encourage you to revisit the lecture on the TFT architecture. It is probably one of the longest theoretical lectures I’ve done, but it’s worth the effort. And if you have questions, I’m always here to help.

We then moved into the hands-on portion. We got our hands dirty by learning how to prepare the data, with a strong focus on covariates, the importance of scaling, and how to configure the model with the right parameters. We also implemented cross-validation, which is a crucial step for reliable model evaluation.

On top of that, we worked with visualizations, which were especially helpful in understanding how the model’s predictions compare to the actual values. Seeing forecasts alongside real data always adds clarity and confidence.

We also discussed the pros and cons of TFT, which is an important step in understanding when and when not to use such a powerful model. TFT has come a long way, and this has been a long journey for us as well.

Whether it’s electricity pricing or any other problem you want to tackle, the key idea remains the same: if it moves, we can predict it—we just need to choose the right forecasting model.

**XVIII) Section 18: CAPSTONE PROJECT: Multiple Series with TFT**

**A) Project Presentation: Multiple Series with TFT**

In this video, I’m going to walk you through what the challenge is and what you need to do. Inside the capstone project folder, you’ll find multiple files. One of the most important ones is the project briefing, which you should review carefully. This document provides the project overview and clearly outlines the key objectives.

The core of this challenge revolves around the electricity dataset, which we have already explored earlier. Your goal here is to select two different time series from the dataset. This is important because the focus of the challenge is on multiple time series forecasting. If you can successfully work with two time series, you can scale the same process to ten or more—the workflow remains essentially the same.

There are two different ways you can approach this challenge.

The first approach is to start completely from scratch. You create your own file and build everything yourself step by step. This is a perfectly valid option, and I would strongly recommend it if you’re still getting familiar with the Darts library or if you have open questions. Starting from scratch gives you more practice and a deeper understanding, and it also gives you more opportunities to ask questions along the way.

The second approach is to use the starter file provided in the folder. This is the approach I’ll be using. The starter file already contains everything that is basic or that we’ve covered so far. All the critical components related to TFT and this specific dataset are already in place.

What makes this challenge different is not the fundamentals—we’ve already gone through those—but rather how you apply TFT to multiple time series. That’s the real focus here. You already know the tools; now the challenge is about putting them together in a multi-series forecasting setup.

That’s it for this challenge overview. I wish you the best of luck, and I’ll see you in the next video.

**B) Python Solutions - TFT Model**

From my perspective, this is more of a Python challenge than a pure time series challenge. The real work here is about handling the dataset properly and working with the right functions. Once the data is structured correctly, the time series modeling itself becomes much more manageable.

To get started, I already have the data and the file connected to the drive. I’ll first make sure everything is visible and properly set up. The next step is to install the Darts library. All the libraries we use will remain the same—there’s nothing new to add here.

Once everything is installed, we inspect the dataset and check how many time series we have. I also left a piece of code that visualizes all time series at once, which helps us understand how the data is structured. I’ll let everything run first, and once it’s done, we’ll look at the output.

After running the code, we can clearly see that there are two time series at the beginning, one in the middle, and two at the end. This gives us multiple options: we could predict the first two, the last two, or even train a model on one group and predict the others. That would be perfectly reasonable. However, that’s not the goal here. The goal is to perform multiple time series forecasting, meaning we want to train the model on more than one series at the same time.

For this challenge, I decided to work with Belgium and France. We isolate these two countries from the dataset and inspect the dataframe to understand its structure. Up to this point, everything we’ve done should feel familiar.

Now we reach the part where the actual coding starts. Since we’re working with multiple time series, we need to handle categorical identifiers properly. If we look at the unique_id column, we’ll see values like Belgium and France. These need to be converted into categorical codes so that the model can process them.

We do this by converting the unique_id column to a categorical type and then using .cat.codes to transform the categories into numerical values (0 and 1). We store this transformation back into the dataframe. This is our first key step: converting the unique_id into a categorical variable.

The second step is handling the index. Currently, the time column is set as the index, but for Darts we need it as a regular column. We use reset_index() to convert it back into a column and then preview the dataframe to confirm everything looks correct.

Next, we define the variables we’ll use throughout the notebook. To keep things clear, I use uppercase variable names. We define:

TIME_COL for the time column

TARGET (Y) for the target variable

GROUP_COL for the static variable (unique_id)

FREQ set to hourly ("H"), since the data is hourly

With these variables defined, we can now create the TimeSeries objects. Since we’re working with multiple series, we use TimeSeries.from_group_dataframe. We pass in the dataframe, the time column, the group column, the target column, and the frequency. This creates a list of time series—one for each country.

When we preview the result, we see that the output is now a list rather than a single TimeSeries object. This is expected and is one of the main differences when working with multiple series. It’s not as neatly packaged as before, but it’s fully usable.

Next, we move on to past covariates. This part is straightforward because we already know how it works. In this dataset, we have two exogenous variables: exogenous_one and exogenous_two. We isolate these variables and again use TimeSeries.from_group_dataframe to create past covariates for each series.

At this point, we have:

A list of target time series

A list of past covariate time series

Both are structured in the same way, which is important.

Now we move to the future dataframe. We isolate the future data for France and Belgium and preview it to ensure everything looks correct. Just like before, we convert the unique_id column into categorical codes. This step is critical because static covariates in TFT must be numeric.

We then prepare a combined dataframe (df_full) by concatenating the historical dataframe (with the target column removed) and the future dataframe. This ensures both dataframes have the same structure, which is required for future covariates.

Using this combined dataframe, we create the future covariates TimeSeries. Again, we use from_group_dataframe with the same parameters, passing in the covariate columns instead of the target.

It’s important to note here that including the same exogenous variables as both past and future covariates is technically redundant. However, the goal here is to demonstrate how both past and future covariates are handled. In real projects, you should only include what actually makes sense for your data.

Next comes scaling. This is something we’ve done many times, so it should feel familiar. We initialize a scaler and fit-transform:

The target series

The past covariates

The future covariates

During this step, we encounter an error related to mismatched time series during scaling. This requires a bit of debugging. After reinitializing the scaler and ensuring we use fit_transform consistently across all series, the issue is resolved.

We then move on to the TFT model setup. Nothing fundamentally changes here compared to previous notebooks. The only thing that usually changes is the forecasting horizon, which we define explicitly.

However, we hit another error:

TFTModel can only interpret numeric static covariate data.

This confirms that the issue is related to static covariates. After inspecting the dataframe again, we realize that the unique_id column was still being treated as an object in some places. Once we reconvert it properly to categorical codes and rerun the necessary steps, the issue is resolved.

At this point, everything is working as expected. We have:

Multiple time series

Past and future covariates

Proper scaling

A TFT model ready to train

This video has already gone on for quite a while, so I’ll stop here. The debugging process is part of the learning experience, and overall this hasn’t been particularly difficult—it just requires careful attention to detail.

**C) Python Solutions - Cross-Validation**

At this point, the modeling step is already complete. The training was actually very fast—it took around 46 seconds, which is great. We still have the trained model available, so now we can move on to the next and very important step: cross-validation.

Let’s go through this step by step.

First, we define the inputs for cross-validation. We use the transformed target series (series_y_transformed), the transformed past covariates (past_covariates_transformed), and the transformed future covariates (future_covariates_transformed). These are the scaled versions of the data, which is exactly what the model expects at this stage.

Next, we define the cross-validation setup. For the starting point, I’m going to use 24, meaning we start the first validation window 24 time steps into the data. I also update the forecasting horizon here. Instead of predicting far into the future, I decide to go 10 forecasting horizons back in time and predict a horizon of the same length.

For the stride, I also set it to the forecasting horizon. Optionally, in a real-world setting, you could divide this value by two to get a more thorough evaluation with overlapping windows. That said, this is optional and depends on how exhaustive you want the validation to be.

I then set retrain = True, which means the model will be retrained at each cross-validation step. This is usually a good practice when you want a realistic estimate of performance. I also set last_points_only = False, so we keep all forecasted points rather than only the last one from each window.

After setting all of this up, I run the cell. At first, I get an invalid syntax error, which turns out to be caused by a comma placed inside a comment. Once I remove that and rerun the cell, everything starts running correctly.

This cross-validation step is expected to take some time—roughly around 10 minutes, depending on the machine and configuration. Because of that, I’m going to stop this video here.

In the next video, once the cross-validation is complete, we’ll take a closer look at the cross-validation output, analyze the results, and interpret what they tell us about the model’s performance.

**D) Python Solutions - Parameter Tuning**

Welcome back. In this video, we’re going to walk through how to get our cross-validation results, and then we’ll continue all the way through to parameter tuning. Let’s get started, because this is probably the most challenging part of the entire process.

First, let’s take a look at our cross-validation object just to understand what it looks like. When we inspect it, we see that it’s essentially a list, and this is what we need to explore. If we print the length of the CV object, we see that the length is two. Now, why do we have two? The reason is simple: we have two time series. Because of that, if we access CV[0], something will show up, but if we try CV[2], we’ll get an error because it’s out of range. The valid indices are only 0 and 1, one for each unique series.

Next, we look at how many backtesting periods we have. If we print the length of CV[0], we see that we have ten backtesting periods. That means indices 0 through 9 will work, but trying to access index 10 will again throw an out-of-range error. If we go one level deeper and print the length of something like CV[0][0], we get 24, which corresponds to the forecasting horizon. So in total, we have two series, ten backtesting periods, and 24 forecasted values per period. This is our starting point.

To make things clearer and easier to work with, we define a couple of helper variables. We define the number of backtests as the length of CV[0], and the number of series as the length of CV. This makes the logic much easier to follow and visualize. Now we’re ready to start looping through the backtesting periods.

We begin by looping through each backtesting period. For each backtesting period, we also loop through each series. Before doing that, we initialize an empty list called period_predictions. This list will be overwritten for every backtesting period, and that’s perfectly fine because we compute the RMSE fresh each time. Inside the nested loop, we append the predictions using the structure CV[i][j], where i represents the series index and j represents the backtesting period.

Once we’ve collected the predictions for a given period, we inverse-scale them using scaler1.inverse_transform. This gives us the predictions back in their original scale. At this point, we want to calculate the RMSE for each series within the current backtesting period. To do that, we initialize another empty list, usually called something like series_rmse.

We then loop through the number of series again. For each series, we convert the predictions into a DataFrame, because working with DataFrames makes it much easier to align predictions with actual values. We determine the start and end timestamps from the prediction index. Using these timestamps, we extract the corresponding actual values from the original dataset by filtering on the time range and the unique ID of the series.

After extracting the actual values, we make sure the indices line up correctly. Optionally, we can check that the prediction index matches the actual index, which is a good sanity check. Once everything is aligned, we compute the RMSE using the square root of the mean squared error between the actual values and the predicted values. We append this RMSE to the series_rmse list.

After looping through all series for the current backtesting period, we compute the mean RMSE for that period and append it to a list that stores the RMSE values across all backtesting periods. At the very end, we compute and print the overall RMSE across all backtests. In our case, we end up with an RMSE of around 34. If you recall, we did better in an earlier attempt, but this is a different problem with different time series, so that comparison isn’t entirely fair. Still, this gives us a solid baseline.

This is exactly why we now move on to parameter tuning and benchmarking. The current performance leaves room for improvement, so we try to do better. We use a parameter sampler along with a parameter grid and decide how many parameter combinations we want to try. In this case, we choose ten combinations. This will take longer because we’re working with multiple time series, which increases the overall complexity.

We then add our fixed parameters, such as using static covariates, and combine them with the sampled parameters to form a parameter list. Next, we initialize a list to store the results. We define the loop that goes through each parameter set, builds the model with those parameters, and runs cross-validation. For the cross-validation logic itself, we reuse the code we just wrote, since it’s already quite lengthy and detailed.

For each parameter combination, instead of printing intermediate results, we compute the mean RMSE and append it to our results list along with the corresponding parameter set. We then print something like “Tested params … lead to RMSE …” so we can track progress as the tuning runs.

At this point, we kick everything off. Assuming nothing has gone horribly wrong, the process will continue running. This step can take a long time—around two hours in our case—because of the repeated cross-validation across multiple series and parameter combinations. Once it’s done, we’ll be ready to analyze the results and see which parameter set performed best.

That’s where we’ll pick things up in the next video, where we review the outcomes and wrap up this entire challenge. See you then.

**E) Python Solutions - Predict The Future**

Alright, so we are almost at the end. I have to say, this was extremely painful. It took about three hours and fifteen minutes in total. During that time, I prepared lunch, had lunch, talked with my family, and honestly got a bit frustrated with how long everything was taking. But in the end, it’s finally done—three hours and fifteen minutes later.

Now let’s look at the outcome. The next step is to check the results, and then we are going to move on to exporting and predicting the future. The code itself is the same as before, and this part should feel very familiar to you by now. Let me jump into it and set this up as a dynamic table. There we go. I’ll zoom out a bit because I can’t see the MSI properly.

Okay, it’s there, but something looks a bit odd. I’m using the ParamList tool here, so let me triple-check what’s going on. I have ParamList2, I have the fixed parameters, and then I add those parameters to the ParamList. This setup should work. Let me run this again, because I’m not sure what happened. I mean, I know the list itself won’t change since we have a random state set, so we should always get the same results. Maybe I double-clicked something—that’s usually the most likely culprit.

Alright, here we go. We have, I think, around 34 or 35 runs. And now, yes, we see a very significant improvement. The score is around 17, which means we really improved what we were doing by using a much more complex model. We’re looking at 20 epochs, six attention heads, four LSTM layers, and a dropout of 0.3. So clearly, the more complex models are giving us better results. That’s actually very curious and very insightful.

Next, we export the parameters. This step is important because otherwise we would need to spend another four hours retraining everything. After that, we move on to the part where we predict the future. We import the best parameters, convert what needs to be converted into integers, and then we build the model again using those parameters. All of this eventually leads us to forecasting the future, which is what we’re already doing at this point.

Once that’s done, we need to unpack the predictions. As always, I want to take a look at the predictions to see what we actually got. Let me check the current status. Okay, let me pause for a few seconds—yes, we’re still at epoch number five. Lucky number five.

Alright, it’s done now. It took around two and a half minutes to run. The predictions themselves were very quick, but with double the number of epochs and a more complex model—especially with more attention layers—things naturally get a bit trickier and slower.

Now we need to inverse the transformation. So we use scaler1.inverse_transform to inverse-transform the predictions. That’s step number one. Let me assign this to unscaled_predictions. Next, we convert each time series into a data frame. So we create a list of data frames using something like pd.DataFrame(pts) for each item in unscaled_predictions.

Let’s take a quick preview of these data frames to see what we got. There you go. Now, this next part is optional. Optionally, you can concatenate all of these together. To do that, we can create something like forecast_data_frame and use pandas.concat(data_frames, axis=0). Then we can inspect the final forecast data frame.

Finally, we export everything to a CSV file. So we save it as something like forecasts_multi_TFT.csv. And that’s it—we’re done.

The very last thing I want to mention is about the format of this video. This was a new format for me. Usually, I do most of the coding together with you step by step, but I felt that this might become repetitive. So I tried something different this time. Let me know what you think. Was it good? Was it bad? Did you like the idea, but feel that the execution was so-so?

I’m here to improve as well, just like you. I’ll see you in the next video.
