# Master-Time-Series-Analysis-and-Forecasting-with-Python-2026-Notes
This Repository contains my "Master Time Series Analysis and Forecasting with Python 2026" Course Notes from Udemy

**I) Time Series Analysis and Forecasting with Python**

**A) Time Series Analysis and Forecasting with Python**

**B) Course Introduction**

**C) Overview of the AI Time Series Assistant**

**D) Diogo's Introduction and Background**

**E) Unlimited Updates and Enhancements 2026**

**II) Section 2: Part 1 - Time Series Analysis**

**A) Time Series Analysis Overview**

**III) Section 3: Python for Time Series Analysis**

**A) Game Plan for Python for Time Series Analysis**

**B) **Load and Explore Data****

**C) Subsetting Stores and Basic Aggregations**

**D) Working with Datetime Index and Weekday Patterns**

**E) Standardizing Sales and Comparing Weekday Patters**

**F) Analyzing Sales on a Specific Weekday**

**G) Removing Outliers and Re-Assessing Intra-Week Seasonality**

**H) Analyzing the Impact of Promotions**

**IV) Section 4: Introduction to Time Series Forecasting**

**A) Game Plan for Introduction to Time Series Forecasting**

**B) What is Time Series Data?**

**C) Python - Libraries and Data**

**D) Python - Time Series Index**

**E) Python - Exploratory Data Analysis Part 1**

**F) Python - Exploratory Data Analysis Part 2**

**G) Python - Data Visualization**

**H) Python - Data Manipulation Part 1**

**I) Python - Data Manipulation Part 2**

**J) Seasonal Decomposition**

**K) Python - Seasonal Plots**

**L) Python - Seasonal Decomposition**

**M) Auto-Correlation**

**N) Python - Auto-correlation**

**O) Partial Auto-Correlation**

**P) Python - Partial Auto-Correlation**

**Q) Python - Building a Useful Function Script**

**R) Can you predict stock prices?**

**S) What did we learn in this section?**

**T) CASE STUDY: Forecasting Gone Wrong**

**V) Section 5: Time Series Analysis Practice**

**A) Data Loading and Index**

**B) Data Visualization for Time Series**

**C) Exploratory Data Analysis for Time Series**

**VI) Section 6: Exponential Smoothing & Holt-Winters**

**A) Game Plan For Exponential Smoothing and Holt-Winters**

**B) CASE STUDY BRIEFING: Customer Complaints**

**C) Python - Exponential Smoothing Set Up**

**D) Python - Exploratory Data Analysis**

**E) Training and Test Set in Time Series**

**F) Python - Training and Test Set**

**G) Simple Exponential Smoothing**

**H) Python - Simple Exponential Smoothing**

**I) Double Exponential Smoothing**

**J) Python - Double Exponential Smoothing**

**K) Triple Exponential Smoothing aka Holt-Winters**

**L) Python - Triple Exponential Smoothing aka Holt-Winters**

**M) Measuring Errors for Time Series Forecasting**

**N) Python - MAE, RMSE, MAPE**

**O) Python - Predicting The Future**

**P) Python - Daily Data**

**Q) Python - Working on the Useful Code Script**

**R) Holt-Winter Pros and Cons**




# **I) Time Series Analysis and Forecasting with Python**

# **A) Time Series Analysis and Forecasting with Python**

You ever feel like you're the keto vanilla ice cream of the business world?
Boring and disappointing that you're still doing forecasting the old way like a rolled ice cream when everyone now talks about a -- pistachio flavor?

Well, not anymore.

In this course, you'll be at the forefront of time series forecasting from time series analysis and diving deep into advanced forecasting models.
I'll guide you through every step.

You will cover everything from simple exponential smoothing to cutting edge models like LinkedIn, Silver Kite, Prophet, and Amazon Cronos.

Think you're just a small fish in a big pond?

Let's change that with hands on exercise, real world challenges, and capstone projects.
You won't just learn, you'll do.

Revenues.
Electricity.
People.
Temperatures.

If it moves, we predict.

By the end you'll be forecasting trends with the precision of a sniper, turning your yearly revenue into your monthly income.

And who's going to lead you through this journey?

Well.

Hi.

I'm Diogo, your guide, mentor, and now ice cream coach with years of experience in data science and a passion for turning complex concepts into actionable insights.

I practice what I preach.

Jump in and let's make this year your breakthrough.

This is your moment.

Make it count.

Enroll now and let's crush it together.

# **B) Course Introduction**

I'm very happy that you picked this course to learn time series analysis.
It's the fifth year that this course has been live, and I truly think it's the most complete out there.

And there's absolutely no video that were in the, let’s say, first two editions that are currently live now.
So it’s currently always, always updating.

What I want to show you throughout this video is every single change since the 2025 version to now, the 2026 version.
This walkthrough will help you understand exactly what has improved and why it matters.

The key changes start with much crisper videos.
I really try to make sure that all videos are shorter and more focused.

We also now have deeper explanations and better explanations overall.
The goal is not just speed, but clarity.

Coding is now done with AI in, I think, most of the videos by now.
This allows us to spend more time looking at documentation and understanding what is what and why we are doing it, without the hassle of writing all the code manually.

Of course, we are very critical about it.
In the end, the code used is the one that I like, not necessarily the one that, in our case, Jamie and I are told to use.

We also add new projects and topics based on popular demand.
So the bottom line is, if you want something, tell me.

If there are a few people that want the same thing, I’ll make it happen.
And that’s a promise.

I also introduced an AI assistant.
You now have a time series analysis assistant that you can take anywhere with you and use in your job or during interviews.

This is a print screen of the assistant.
All the materials of the course are there—honestly, everything is there.

It’s just about asking the right questions.
And the assistant responds, I think, in my tone.

So if you like the way I talk or the way I use words, you’re going to like this as well.

Now, let’s look at what the course was in 2025.
At that time, we had five parts.

These were time series analysis, modern time series models and forecasting, deep learning for time series, and advanced content for time series.
We also had a Python appendix.

The Python appendix was for those who were not very familiar with Python but still wanted to learn.
There was a crash course at the end.

These were all the sections that were available back then.
I don’t want to spend too much time listing them, but they’re all there.

The question then becomes: what did we change?
This is where the remakes come in.

One important highlight is a new part called the Time Series Graveyard.
These are techniques that existed before but have not been updated by the teams maintaining them.

That’s why I’m calling it the graveyard.
This section also exists because of your requests.

In the past, I removed some sections.
Some people later asked if they could still access them.

So now, instead of removing them completely, they live in the Time Series Graveyard.
I wouldn’t advise you to focus on them, but it’s your choice.

This section includes everything that was either remade or added.
Anything marked as new represents a true addition.

We added new practice exercises for time series analysis.
We also added new intermittent forecasting and classification for time series.

For sections that are not marked as new, their Python tutorials were fully remade.
This was necessary because new changes in the libraries required updates.

As a result, the course is now 100% up to date.
Nothing is outdated or obsolete.

This is how the 2026 version of the course is now organized.
It consists of six parts.

These are time series analysis, modern time series, deep learning, advanced content, the graveyard, and the Python appendix.

The course is also now split between sections and projects or exercises.
Depending on your goals, you can choose how deep you want to go.

If you want structured learning, you can focus on the sections.
If you want to build a portfolio, you’ll find plenty of projects and exercises.

One important thing to note is that the course is actually shorter than it was one year ago.
This is despite having a lot of new content.

The reason is simple: the videos are crisper and better edited.
Using AI for coding also makes learning much faster.

This creates a smoother and more efficient learning experience.
That’s something I’m genuinely excited about.

The 2026 version is live.
So let’s get started.

# **C) Overview of the AI Time Series Assistant**

In the previous lecture, I gave you a link to the AI time series assistant, and in this video I want to give you a brief introduction to it. I’m going to call this version one of the assistant. I focused a lot on getting the core functionalities and the architecture right so that I can build on it later. There are quite a few things that could still be improved, and that’s exactly what I’m looking for when you share your feedback.

In this video, I want to introduce what it can do, what it cannot do, and the other actions available. It’s always going to be a time series course assistant, and it has a collapsible bar where you can see its capabilities, the core topics it covers, and also what it cannot do. For example, it cannot give medical advice, it cannot run code, and it cannot browse the web. You’ll also see a “ready to predict the future” message and some initial information explaining what it can and cannot do.

Let’s say we enter a message, for instance, “explain time series analysis,” and then click on send. You’ll notice that it shows when it’s thinking, both in the chat itself and at the top, where it indicates that it’s running. This assistant is going to complement the AI assistant from Udemy. There are a few things they have in common, especially when it comes to the videos themselves, because both have access to the transcripts. In fact, everything from the Udemy AI assistant comes from those transcripts, so if you want something specific to a given lecture, that’s where it’s useful.

For example, if you want a summary of a lecture, the Udemy assistant is very good because everything is directly linked to your current video. However, this time series assistant is not linked to your current video experience, so that’s one area where Udemy’s assistant is better. On the other hand, this assistant has some clear advantages. First, it’s always going to be up to date. For instance, we’re currently using GPT-5 already, whereas the Udemy assistant, based on things like its use of words such as “delve,” still feels very much like GPT-4.

At the same time, this assistant has all the course material loaded into it. That includes all the Python code, all the PDFs, and everything else from the course. So if you want something really tailored to the course—especially things you want to use, reuse, and take into your professional life—this is the one to use. It’s also not really tied to Udemy, which means you can keep it open and ask questions while you’re working on your own projects.

This tool is meant for you to use and reuse. Of course, there are many things that can still be improved. There’s an option to give feedback, and while I’d obviously love it if you love it, what I really care about is that you share what you like and what you would improve. This could be things like the colors, the layout, or any other aspect. Again, this is version one.

If you’ve been with the course for a long time—especially since the course originally dates back to 2021—you know that I’m always improving it. There will be a version two, version three, and so on, because I’m very focused on making this the best course possible.

While you’re here, you can also ask things like, “Give me the code for exponential smoothing,” press control-enter, and the assistant will think and respond. Alongside this, I want to emphasize that the feedback section is really important. Sometimes people ask for personal help through the feedback, but I can’t help you that way. If you need my help directly, you should always use the Q&A section. I answer questions there almost every day—there’s maybe one day a month when I don’t check it.

The Q&A is always the best way to reach out to me. I do gather feedback from this tool, but I don’t check feedback every day. I usually review it weekly or monthly, make sure everything is working, and then act on it after a couple of months to improve the tool. One example of feedback is that the Udemy assistant allows you to add images, which is really cool, whereas this one currently cannot. That’s something to keep in mind for future improvements.

You’ll also find all the code used in the course here. This is the same code we use throughout the course, and it should always be up to date because it reflects the latest course materials. If you ever find any issues, please let me know, because sometimes it can produce hallucinations, which I can’t fully control.

If at any point you don’t want to continue a conversation and you want to start fresh, you can reset the conversation. That will start a new session. Everything here is session-based only—there’s no database in the backend, and nothing is stored.

If this is something you love and use a lot, there’s an option to donate. This is completely optional, but I want to be transparent that this tool does cost money to run. The cost for one to ten people is basically nothing, but when you get to thousands of users, it becomes more significant. So if it genuinely makes a difference in your life, you can support it, but again, it’s absolutely optional.

At the end of the day, this is yours. Even if you hate me, you can bookmark it. There’s no login, and it’s an open tool that you can use and reuse. It’s here for you to keep. I personally find it very cool when people use my work, so if you do use it, let me know.

# **D) Diogo's Introduction and Background**

Hi there, I’m Diogo, and I’m thrilled to welcome you to this course. Before we get started, let me share a bit about myself so you know who’s guiding you through this journey.

I hold a Master of Science in Management from ESMT Berlin, one of the top business schools in Europe, where I specialized in business and analytics. My professional career has been centered around using data to solve complex business challenges. I’ve worked on projects ranging from sales planning involving billions of euros to A/B tests where companies had to invest hundreds of thousands of euros. I’ve truly been around the block and have had a lot of hands-on experience.

Beyond teaching, I’m also a startup founder. My company, which you can find at Join Betacom, aims to leverage the power of data to help restaurants around the world. We analyze vast amounts of data to provide actionable insights, such as optimizing menus or determining the best pricing strategies for their products. And if you’re interested in my startup, feel free to reach out and drop me a line.

I’m here not just to teach, but to genuinely support your learning journey. If at any point you feel that this course isn’t the right fit for you, don’t hesitate to reach out. I’m more than happy to guide you toward a learning path that suits you best, even if that means recommending other resources or courses that better align with your learning style.

Before we wrap up, I’d like to extend a personal invitation for you to connect with me on LinkedIn. Simply search for Diogo Alves de Rezende, and let’s keep the conversation going. I’m always eager to engage with my students and grow our professional networks together.

Once you complete this course and earn your certificate, I encourage you to share it on LinkedIn. I make it a point to repost these achievements and celebrate your success, just as I’ve done for many of my students. It’s a great way to gain visibility and start building your professional reputation in the business and analytics community.

My goal is for you to succeed and feel empowered to make a real impact. Whether you’re here to advance your career, pivot into a new industry, or innovate within your own business or company, I’m excited to help you achieve those dreams.

# **E) Unlimited Updates and Enhancements 2026**

Hey, before we start, let me talk about the current status of this course and what’s coming next.

All the content you’re about to dive into has been pre-checked and updated to ensure it’s relevant and accurate for 2026. I know how fast technology evolves, and that’s why I’m committed to keeping this course up to date with the latest developments and insights, so you can stay ahead of the curve.

That said, in a world that’s constantly changing, there’s always a chance that something might slip through the cracks. If you notice any outdated information or anything that doesn’t feel quite right, please don’t hesitate to let me know. Your input is invaluable in maintaining the quality and relevance of this course.

In addition, if there’s any specific content or material that you’d like to see added, I’d love to hear from you. Your suggestions help shape the future of this course.

In the next lecture, I’ll share a form where you can report anything that’s incorrect, offer suggestions, or request additional resources. This course is a collaborative journey, and your feedback plays a key role in making it the best it can be.

Thank you for being an active part of this learning community. I’ll see you in the next video.

# **II) Section 2: Part 1 - Time Series Analysis**

# **A) Time Series Analysis Overview**

Welcome to the time series analysis part of the course. This is where you take your data skills to the next level, learning how to make your data work for you and predict future trends with accuracy. Let’s get started.

Have you ever wondered how companies like Amazon and Netflix always seem to know what’s coming next? It all comes down to mastering time series forecasting. By the end of this section, you’ll be applying the same techniques to your own business and career, turning raw data into accurate, actionable predictions.

We’ll begin by understanding what time series data is and why it plays such a critical role in forecasting. You’ll work with real-world data in Python and set up all the essential tools needed for time series analysis. This foundation will allow you to build robust models and generate precise predictions.

Before forecasting effectively, it’s essential to know your data inside out. You’ll perform exploratory data analysis to uncover hidden patterns and structures in your time series. Through visualization and data manipulation techniques, you’ll learn how to interpret your data and prepare it properly for forecasting.

You may notice that certain trends repeat over time—this is known as seasonality. You’ll learn how to decompose your data into seasonal, trend, and residual components. This breakdown will help you model your data more accurately and significantly improve your predictions.

Understanding how past data points influence future values is another key concept. You’ll explore autocorrelation and partial autocorrelation to measure these relationships. This knowledge is essential for selecting, tuning, and validating your forecasting models.

Next, we’ll dive into a range of forecasting techniques. You’ll start with simple exponential smoothing, which works well for short-term predictions. From there, you’ll move on to the Holt-Winters method to handle seasonality. You’ll also work with more advanced models such as ARIMA and SARIMAX to capture complex patterns in time series data.

Imagine being able to predict stock prices or customer demand with confidence. You’ll develop these skills through hands-on exercises and real-world challenges. This section is not just about theory—it’s about applying these techniques to real data and understanding their results.

You’ll also explore what happens when forecasting goes wrong. By studying real case studies where predictions missed the mark, you’ll learn to recognize common pitfalls and improve your forecasting approach.

Finally, you’ll bring everything together in a capstone project focused on forecasting airline miles. You’ll prepare the data, build forecasting models, and evaluate their performance. By the end of this project, you’ll have a strong, practical example of your forecasting skills to showcase professionally.

You’re not just learning how to forecast—you’re mastering a powerful skill that can transform your career. See you in the next video.

# **III) Section 3: Python for Time Series Analysis**

# **A) Game Plan for Python for Time Series Analysis**

In this section, you are going to use Python as a tool to understand time series. This is not really an academic exercise. We’re going to work with real data, real questions, and we’re going to aim for real decisions.

We’re going to move fast, stay focused, and build intuition step by step. The goal is to truly understand time series, not just in theory, but in practice.

I can tell you already that you don’t need to be a master of Python. You only need enough to slice data, spot patterns, test ideas, and eventually explain what’s actually going on in the business. That’s really the core objective here.

If you already feel comfortable with Python, that’s great—you’ll be sharpening your instincts. If, however, you’re rusty or starting fresh, don’t worry. There’s a full Python crash course at the end of this course where you can practice more or even start from zero at your own pace.

# **B) **Load and Explore Data****

You are going to find this inside Python for Time Series, which itself is inside Time Series Analysis, and then Python for Time Series Analysis. There, you will find a file called lab starter one.

And here we go.

So we are here. I need to reconnect. This is basically taking our script and saying, “Hey, this is live.” Then I would mount the drive here. Of course, if you’re not using Google Colab, you don’t need to do this step. This is just for those who are using it. Whichever workspace you’re using, just go for it.

In case you’re using Colab, let me show you. You go to Drive → My Drive. This is so that you get the path. Inside My Drive, you’ll find Python Time Series Forecasting. Then inside Time Series Analysis, you will find Python for Time Series Analysis. I will copy the path here. Here we go. Control. Of course, nothing changes, and here we go.

So this is the part where I do this with you. Of course, it’s very specific for Colab. And now we can go.

Task number one is to import the pandas library. All right, let’s do this. Import the pandas library. It’s very simple:
import pandas as pd.

So pandas is the library, and pd is the short form that we are going to call. Cool. Shift + Enter. Task number one is done.

Then task number two is to load the train.csv file into a DataFrame called df, and preview the first rows. You’re going to find the dataset in your materials. It’s a very cool dataset. It has a lot of stores, the day of the week, and it’s a very rich dataset for us to explore and practice some Python.

And here we go. DataFrame. So I’m going to call it dataframe = pandas.read_csv("train.csv"). There was an extra dot here, so let me remove it.

Then I do dataframe.head(). In case you’re using the Jupyter functions of Colab or whichever it is, the comments are going to give quite a few things away. I would always encourage you to double-check and make sure that whatever you’re doing makes sense. In the end, or in case you can, just try to do it on your own. That’s always the best way to practice.

Of course, when you’re actually doing the work, go for it. Go fast. As for practicing, doing it yourself is a tiny bit better.

Now we look at the dataset. We have store, day of week, date, sales, customers, whether it was open, whether there was a promo, whether we have a state holiday, and a school holiday.

The dataset is from a very well-known store, Rossmann. It’s all in German, but it’s basically something that sells things like cleaning products, beauty products, and so on. And of course, at least in Germany, there’s something very specific: some stores are closed on Sunday. That’s something we need to consider.

But if we look at it here, let’s look at something. We see a dtype warning for column seven. Remember, Python indexing starts at zero, so we have columns 0, 1, 2, 3, 4, 5, 6, 7. So the state holiday column has something weird going on.

We can specify the dtype option on import or set low_memory=False. This is clearly something we need to investigate. So I’d say the first step is to look at information about the DataFrame.

To do that, we use dataframe.info(). This provides some information. We can see that, for instance, state_holiday is currently an object. And when we look at it, it has zeros, but it’s an object type, which means it has characters or letters inside. That’s something we definitely need to investigate.

This leads us to task number four. We’re going to print the unique values in state_holiday and their counts.

The way we do this is dataframe["state_holiday"].value_counts(). And we should get something. If you’re not using a notebook style environment, you’d have to use print. I personally have a big preference for notebook styles because of the input-output flow—it’s much easier to explore data this way.

So let’s remove the print because this looks way nicer. We can see that we have a zero, and then we have a different type of zero, and then we have A, B, and C. There is something odd here.

I’m going to assume that A, B, and C are types of holidays, but these two different zeros suggest that something went wrong during data entry. That’s how I’d interpret this right now.

This leads us to task number five. We are going to binarize this set. Instead of having zero, zero, A, B, and C, we’re going to have just zero and one. The A, B, and C values will become ones, and both types of zero will become zeros.

Let’s do this. I’ve found that the simplest way is to use the following format. We take dataframe["state_holiday"] and use .isin(["A", "B", "C"]). Then we convert this to integers using .astype(int). This makes sure that whatever matches A, B, or C becomes a one.

I find this approach intuitive because we’re simply saying whether the value is in A, B, or C. The astype part may not be intuitive at first, but it’s a very simple way to convert booleans into zeros and ones.

Cool. Then we run this and replace the column in the same variable. There was one extra quote there, so we remove it.

Now let’s check again by running value_counts() on state_holiday, just like before. Control + Enter, and here we go. We now only have zeros and ones.

# **C) Subsetting Stores and Basic Aggregations**

When we look at the data, we can see that we have stores, and because there are a lot of entries, it’s not immediately clear how many stores we actually have. It’s usually very helpful to understand the structure of the data you’re analyzing.

So we’re going to count this. Let’s make it nice and readable by printing it using an f-string. We print something like: “There are” and then the length of dataframe.store.unique(), followed by “stores”.

When we run this, we see that there are 1,115 stores. That’s quite a lot for the sake of our analysis.

As we start exploring the data or even doing data visualization, visualizing 1,115 stores is simply not reasonable. So there are two things we’re going to do. First, we’re going to randomize a subset to see how things work. Second, we’re going to simplify our analysis from here on out.

What we’re going to do is subset the data to ten random stores, using a random seed of 1502, and then store the result. Let’s do this.

We start with the original DataFrame and sample from the store column. Step by step, we first sample ten stores. When we run this, you’ll notice that every time we run it, the results are the same. That’s because we set a random state.

The random state makes sure that the results I get and the results you get are the same. It doesn’t matter how many times we run it. If you change the random state, the stores will change, but as long as we keep it at 1502, the output remains consistent.

So now that we have these ten stores, we take the DataFrame and filter it. We go to the store column and use isin() to check whether each row belongs to one of these selected stores. When we run this, we get a series of booleans—True and False.

The next step is to use this boolean mask to retrieve only the rows where the value is True. This tells the DataFrame to keep only the rows corresponding to those ten stores. When we do this, we can see stores like 24, 47, and others appearing in the filtered data.

At this point, we want to store this result. So we save it as dataframe_ten. We can quickly preview it to confirm everything looks correct.

One important best practice here is to use .copy(). This ensures that we’re working with a separate copy of the data and that any modifications we make won’t unintentionally affect the original DataFrame. This is just good hygiene when working with pandas.

Now that dataframe_ten is properly set up, we can move on.

Task number three is to compute the mean sales for each of the ten stores and sort them in descending order. Let’s do this.

We take dataframe_ten, group it by store, then select the sales column and compute the mean. Once we have the result, we sort the values in descending order.

When we look at the output, it’s not immediately obvious which store is the biggest just by scanning the numbers, so sorting helps. After sorting, we can clearly see that store number 4 has the highest average sales, while store 794 has the lowest.

There’s quite a big difference here—roughly a threefold difference in average sales. And remember, this is just within our subset of ten stores.

Now, one last thing that I find particularly interesting is the following: for each store, we want to find the row corresponding to the day with the all-time highest sales.

In other words, we want to know which specific day each store achieved its maximum sales.

To do this, we take dataframe_ten, group by store, and focus on the sales column. Instead of computing the mean, we now use idxmax(), which gives us the index of the row where sales were highest for each store. We could also look at idxmin() if we wanted the worst day.

Once we have this, we can optionally sort the results in descending order to see the biggest peaks first.

When we inspect the output, we see something remarkable. For example, store 864, which averages around 3.5K in sales, had one day where sales were close to one million. That’s roughly 300 times its average daily sales, which is absolutely insane.

# **D) Working with Datetime Index and Weekday Patterns**

Task number one is to set the date column as the index, and then preview the first rows.

To give a bit of insight here, when we work with time series data, we care about time. Setting time as the index enables us to manipulate the data in a much easier and more natural way. This is especially important for visualization, because Python then understands that we are dealing with data evolving over time. Without this, Python doesn’t really know that the data is temporal, and I’ll show you that in a moment.

For now, we set the date column as the index. The way we do this is by using dataframe_ten.set_index("date", inplace=True).

Let’s preview a few rows to see what we get. We can see that the date is now shown in a consistent format. That looks good.

But now let’s look at what Python actually thinks about our date column. If we go back and inspect the information, we see that the date is still an object. An object means characters or strings, not a real date.

So we inspect dataframe_ten.index. When we look at it, we see that it has a start, but it’s still considered an object. The name is “date”, but that name itself is meaningless—it’s just a label.

What we need to do is convert this into a proper datetime object. To do that, we use pandas and apply to_datetime on the index. Once we run this, Python now understands that this index represents dates.

At this point, Python knows that we’re working with time, and this allows us to properly explore and manipulate the data from a time series perspective.

Cool.

Now let’s look at something interesting. Even though we’re not heavily using the index yet, a useful exercise is to compute the average sales per day and per store.

Let’s do this.

We take dataframe_ten and use groupby. At this point, you can probably already see how relevant grouping is. We group by day of week and by store. Then we select the sales column and compute the mean.

Here we go.

Now, I know—I know—it’s not very easy to interpret this output just by looking at the table. That’s fine. That’s going to be addressed in task number four.

Before moving on, I want to highlight something important. Instead of using the existing day_of_week column, we could have used dataframe_ten.index.dayofweek, and the result would be exactly the same. In our case, the day-of-week variable already exists, which makes things easier.

But when you work with time series data, you can create all sorts of time-based variables directly from the index—day of week, month, year, and so on. This makes feature engineering very powerful and very simple once your index is properly set.

All right.

So now we move on to visualization.

We want to visualize the average sales per weekday and per store using a line plot.

The simplest way to do this is to reuse the code we already have. I’m a big fan of reusing code whenever possible.

The first thing we need to do is unstack the grouped result. Once we unstack, we can already see that this becomes easier to compare across weekdays—0, 1, 2, 3, and so on. Day 6 corresponds to Sunday.

Since we’re dealing with Germany, this makes sense because many stores are closed on Sundays. Still, even with this structure, it’s not very easy to visualize just by looking at numbers.

So the next step is to call .plot().

When we do that, the visualization becomes much clearer. Now we can actually see the patterns.

Looking at the plot, one thing immediately stands out. All stores have zero sales on Sunday except for one. This is something interesting and consistent with what we saw in the table.

However, there’s another issue. The magnitude of sales differs a lot between stores. Some stores reach sales around 10,000, while others are much lower. This makes comparisons tricky, because the scale dominates the visualization.

And this is where the exercise really starts.

I want you to start thinking about this: when we want to understand how sales evolve during the week, we’re essentially looking at a seasonal pattern. To truly compare these patterns across stores, we should also start thinking about standardizing the data.

# **E) Standardizing Sales and Comparing Weekday Patters**

In order to properly compare behavior across different stores, we need to standardize the data. Standardization ensures that there is a common denominator among the stores, so that we are truly comparing apples with apples.

The standardization formula itself is very straightforward. It works as follows: we take the value, subtract the average, and divide by the standard deviation. This is the classic z-score formulation.

So this is step one. The main challenges here are twofold. First, we need to build a function. This is a very standard Python task and a great way to practice writing functions. Second, we need to apply this function correctly to our data, making sure that we zoom in on each store individually. In other words, we want to compute the mean and standard deviation per store and standardize sales within each store.

This brings us to task number one.

We need to define a function that standardizes a numeric series using z-scores, meaning value minus mean divided by standard deviation. Let’s do this.

We start by defining a function called standardize, and we pass a series as the input. Inside the function, we return the series minus the series mean, divided by the series standard deviation. This is very straightforward.

Now that the function is defined, we need to apply it to our data.

What we need to do is take our DataFrame and group it by store. This ensures that we are focusing on one store at a time. Then we focus on the KPI we care about, which is sales.

When we do this, we group by store, select sales, and apply a transformation using our standardize function. At first, it may show as a generic grouped series because we haven’t applied the transformation yet. But once we apply transform(standardize), the values are standardized per store.

After running this, we can see that the values are now standardized. This allows us to really work with and compare patterns across stores much more easily.

Recall that standardized values are centered around zero, and most values typically lie between -2 and 2. This makes interpretation and comparison much more intuitive.

This leads us to task number three. But before we do that, we need to store the standardized values.

We assign the standardized sales back into our DataFrame using .loc across all rows, and we create a new column called sales_standardized. The name “STD” here refers both to standard deviation and to the normalization itself.

Once we run this, the new column is created. We can quickly preview a few rows using .head() to confirm that everything looks correct.

Now we move on to computing the mean of the standardized sales.

We take dataframe_ten and group by day of week and store. Then we select the sales_standardized column and compute the mean. This gives us the average standardized sales per weekday and per store.

This naturally leads us to task number four.

We take the result, unstack it, and then plot it. This allows us to visualize the standardized weekday patterns much more clearly.

However, there are a few issues here. Nine out of ten stores have no sales on day seven, which is Sunday. This completely skews the visualization. From an analysis perspective, this is not ideal because zeros don’t really carry meaningful information in this context.

One thing that clearly stands out is the brown series, store 353. It shows extremely strong seasonality on Sundays. That makes sense, because if most other stores are closed, this store benefits disproportionately.

This is something we can infer from the data.

If we look at the other series, we see a different pattern. Sales tend to decrease day by day until Saturday, showing a clear weekday seasonality. That’s one conclusion we can draw.

At the same time, Sunday completely distorts the picture and makes it difficult to analyze the remaining patterns properly. We clearly need to clean this up further.

Still, let’s continue exploring.

Let’s take a closer look at Sundays specifically. Are stores always closed? Is there even a single Sunday where some of them are open? The same question applies to the other stores as well.

These are things we can visualize, explore, and then use to draw our own conclusions.

# **F) Analyzing Sales on a Specific Weekday**

I have to say, I only listed one task here, but this could have been done in multiple ways. What we need to do is filter the data for day seven and then visualize it. So this will be step number one and step number two at the same time.

First, let’s import what we need for visualization. We import matplotlib.pyplot as plt. That’s step number one.

Next, we create a new DataFrame called dataframe_d7. We start from dataframe_ten. There are many ways to do this. One way is to filter where the day of the week is seven and then make a copy. That’s one valid approach.

Another option would have been to use the index directly, since we’ve already set it to datetime. That’s another possibility. In this case, we’ll stick with using the existing variable and filtering based on the day of the week.

Now we move on to plotting.

We start by creating a figure using plt.figure(). We set the figure size to 12 by 6, which is a fairly standard size.

Next, we loop through the data. For each store and its corresponding data in dataframe_d7 grouped by store, we plot a line. We plot the date, which is stored in the DataFrame index, on the x-axis, and sales on the y-axis. For each line, we set the label using an f-string so that it shows the store number.

This loop allows us to plot each store’s Sunday sales on the same chart.

We close the parentheses and then display the plot. This is already enough to generate the visualization. It’s also a nice introduction to matplotlib if you’ve never used it before, because it shows the basic structure of creating a figure, plotting data, and labeling it.

Now, here’s a cool thing that you might or might not have noticed. Let’s quickly look at dataframe_d7 itself. You’ll see that the earliest dates are from 2015, and the later dates are from 2013. In other words, the data is reversed.

However, because Python knows that we’re dealing with dates, this doesn’t matter at all. When we plot the data, everything is automatically ordered correctly along the time axis.

That was just a quick side note.

Now let’s clean up the plot a bit. We add a title, “Sales on Sunday”. We label the axes with date and sales. We also add a legend so that we can identify each store.

I also like to add plt.tight_layout(). This usually tidies things up a bit and makes the plot easier to read, which I personally appreciate.

When we look at the plot, I feel it’s a bit too tall, so we reduce the figure height from 6 to 5. That small adjustment makes it look better.

Now, what do we see?

We see that every single store except one is always closed on Sundays. There are no exceptions at all. This tells us that by including Sundays in our analysis, we’re potentially introducing noise—especially when we standardize the data.

For store 353, there is one Sunday where the store was closed, which explains a drop. But aside from that, this store has recorded sales on every single Sunday, and those sales are growing over time. That’s definitely interesting to observe.

And that’s it.

This is how we do a quick visualization to explore a specific pattern in the data. I would say this exercise is on the more difficult side, because it includes a for-loop and multiple steps. But if you understand this, you’ll be able to handle virtually every visualization throughout this course.

# **G) Removing Outliers and Re-Assessing Intra-Week Seasonality**

Our conclusion so far is that we clearly have store 353, which behaves very differently from the others. There’s really no point in comparing it directly with the rest. At the same time, we also have day seven, which is Sunday, and this day is kind of “poisoning” our data because it introduces a lot of noise.

Because of this, task number one is to remove all observations for day of week seven and for store 353. Doing this will give us a much cleaner DataFrame, allowing us to continue exploring seasonality in a more meaningful way.

Cool, let’s do this.

We start from dataframe_ten. We want to keep only the rows where the day of week is not seven and the store is not 353. Since we have two conditions, we need to combine them.

First, we filter dataframe_ten where the day of week is not equal to seven. Then, we add a second condition where the store is not equal to 353. Finally, we make a copy of the result to avoid any unwanted side effects.

We store the result in a new DataFrame called dataframe_clean.

Once that’s done, we can take a quick look using .head(). This preview doesn’t show anything dramatic, because we’ve removed rows rather than added new ones. The main purpose here is simply to confirm that we still have data and that the operation worked as expected.

Now that we have a clean DataFrame, the next step is to re-standardize the sales column within each store, but this time using dataframe_clean.

We essentially repeat the same process as before. We take the sales column, group by store, and apply the standardization transformation. The result is stored again in the sales_standardized column.

If we preview a few rows using .head(), we can immediately see that the standardized values have changed. For example, values that were previously around 1.5 and 1.1 are now closer to 1.65 and 1.16. This makes sense, because we’ve changed the underlying data by removing Sundays and the outlier store.

This brings us to task number three.

We now take dataframe_clean and group it by day of week and store. Then we select the sales_standardized column and compute the mean. This gives us the average standardized sales per weekday and per store, using the cleaned data.

As mentioned earlier, we can store this result. Let’s call it intra_week.

Now we move on to task number four, which is visualization.

To visualize the intra-week seasonality, we take intra_week, unstack it, and then plot it. Compared to before, the view is much clearer. Earlier, everything almost looked like a straight line. Now we can actually see meaningful differences and patterns.

We can also customize the plot directly within pandas. For example, we can set the figure size to 12 by 6, add a title such as “Intra-week Seasonality of Standardized Sales”, and adjust the layout. If we want to remove or fine-tune certain axes, we can do that as well.

Reducing the figure height slightly—from 6 to 5—makes the visualization a bit cleaner and easier to read.

Now let’s interpret what we see.

There’s one store, store 267, that shows very strong Saturday performance, which is quite unique. In general, Thursday to Saturday seem to be the strongest days for most stores. From Monday to Tuesday to Wednesday, sales tend to decrease, and then there appears to be some stabilization.

Overall, Monday is often the strongest day, Saturday the weakest—except for that one store. We also see a general decrease until Wednesday, followed by a flatter pattern for the rest of the week.

One important takeaway here goes beyond just seasonality. This exercise demonstrates the process of time series analysis. We take a sample of data, analyze it, and identify problems. Then we ask questions like: “What’s causing this distortion?” In this case, Sunday was the issue, so we removed it.

Next, we notice a store that behaves completely differently. That store clearly belongs to a different category and should be analyzed separately. Ideally, we would later compare stores that are open on Sundays with those that are not, because they may not be directly comparable.

This is how we move step by step toward insight: analyze, detect anomalies, clean the data, reanalyze, and visualize the results. And ideally, at the end of this process, we present clear visualizations that support our conclusions.

# **H) Analyzing the Impact of Promotions**

One of the most interesting questions—especially in revenue planning or demand planning—is understanding the impact of promotions on revenue. This is something we are going to explore slowly here, just a little bit, from a more data-exploratory perspective.

Let’s start with the first task. We want to compare the average sales on promotion days versus non-promotion days. To do this, we use a function you’re probably already tired of by now: groupby. We group by the promo variable and then compute the mean. This immediately gives us something concrete to compare—average sales when promotions are active versus when they are not.

At this point, you might notice something odd happening, especially if you’re working in Colab. You may realize you’re doing something wrong, or Colab might not be cooperating properly. When that happens, it’s totally fine—and often necessary—to restart the runtime and rerun everything from scratch. That’s exactly what we do here.

After restarting and rerunning the pipeline, we see the results clearly. Average sales on promotion days are around 7991, while on non-promotion days they are around 4.4k. That’s already interesting. Here, I’m intentionally switching back to using the full data frame instead of one of the cleaned or filtered versions from before. For this particular analysis, working with the complete data frame feels more natural and helps illustrate a few additional concepts.

Now we move to task number two. Instead of looking at promotions globally, let’s break this down per store. We want to compare promotion versus non-promotion sales at the store level. To do this, we group by both store and promo, select sales, compute the mean, and then unstack the result. This gives us a table where, for each store, we can directly compare average sales with and without promotions.

We store this result in a new object called promo_uplift. This data structure now contains the mean sales per store, split by promotion status. We can inspect it using .head() or similar methods to confirm everything looks correct.

Task number three is where things get more interesting. For each store, we want to compute the relative uplift in sales during promotion days compared to non-promotion days. The idea is simple: we take the difference between promotional and non-promotional sales and divide it by the non-promotional baseline. In other words, (promo − non_promo) / non_promo.

This relative format is very useful because it puts all stores on a common scale, similar to standardization. Even if stores have very different absolute sales levels, relative uplift allows us to compare them fairly. We compute this relative difference and add it back to our promo_uplift structure. After that, we can again inspect the results to make sure everything looks reasonable.

At this stage, a very natural and insightful question arises: in which stores is promotion actually making a meaningful difference? To answer this, we look for the stores with the largest relative uplift. We take the computed uplift values and use nlargest(5) to identify the top five stores in terms of promotional elasticity.

This reveals some very interesting results. For example, stores like 198 and 607 stand out, and one store—store 108—shows an uplift of around 2.25x. That’s a very significant increase and clearly indicates that promotions are extremely effective in that store.

Task number five is simply the flip side of this analysis. If we can find the highest promotional elasticity, we should also look for the lowest. Using nsmallest(5), we identify the stores where promotions have little to no impact on sales. This immediately raises important business questions: what’s happening in those stores? Why don’t promotions work there?

Of course, there are many possible explanations. There could be other factors we’re not accounting for, such as how often promotions are run, when they are run, or how large they are. Promotions aren’t just a binary yes-or-no variable. They have magnitudes—depth of discount, breadth of products included, and timing over time. These are what we call confounding factors, and they can strongly influence the observed effect of promotions.

Overall, this exercise shows how we can start uncovering meaningful insights step by step using relatively simple exploratory techniques. If you’d like to see more exercises like this—especially ones that dig deeper into real business questions—just let me know, and I’d be very happy to create them for you.

# **IV) Section 4: Introduction to Time Series Forecasting**

# **A) Game Plan for Introduction to Time Series Forecasting**

Welcome to this video where I’ll make this game plan for our introduction to time series forecasting.

The thing is that we are going to talk about this new type of data. Well, not really new, but it could be new for you, which is time series data. Have you ever heard of it?

It’s all about looking at how things like stock prices or the weather change over time. And guess what? We are going to tackle it with Python. So don’t worry if you are new to this, I’ll guide you every step of the way, and I promise you it will be fun.

First off, let’s discuss what time series data is and what this really means. Imagine that you’re keeping track of your daily coffee spending. That’s time series data. Anything that records how things change day by day or month over month—that is time series data.

Now the cool part: we are going to use Python for all of this. We’ll start with the basics, and you’ll be amazed at how much you can do with just a few lines of code. I think you’ll be quite happy with how much we can achieve just in this section, which is an introduction.

We’ll get our hands dirty by sorting and playing with data. It’s really like putting a puzzle together, figuring out how to do it. We’ll look at patterns and trends. For instance, can we understand why Bitcoin’s price skyrocketed last week? Or why do sales dip every July?

You’ll learn how to spot these patterns, and of course, we’ll draw some graphs. Not just any graphs, but ones that really tell a story. You’ll learn how to turn numbers and dates into cool visual stories that anyone can understand.

Ever wonder if you can predict stuff like stock prices? We’ll talk about that too. It’s a bit like trying to guess the end of a movie, but with data and trends.

And to wrap it up, we’ll look at real-world examples where forecasting didn’t go as planned. It’s really about looking at someone else’s mistakes—and trust me, there’s really a lot to learn there.

# **B) What is Time Series Data?**

In this video, we are going to dive deep into time series data, and I’m also excited to introduce you to a particularly intriguing data set: the Bitcoin price data.

This data set will be the central focus of our tutorials. It tracks the daily price of Bitcoin spanning nearly a decade, from 2014 to 2023.

Now, why Bitcoin price data? Bitcoin, as a pioneer of cryptocurrencies, has a rich and dynamic data set. Its market is known for volatility, rapid price changes, and significant trends, making it an excellent subject for time series analysis.

By studying this data set, you’ll gain insights not just into Bitcoin’s price movements, but also into broader financial market dynamics and investment behavior.

Now let’s understand the essence of time series data. Time series data is unique, and I really want you to understand why. It’s like a chronological story, where each data point is a moment in time, neatly lined up from the oldest to the newest.

You’ll often find this data captured at consistent intervals—think daily, weekly, or monthly snapshots of data.

If we explore its wide-ranging applications, time series data isn’t just about finance. Imagine it being used in weather forecasting, where it helps predict rainfall or temperature trends. In economics, it’s crucial for analyzing GDP growth, and in healthcare, it’s used for monitoring patient heart rates over time.

Now let’s go deeper into the unique statistical tools used in time series analysis. Time series analysis introduces some fascinating statistical concepts. Together, we’ll look at autocorrelation—understanding how a data point is related to its past.

In fact, time series data is very unique because we use data from the past to predict the future. Moreover, we’ll discuss seasonality, identifying patterns that repeat over time. These concepts are key to accurate forecasting and trend analysis.

As we step into this journey through time series data, I encourage you to think about how this knowledge could enhance your own projects. What questions do you have? Do you see any practical applications for what you are learning?

Feel free to reach out in the Q&A or the student communities. I’m here to help. Until the next video—have fun!

# **C) Python - Libraries and Data**

Welcome to our first Python tutorial.

In this video, we are going to set up everything when it comes to our Python script. I have stored my materials here in my drive because we are going to use Google Colaboratory. You are welcome to use anything else, but I would strongly recommend that you do this with me.

That said, the materials are there for you, and of course, your IDE is your IDE. If you are new to Python, especially, I strongly encourage you to follow along, but I’m always here to help in case you have any issues.

If you decide to pick something else, you’ll find in the materials that there are several folders. We’re going to start with the time series analysis, and then with the introduction to time series analysis. We’re going to click on New, then More, and then Google Colaboratory—either create and share or just create.

If you have not shared it with anyone, it will just be “Create.” In case you’re having issues finding all the materials, they are in lecture number three or four of this course. You’ll be able to find everything there. There’s a link to my website, and on my website, there’s a big download button.

Now I’m going to change the title to Introduction to Time Series Analysis. You’ll notice that there are several things here. This is the table of contents, and we’ll build on this by creating headings and subheadings.

There’s also something here called Secrets, which we’re not going to use in this course. Then there’s Files—this is how we connect to Google Drive, and we’re going to use this extensively.

Let me mount the drive already. Connecting to Google Drive is called mounting Google Drive. You click on the drive icon, connect to Google Drive, and sometimes you’ll get a piece of code that you need to run.

If you’re wondering what that piece of code might be, it’s:

from google.colab import drive
drive.mount('/content/drive')

This is the code used to mount the drive. It doesn’t happen every time that you get this piece of code, which is why I’m sharing it—especially since this is the first time.

Once the drive is mounted, you’ll see something called drive. You click on drive, then go to My Drive. I have personally stored the folder on the homepage of the drive—the main part of the drive.

You’ll see Python Time Series Forecasting, then Time Series Analysis, and then Introduction to Time Series Analysis. You go to the three dots, copy the path, and then click back on code.

Now we’re going to set the directory. This is how we always connect to that specific directory, where in this case we have a couple of CSVs that are going to be our datasets. To do that, you use %cd to change the directory, and then you paste the path using Ctrl + V.

Here we go.

Let me also add a section here—this is going to be the Setup. To see this, we go to the table of contents, and you’ll notice that Setup now appears there.

Next, I definitely want us to import libraries and data. For importing libraries, we’ll import pandas as pd, numpy as np, and matplotlib. For now, this is going to be it. Each time we require a specific function or library, I’ll import those later. This way, we’re building everything step by step.

Then we’re going to load the dataset using pandas.read_csv. We’ll start with the Bitcoin price dataset. This is a good dataset for daily data—bitcoin_price.csv.

Then we use DataFrame.head(). This allows us to preview the first five rows of the data.

Here we go. We have seven KPIs. We have the date, starting from 2014, and then we have open, high, and low. Open is the price at the start of the day, high is the maximum, and low is the minimum.

Then we have the close, which is the value at the end of the day, and the adjusted close. The adjusted close accounts for things like dividends or splits. In this case, there’s nothing like that, but the adjusted close is the value we typically use because it’s the most relevant indicator of the value at the end of the day.

Finally, we have the volume, which is the trading volume for that specific day.

Now, what I want us to do is to close this video. In the next one, we’re going to return and explore something called the time series index. We’ll see how to do it, and that’s going to be it for now.

# **D) Python - Time Series Index**

In this video, we’re going to focus on the Time Series Index. This is a very important topic because when you’re dealing with the index—and you actually make the index out of dates—it makes your life much easier when it comes to time series analysis. Whether it’s plotting, forecasting, or the analysis itself, everything becomes quite straightforward.

Not all libraries that we are going to work with require this step, and I’ll show you which libraries do and which ones do not. However, for the sake of doing some pre-analysis, the main libraries that we work with—such as pandas, matplotlib, and later on, Statsmodels—use the date as the index. This is why it’s very important that you master this part of time series analysis.

What we’re going to do is convert the date column into a datetime object and then set the date as the index. These are two different steps, so let’s give it a go.

The first thing we need to focus on is the date format. In our case, the date is already in the correct format, which is year-year-year-year, month-month, day-day. This format is expressed using %Y-%m-%d. The capital Y means we are using four digits for the year, followed by the month and the day.

We can even see that tools like Google Gemini suggest using pandas.to_datetime. We set the column as date and specify the format. For our dataset, the date is already in the correct format, but it’s still a good practice to check.

Let me check this one. Okay, this one is fine as well. But in the next section, you’ll see that the date is not in the correct format. The bottom line is that you can always handle this by specifying the current format, and pandas will transform it into the correct one.

If we run DataFrame.head(), nothing is really going to change visually. The important step is setting the date as the index. To do that, we use set_index, specify the date column, and use inplace=True. Inplace means that the change is applied directly to the DataFrame, and the date column itself is removed.

Let me run this. Here we go—now the date is our index.

There are a few different ways to do this, and I’ll show you another one in a moment. But first, I want you to see what this allows us to do. For example, if we want to select a specific day from our DataFrame, we can use .loc and reference the index directly.

We specify a date, such as 2021-11-09, and when we run it, we get all the values for that specific row.

Now, another thing I want to show you is that we can also set the index at the time of importing the data. Let me call this DataFrame one. We again use pandas.read_csv with the same Bitcoin price CSV file. This time, we specify index_col='date' and parse_dates=True. This ensures that the date is parsed correctly and set as the index right away.

This approach gives us exactly the same result.

Before we close this video, I want to show you one last thing. You can always resample time series data to a different time granularity.

What does this mean? Imagine we have daily data, but daily values might be too noisy. Maybe weekly data would make more sense for our analysis. This is completely possible.

To do this, we take the DataFrame, use .resample(), specify 'W' for weekly, and then apply an aggregation such as .mean(). As you can see, this gives us weekly data.

You could also use the median, the maximum, or any other aggregation you can think of. If you can think of it, it probably exists—it’s just a matter of knowing what to apply.

Instead of weekly, you could also resample monthly, quarterly, or yearly. It’s really about deciding how to transform daily data into another time granularity.

This always depends on the business requirement. Do you care about daily data? If yes, then keep it. If weekly data is more relevant to your business problem, then go with weekly. It’s always about defining the business problem first, and then letting the analytics follow.

# **E) Python - Exploratory Data Analysis Part 1**

In this video, we’re going to talk about exploratory data analysis. Here we go.

It’s very important that we always, always understand the data that we are working with. In this case, we want to understand its cycles, its growth, and how it is developing. This is one of the fundamental things that really distinguishes a good analyst, scientist, or engineer from a mediocre one.

It all comes down to purpose. What are we doing? What is this for?

I’m going to cover a couple of things that are extremely simple, but also very common in time series analysis—one of them being the rolling average. This is particularly important when we start doing feature engineering, such as combining variables or averaging them. In this case, rolling averages create smooth curves, which are very useful for analysis.

Let’s do it.

Our first step is to generate a seven-day rolling average for the closing price. This is very straightforward. The way we do it is by taking the DataFrame, selecting the close column, and then applying the rolling method.

We specify a window of seven days, and then we decide how we want to aggregate—here, we use the mean. Once we do that, we get the rolling average. Of course, the first few values are missing because we don’t yet have a full seven-day window, but after that, the values appear.

What I think is particularly relevant is plotting two things together. Before doing that, I want to store this rolling average in a variable. I’ll call it the seven-day closing average. Now this value is stored and ready to use.

Next, I want to plot both the closing price and the seven-day rolling average together. Once we do that, we can clearly see the difference between the raw data and the smoothed curve.

Let me make sure we actually show the plot. Sometimes, especially in Jupyter environments, plots don’t display correctly unless we explicitly call plt.show(). This helps avoid issues with axis labels and rendering.

You can see quite a lot of data here, so let’s zoom in. A very easy way to do this is by using .loc and specifying a year, for example 2023.

Now we can clearly see what’s happening. The orange line represents the seven-day closing average. You’ll notice that it’s always slightly delayed compared to the actual closing price. This makes sense—it takes time for new values to influence the average.

That’s exactly how rolling averages work. You can shift values if you want, and I’ll show shifting later, but that’s not the nature of a rolling average. The more days you include in the window, the more delayed the curve becomes. Seven days is a relatively small window, so the delay is manageable, but it’s still there.

The next thing I want to show you is also related to rolling, but it’s more about aggregation. Let’s say we want to find the month with the highest average closing price.

To do this, we resample the data on a monthly basis and calculate the mean. Then we focus on the close column and look for the index of the maximum value. This gives us the month with the highest average closing price.

You might see a warning that the letter M is deprecated and that you should use ME for month-end, but the logic remains the same. Once we run this, we get our result.

This is one way to explore the data. You can also preview different parts of the dataset. For example, if you want to see the last five rows, you can use DataFrame.tail(). This is essentially the opposite of DataFrame.head().

And that’s it for this video.

You might notice that I mention things like data manipulation, seasonality, autocorrelation, and partial autocorrelation. Realistically, this entire section is dedicated to giving you techniques to get to know your time series better.

The goal is that, by the end, you have a toolbox of methods you can use to understand any time series you’re working with.

# **F) Python - Exploratory Data Analysis Part 2**

In this video, we are going to continue with the exploratory data analysis.

One thing that’s very interesting in time series analysis is looking at day-over-day or period-over-period comparisons. For example, you take one day and then look at the percentage change compared to the previous one. This is exactly what we’re going to do here.

We’re going to compute the percentage change for the closing price variable. There is a built-in function in pandas that allows us to do this very easily.

We always start by selecting our variable, so we take DataFrame['Close']. Then we apply the percentage change function, which is already available. When we do this, we immediately get the percentage change for each day.

The first day is, of course, not available because we don’t have a previous day to compare it to. Essentially, the calculation works by dividing the current day’s value by the previous day’s value. For example, one day divided by the day before it. This type of calculation is extremely common in financial data and is very helpful because it connects the dots between consecutive periods.

Personally, I like to multiply this value by 100 so that it’s easier to visualize as a percentage. I find that having the values expressed this way makes interpretation much more intuitive.

We then store this under a new column in the DataFrame, something like daily_returns_100pct, so it’s always clear that the values have been multiplied by 100.

Once we have this, we can start exploring the data. For instance, we might want to check which days had more than a 10% change in price.

To do that, we take the DataFrame and filter it by selecting the daily returns that are greater than 10%. This gives us all the days where the price increased by more than 10%.

But then you might think—what about days where the price dropped by more than 10%? Those are not included in this filter.

The solution is very simple. We take the absolute value of the daily returns and then check which values are greater than 10%. This way, we capture both large positive and large negative changes.

Now we’re looking at all the days that experienced more than a 10% change in either direction. For this dataset, you can see that there are around 97 days with more than a 10% change.

That’s massive and clearly shows how extremely volatile this data is.

That’s it for this video. In the next one, we’re going to focus on data visualization and explore this dataset visually.

# **G) Python - Data Visualization**

In this video, we are going to explore data visualization. Of course, we have already done some visualization earlier, but now I want to explore a few additional parameters that you can add to make your visualizations cleaner and more visually appealing.

Let’s take a look.

We will start with the very basics, as always. For example, plotting the daily closing price. We simply go to our DataFrame, select the close column, and call .plot().

We’ve done this before, but let’s improve it a little. For instance, we can add a title. We just specify something like “Daily Closing Price”, and now the plot looks more informative and polished. This is one simple way to enhance your visualization.

Next, let’s explore something slightly different. We’ve already looked at rolling averages earlier, but now let’s apply it to a different variable.

We’ll create a 30-day rolling average for volume. To do this, we start with our DataFrame, select the volume column, apply a rolling window of 30, and then compute the mean. This gives us a new variable representing the 30-day rolling volume.

Now let’s try to plot both the closing price and the 30-day rolling volume together.

If we try to plot them directly on the same chart, it doesn’t really make sense because the magnitudes are completely different. The volume values are much larger, which causes the closing price to appear almost like a flat horizontal line.

So how do we fix this?

The solution is to use two different y-axes.

We start by plotting the 30-day rolling volume and enabling the legend. Then we add plt.show() to display the plot properly.

Next, we plot the closing price on a secondary y-axis. We specify secondary_y=True and also enable the legend. It’s important to remember that True must be capitalized so that Python recognizes it as a boolean value.

Finally, we label the y-axis for the closing price. This is how we correctly visualize two variables with very different magnitudes on the same plot.

Now we can actually start connecting the dots.

By looking at the visualization, we can begin to see potential relationships between volume and price. While it’s sometimes difficult to interpret visually, in general, when volume goes up, the price also tends to go up. This kind of relationship becomes much clearer when both variables are plotted together with separate axes.

As a final step, let’s briefly look at correlation.

We can compute the correlation between the closing price and the 30-day rolling volume. This gives us a numerical measure of how strongly these two variables are related.

If we print the results properly and use the .corr() function, we can see that the values are the same, just formatted slightly differently depending on how we display them.

It’s important to note that this correlation is the Pearson correlation, which is the default in pandas. Pearson correlation is often not ideal for time series data, but for our purposes, it still gives us a good intuition.

Just as an FYI, you might want to explore Spearman correlation, which is more commonly used for time series. However, I don’t want to go down the rabbit hole of statistics here.

Even with Pearson correlation, we’re still about 90% correct, and the overall conclusion does not change: the two variables are strongly connected. In fact, we don’t even strictly need the correlation value here, because the visualization already tells us the story.

That’s it for data visualization.

In the next video, I want us to focus on data manipulation. This is a specific topic that I’d like us to dive into next.

# **H) Python - Data Manipulation Part 1**

The first step is to identify missing values. The way we do this is by using our DataFrame, calling isnull(), and then applying sum(). This allows us to count the number of missing values per column.

Once we do this, we can clearly see that some of the columns we are working with contain null values. At that point, the real challenge becomes: how do we actually fill these missing values?

There is no single correct solution here. The approach you choose very much depends on the context of the problem. What I want to do is show you a few common techniques, and then based on your own use case, you can decide which one makes the most sense.

The first technique is to fill missing values using the next available observation.

For example, let’s say Day 29 is missing, but Day 30 has a value. In this case, Day 29 would take the value of Day 30. This approach is very useful when you want to maintain continuity in the data, or when you believe that something went wrong on a specific day and the next value is a reasonable replacement.

Again, this is just one option, and whether it makes sense or not depends entirely on your situation.

Let’s take a concrete example: the 30-day rolling volume.

We take the 30-day rolling volume series and apply fillna() with the method set to backward fill (bfill), and we try to do it in place.

When we do this, we may encounter an error or warning. This is actually an important topic to address.

The warning says that a value is trying to be set on a copy of a DataFrame or Series through chained assignment using an in-place method. It also mentions that this behavior will change in pandas 3.0.

This is one of the reasons I’m explicitly recording and explaining this, because these kinds of issues can easily show up in real projects. At the moment, we’re using pandas version 2.2, but future versions will be stricter.

The warning essentially tells us that instead of modifying a column in place using chained assignment, we should reassign the result back to the column explicitly.

So rather than using an in-place operation, we should do something like:
assign the filled result back to the DataFrame column itself.

When we try to follow this approach, we might run into another issue. For example, using method= inside fillna() on a Series may result in a deprecation warning. The message tells us that using method is deprecated and that we should instead use ffill() or bfill() directly.

This is another example of pandas evolving over time and why it’s important to keep your code up to date.

So instead of using fillna(method="bfill"), we directly use bfill().

Once we do this correctly, everything works as expected. This is a good example of how you can take a warning message, read it carefully, and fix your code step by step.

At this point, it’s worth clarifying the difference between backward fill (bfill) and forward fill (ffill).

Backward fill means that a missing value is filled using the next available value. This matches the earlier example where Day 29 takes the value of Day 30.

Forward fill does the opposite: it fills the missing value using the previous available value. In that case, Day 29 would take the value of Day 28, assuming it exists.

In practice, I personally tend to use backward fill more often, because in many time-series scenarios it makes more intuitive sense. But again, this depends entirely on your data and your assumptions.

Another common technique for handling missing values is interpolation.

Let’s take a different example: the 7-day closing average. We already know that rolling averages naturally introduce missing values at the beginning of the series.

Instead of forward or backward filling, we can interpolate these values.

To do this, we take the 7-day closing average series and call .interpolate() on it. We no longer use inplace=True, and the operation completes successfully.

This is how interpolation works in practice.

Whenever you use interpolation, it’s very important to check the documentation. I’m a big fan of going through the pandas docs, especially when working with something new or nuanced. It does take time to get used to, but it’s absolutely worth it.

In this case, we are working with a Series, which is one-dimensional data. A DataFrame would be two-dimensional, but that distinction doesn’t change much for interpolation in this scenario.

Looking at the pandas Series interpolate() documentation, we see that the default method is linear interpolation.

Linear interpolation assumes that the data points are equally spaced, which is true for daily time-series data. In most cases, linear interpolation will be sufficient.

The key difference between interpolation and forward or backward fill is that interpolation looks at both sides of the missing value. It considers the value before and the value after, and then computes a reasonable estimate in between.

Forward fill and backward fill, on the other hand, only look at one side of the equation.

To summarize:

Forward fill (ffill) uses the previous value

Backward fill (bfill) uses the next value

Interpolation uses both before and after values

Each approach has its place, depending on the problem you’re trying to solve.

That’s it for now. I spent some extra time here because missing values are a critical part of time-series data manipulation.

We’re going to come back to data manipulation and explore it further in the next video.

# **I) Python - Data Manipulation Part 2**

In this video, we’re going to focus on the index and also on feature engineering. Let’s get started.

The first thing we’re going to do is focus on the index. If we fetch the index from our DataFrame, this is as simple as calling dataframe.index. When we do that, we get all of the dates associated with our time series.

You may notice that the frequency is currently set to None. We’ll talk about frequency later, but for now, that’s perfectly fine.

What’s important here is that the index already contains a lot of valuable information. Because these are dates, we can actually create multiple features directly from the index. For example, we might want to know whether a given observation falls on a Monday, whether it’s a weekend, which month it belongs to, or which year it comes from.

All of this information already exists inside the index.

For instance, if we access the day of the week, we’ll see values like 0, 1, 2, 3, 4, 5, and 6. These correspond to Monday through Sunday.

Our goal here is to extract time variables and store them as new features.

Let’s start by extracting the year.
We create a new column called year, and assign it the value dataframe.index.year.

Next, we can extract the month by creating a month column and assigning dataframe.index.month.

We can do the same for the day of the month by creating a day column and assigning dataframe.index.day.

We can continue with more granular features. For example, we may want to extract the day of the week numerically. We do this by creating a column and assigning dataframe.index.dayofweek.

Sometimes, instead of numeric values like 0, 1, or 2, we want the actual day names, such as Monday, Tuesday, and so on. To do that, we create another column and assign dataframe.index.day_name(). This one requires parentheses.

There’s also another numeric representation we can use, which is weekday. This is essentially the same as day of week, but it’s still useful to store explicitly. We create a column and assign dataframe.index.weekday.

At this point, if we take a quick look at the DataFrame using head(), we can see that at the end we now have several new columns: year, month, day, day of week, weekday name, and weekday numeric.

As a final time-based feature, let’s create a weekend indicator.

We define a new column called is_weekend. This column will be a boolean that tells us whether a date falls on a weekend or not. The result is True for weekends and False otherwise.

This is actually very useful, because booleans are often easier to work with in models.

If you prefer to have this feature as a binary variable instead of True and False, you can convert it to integers using astype(int). This gives you 0 and 1, which is often ideal for machine learning models.

You can even add a comment to remind yourself that this conversion turns the feature into a binary representation.

All of this work is done using the index, but it also falls under feature engineering, which is what I want to focus on next.

There are many ways to perform feature engineering, and we’ve already covered some of them. For example, rolling averages—like a 30-day rolling average—are also a form of feature engineering. They smooth the data and help capture trends.

However, one of the most common and most important feature engineering techniques in time series is the creation of lagged variables.

So what are lagged variables?

Think about a real-world example. You’re browsing the web and you see an ad. You think it looks good, but you don’t buy the product that day. Instead, you purchase it the next day.

From a modeling perspective, the influence of the ad doesn’t happen immediately—it happens with a delay.

If we don’t include lagged variables in our model, we fail to capture this delayed effect. Lagged variables allow us to model situations where the impact of a regressor occurs in the future, rather than at the same time.

In other words, the value today may influence outcomes tomorrow.

Creating lagged variables is extremely simple.

For example, if we take the close column and apply shift(1), we create a lag-1 variable. This means that today’s row now contains yesterday’s closing price.

If we look at the values, we can see that what originally appeared on one date now appears on the next date.

If we want to go further, we can create a lag-2 variable by using shift(2). This shifts the values back by two periods.

As you can see, building lagged variables is extremely easy and very powerful.

We can store these as new columns, such as close_lag_1 and close_lag_2. If we preview the DataFrame again using head(), we’ll see these new lagged features added at the end.

All of this opens up a wide range of possibilities when it comes to modeling. As you start to understand your problem better, you begin to think about which variables influence outcomes and how those influences unfold over time.

This is where time series analysis and forecasting really begin to take shape.

# **J) Seasonal Decomposition**

In this video, I’m going to introduce you to seasonal decomposition.

The general idea behind seasonal decomposition is that we separate time series data into three distinct components: the trend, the seasonality, and the error term (also called the residual).

Let’s look at each of these components individually, starting with the trend.

The trend represents the general direction of the time series over time. You can imagine this as a smooth curve showing whether the data is generally increasing, decreasing, or remaining stable.

One important thing to keep in mind is that a trend can change over time, but it does not change every single day. If it did, it would no longer be considered a trend.

Next, we have seasonality.

Seasonality refers to recurring, cyclical patterns that repeat at regular intervals. A classic example is a time series that tends to be higher during the summer months and lower during the winter months.

You can imagine a chart where the data follows a repeating seasonal curve. This curve is cyclical, remains fairly consistent over time, and has a certain amplitude between its peaks and troughs.

Finally, we have the error term, also known as the residual.

The error term represents everything that is not explained by the trend or the seasonality. Ideally, this component behaves like random noise and does not exhibit any clear pattern. You can think of it as a random walk with no structure.

Now let’s zoom in specifically on seasonality.

One important thing to understand is that there are two main types of seasonality.

The first type is additive seasonality.

Additive seasonality is characterized by constant seasonal fluctuations. For example, this could mean that we always add 10 units in July or subtract 50 units in December.

In this case, the size of the seasonal effect stays the same regardless of whether the trend is low, medium, or high. If you were to plot this, you would see the same seasonal pattern repeating with the same amplitude over time.

The second type is multiplicative seasonality.

Multiplicative seasonality occurs when the seasonal cycles are proportional to the trend. Instead of thinking in terms of absolute units, we think in percentages.

For example, the data might increase by 10% in July or decrease by 50% in December. In a chart, you would observe that the size of the seasonal fluctuations increases as the trend increases. Even though the pattern repeats, the amplitude grows over time.

Now you might ask: why does this distinction matter?

By understanding whether your data exhibits additive or multiplicative seasonality, you can make better forecasts and better-informed decisions. The type of seasonality directly influences how models interpret patterns and predict future values.

Another important question is: how do we identify which type of seasonality our time series has?

Unfortunately, there is no statistical test that can definitively tell us whether the seasonality is additive or multiplicative. However, we do have two practical options.

The first option is data visualization.

By plotting the time series, we can visually inspect whether the seasonal fluctuations remain constant over time or whether they grow and shrink proportionally with the trend. This is something we’ve already started doing in earlier videos.

The second option—and this one is very powerful—is to focus on model performance.

This means building two different models: one assuming additive seasonality and another assuming multiplicative seasonality. We then compare their performance and see which one better fits the data.

Ideally, we should use both approaches every time.

Visualization helps us form an initial intuition before modeling, while model performance helps us validate that intuition afterward. In practice, the second approach is often preferred because it is results-driven.

In other words, the first option is about making an assessment before modeling, and the second option is about evaluating the results after modeling.

Of course, we’ll try both approaches.

But for now, let’s move on and see how to check for seasonality and plot it in Python.

# **K) Python - Seasonal Plots**

One of the most important topics in time series analysis is seasonality, which refers to the cyclical patterns that occur over time. These recurring curves appear again and again across specific time intervals, such as months or quarters.

There are many different ways to identify, analyze, and model seasonality, and throughout this course, I’ll show you several approaches. For now, however, we are going to focus purely on visualizing seasonality, which is often the first and most intuitive step.

There is one specific set of functions that I want to introduce for this purpose. To keep everything organized, we’ll start by importing them.

From statsmodels.graphics.tsa.plots, we are going to import two functions: month_plot and quarter_plot. These are specifically designed to visualize seasonal patterns.

In addition, although we won’t use it in this video, we will also import seasonal_decompose from statsmodels.tsa.seasonal, as it will be important later in the course.

To begin, let’s focus on monthly seasonality.

One of the simplest ways to analyze monthly seasonality is to take the monthly values of a variable and observe how they behave over time. This approach provides a clear visual indication of whether seasonal patterns exist.

We start by selecting the closing price from our data frame. Then, we resample the data to a monthly frequency using month-end ("M") and calculate the mean for each month.

Once this is done, we apply the month_plot function to the resampled data.

The resulting visualization contains two important components.

The black line represents the actual observed values for each month over time. In the case of Bitcoin, this line trends upward overall, but with large fluctuations due to volatility. Each point on this line corresponds to an actual monthly value.

The red line represents the average of the averages. In other words, for each month (January, February, and so on), it shows the average value across all years. This red line is what we use to identify seasonal patterns.

In this particular case, we do not observe strong seasonality. The red line does not show clear peaks or troughs across months, indicating that Bitcoin does not have a pronounced monthly seasonal pattern.

To clean up the output and ensure that only a single plot is displayed, we use plt.show().

We can also improve readability by adding a y-axis label, such as “Closing Price”, which makes the visualization easier to interpret.

Next, we can explore quarterly seasonality using a similar approach.

Once again, we start with the closing price, but this time we resample the data using quarter-end ("Q") instead of month-end. After calculating the mean, we apply the quarter_plot function.

This visualization makes seasonal differences more apparent. With the exception of Q3, which appears unusually flat or lower, the quarterly seasonality becomes clearer. This kind of insight can signal that further investigation is worthwhile.

At this point, it’s useful to look at a different data set that exhibits clearer seasonality.

For this purpose, we import a second data set containing monthly revenue for a chocolate company. This data set is already aggregated at a monthly level, which makes it ideal for seasonal analysis.

We read the CSV file using pandas.read_csv, specifying the date column as the index and enabling parse_dates=True to ensure the dates are handled correctly.

Because this data is already monthly, we do not need to resample it. We can directly apply the month_plot function to the revenue column.

This time, the seasonality is much more apparent. We can clearly see seasonal bottoms occurring during certain months and strong peaks during others.

For example, November stands out as a peak month, which aligns well with real-world events such as Black Friday, Singles’ Day, and holiday shopping seasons.

If we wanted to visualize quarterly seasonality for this data, we could easily do so by resampling using quarter-end frequency and applying the quarter_plot function.

However, we’ll stop here for now. There is much more to explore when it comes to seasonality, and we will continue building on this foundation in the next video.

# **L) Python - Seasonal Decomposition**

Now let’s take a look at one of the most important techniques in time series analysis: seasonal decomposition.

The general idea behind seasonal decomposition is that we take a time series and split it into three separate components. These components are the trend, the seasonal cycles, and the noise (or residuals), which represents whatever remains unexplained after removing trend and seasonality.

To start, we apply seasonal decomposition to our time series data. In this case, we are working with the adjusted close price from our data frame. We use the seasonal_decompose function and then call .plot() to visualize the result.

When we run this, we obtain a plot that shows four panels: the original series, the trend, the seasonal component, and the residuals. This gives us a structured way to understand what is driving the time series.

To make the visualization cleaner and easier to work with, we store the decomposition output in a variable called decomposition. From there, we generate the plot and explicitly set the figure size. A size of 18 by 10 generally works well and makes the components easy to read.

When we reduce the figure size significantly, for example to 5 by 5, we notice that the seasonal cycles are very difficult to see. Everything appears compressed, and the seasonal structure is effectively hidden. This highlights how important visualization choices are when working with decomposition.

At this point, we need to talk about an important parameter in seasonal decomposition: the period.

The seasonal_decompose function needs to know the length of the seasonal cycle. If the data index does not have an explicit frequency, or if we want to override the default behavior, we must specify the period manually.

This is critical because seasonal decomposition only supports one seasonality at a time.

This limitation is important to understand. For example, with daily data, we may have weekly seasonality (7 days), yearly seasonality (365 days), and potentially even monthly effects. However, classical seasonal decomposition forces us to choose only one of these.

This is a known limitation of early time series techniques. More modern models can handle multiple seasonalities, but here we must be explicit about which one we want to analyze.

Since we are working with daily data, one reasonable choice is to set the period to 365, which captures yearly seasonality.

When we do this, we start to see a seasonal pattern emerge. The seasonal component is not perfectly smooth, but it is consistent across time. The trend component shows longer-term movements up and down, and the original series reflects the raw adjusted close values.

If instead we set the period to 7, which corresponds to weekly seasonality, the decomposition becomes much less insightful. The seasonal and trend components do not reveal meaningful structure, and the output is not very informative.

This is expected, especially for stock market data. Stock prices are widely known to behave like a random walk, meaning they do not exhibit strong or reliable trends or seasonality. Because of this, predicting stock prices consistently is extremely difficult, if not impossible.

As a general rule, if someone claims they can reliably predict stock prices, they are likely trying to sell you something.

Now let’s contrast this with a different data set: the chocolate company revenue data.

This data is monthly and contains a single variable: revenue. We apply seasonal decomposition again, this time using a period of 12, since the data is monthly.

We start with an additive model, and the results immediately make more sense. The trend clearly increases over time, the seasonal component shows a repeating annual pattern, and the residuals are relatively small.

However, when we look closely at the revenue data itself, we notice something important. The seasonal spikes become larger over time. Early in the series, the seasonal fluctuations are small, but as revenue grows, the seasonal peaks and troughs grow as well.

This behavior suggests that an additive model may not be ideal.

In cases where seasonal fluctuations increase proportionally with the trend, a multiplicative seasonality model is more appropriate.

When we apply a multiplicative decomposition, the seasonal component is expressed as percentages rather than absolute values. A value of 1.0 represents a neutral effect. Values below 1 indicate a negative seasonal impact, and values above 1 indicate a positive seasonal impact.

For example, a value of 0.8 means revenue is about 20% lower than average due to seasonality, while a value of 1.4 indicates roughly a 40% increase. In this data set, months like November typically show strong positive seasonal effects, which aligns well with real-world business patterns.

This interpretation makes multiplicative seasonality far more intuitive for growing time series like revenue.

With that, we’ll stop here for now. There is still much more to explore when it comes to seasonal decomposition, and we’ll continue building on this in the next video.

# **M) Auto-Correlation**

In this video, I’ll introduce a very important and interesting concept called autocorrelation.

The basic idea behind autocorrelation is to understand whether information from the past can help us predict the future. To do this, we correlate the values of a time series with its own past, or lagged, values and observe how strongly they are related.

Imagine two axes where we plot the observation 
𝑦
y at time 
𝑡
t against 
𝑦
y at time 
𝑡
−
1
t−1. This represents the relationship between the current value and the value from the immediately previous period. If the data points show a clear upward trend, this indicates a positive correlation. Suppose we compute this correlation and get a value of 0.8.

Now we create a main graph where the correlation value is shown on the y-axis and the number of lags is shown on the x-axis. The point corresponding to lag 1 would be plotted at 0.8 on this graph.

Next, we move to lag 2. Here, we correlate the time series with values lagged by two time units. Let’s imagine the correlation value for lag 2 is 0.6. We plot this value on the same graph as well.

One key idea to understand is that as the lag increases, the correlation usually decreases. This is because the information comes from further in the past and is therefore less relevant for predicting the present. As a result, the correlation values tend to get smaller as the lag increases.

If we continue this process for higher lags, the correlation values may become very small or even negative, but they generally move closer to zero. By analyzing this pattern, we can understand how far back in time we can go and still find useful information for prediction.

Of course, this does not give us the complete picture by itself, but it is a very important piece of information when working with time series data.

To summarize, an autocorrelation plot helps us determine whether there is useful information in the past values of a time series and how long that information remains relevant.

Now let’s apply this concept and see how it works in practice. Until the next video.

# **N) Python - Auto-correlation**

We’re now going to work with autocorrelation using the statsmodels library, which is great because it not only computes autocorrelation but also plots it for us. Here, I’m following a common suggestion, which is to plot both the autocorrelation and the partial autocorrelation, although we still need to properly learn what partial autocorrelation is.

The good thing is that this is very easy to do. In general, we don’t care much about the exact numerical values of the correlations, because we’re not going to directly use those numbers anywhere. What really matters is how the plot looks, what it tells us, and the intuition behind it. That’s what we’re going to focus on.

Let me activate the environment first. I actually need to activate everything else as well. Alright, everything is activated now. I jumped to the bottom of the script. If you’re wondering why I needed to activate everything again, it’s because I’m re-recording this video. Sometimes recordings don’t go as planned, and I need to redo them to keep the content and the Python code as up to date as possible. Anyway, let’s move on.

The first step is to plot the autocorrelation of the Bitcoin adjusted close price. I have a suggested snippet here, but I’m not going to use it immediately because I want to control the figure size. So I first define the figure using plt.figure and set the size. I’ll go with a width of 12 and a height of 6. Then I plot the autocorrelation for the adjusted close values.

One important thing we still need to specify is the number of lags. Let’s set it to 100 and see what this gives us. Once we run it, we get the plot.

What we see here is quite striking. The level of correlation in Bitcoin is absolutely massive. There is a very strong correlation across many lags, which is honestly quite uncanny. Now, if we want to be a bit more critical and properly assess this, it almost looks like a linear curve. That immediately raises an important question: is this actually relevant?

In other words, if we look at lag 100, what information does lag 100 really contain that was not already present in lag 99? That’s the key question. What this plot is indicating is that all of these lags contain information, and possibly even relevant information. You could technically take lag 100 and still observe a strong correlation. But the real issue is uniqueness. What unique information does lag 100 bring that was not already captured by lag 99, lag 98, and so on?

This is exactly where partial autocorrelation comes into play. I’m spoiling it a bit here, but that’s essentially what partial autocorrelation answers. It tells us the correlation at a specific lag after removing the effects of the previous lags. In other words, it helps us understand whether the correlation at lag 100 is truly new information, or whether it only exists because the correlation was already present at lag 99, which itself depended on lag 98, and so forth.

In many cases, what we see at higher lags is not really new or relevant information. It’s just inherited from earlier lags. We’ll confirm this more clearly once we look at partial autocorrelation.

Now let’s shift our focus to a different dataset. If we inspect the df_choco dataframe, we can see that it has only 60 entries. Because of that, it wouldn’t make sense to use 100 lags here. Instead, something like 20 lags is much more reasonable.

So we now plot the autocorrelation function for the revenue KPI in df_choco, using 20 lags. We follow the same approach as before, just changing the data and the number of lags. Once we plot it, the result looks very different.

Notice that we don’t see the same kind of linear decay here. Instead, the pattern goes down, up, down, up. Another very important thing to notice is the blue shaded area in the plot. This represents the confidence interval. As we move further into the past, this interval widens. That makes sense, because the further back we go, the fewer observations we have, and the more variability we expect.

What this plot is telling us is very insightful. We can clearly see seasonality in the data. There is strong correlation at the most recent lags, which reflects the trend. But we also see notable spikes around lag 5 or 6, and again around lag 12. This suggests seasonal behavior at roughly six-month and twelve-month intervals.

This aligns well with intuition. There is information repeating itself on a seasonal basis. For example, something meaningful is happening every six months and every twelve months. This gives us strong evidence that seasonality plays a significant role in this dataset.

Again, the natural question arises: what is the value added by a specific lag? For instance, what does month three contribute that was not already present in month two or month one? This is exactly the kind of question that partial autocorrelation is designed to answer.

Overall, this exercise highlights how important it is to understand our data deeply. Autocorrelation helps us see whether past values contain useful information and how far back that information remains relevant. In this case, it looks like information up to around 12 months in the past can help us predict the future.

# **O) Partial Auto-Correlation**

Now that we understand the autocorrelation, let’s move on to the partial autocorrelation, which is a very cool concept in the world of time series. Don’t let this scare you—it's not super complicated at all. It’s actually really handy, and I’m going to break it down for you in very simple terms. So let’s kick it off.

What is this PACF, or partial autocorrelation function, all about? Imagine you’re trying to understand the relationship between your coffee consumption today and how much coffee you drank a few days ago. But here’s the twist: you want to know this relationship without the influence of all the days in between. That’s exactly what partial autocorrelation does. It tells you the direct relationship between your data points at different times, while removing the effects of the points in between.

Now remember the autocorrelation function. That’s like looking at the total correlation between a time series and its lagged values, including both direct and indirect effects. It’s similar to asking, “How does my coffee consumption today relate to all the past days?”

Partial autocorrelation, on the other hand, is more specific. It’s like asking, “How does my coffee consumption today relate specifically to three days ago?” while completely ignoring the days in between. So while autocorrelation gives you the overall picture, partial autocorrelation zooms in on specific, direct relationships.

When you plot the PACF, you’ll usually see bars at each lag, just like you do with the autocorrelation plot. If a bar stands out significantly, it means there is a noteworthy direct relationship at that particular lag. If these bars drop off quickly, it suggests that only recent values have a direct effect on current values. If they tail off slowly or oscillate, it indicates that older values still have a direct influence.

You might now wonder why PACF is needed when we already have ACF. The autocorrelation function starts the story by showing all correlations, but it can include noise from indirect correlations. The partial autocorrelation completes the story by isolating only the direct correlations, giving you a much clearer picture of how each point in time directly influences another.

# **P) Python - Partial Auto-Correlation**

Welcome back, and welcome to partial autocorrelation. Partial autocorrelation is very simple in terms of application. Just like before, we’re going to compute the PACF for Bitcoin adjusted close. There we go, and we’re going to do the exact same thing as we did earlier. Even Gemini catches on very quickly if you know what to do, and you can put it in comments and get this done very, very fast.

The only difference between this code and the ACF code is this “P” here. That’s literally the only difference. Now, look at this chart. It’s quite different, right? If you look at it closely, it’s just so different compared to the autocorrelation plot.

What this chart is telling us is that the only information that is actually relevant, the only information that is truly unique, comes from the day before. And this is kind of crazy when you think about it. You saw earlier that we had so much information, but basically all of that information was always connected to the information that happened just before it. There was no unique information beyond that. Of course, you do see some values here that go outside, but really, for Bitcoin—and please take this seriously—if you’re trying to predict Bitcoin, the only relevant information is what happened the day before. This is a very clear conclusion from this chart.

Let me actually write it down. The only relevant information for the price of Bitcoin is what happened the day before. There we go.

Now we can do the PACF for revenue in the data frame. Let me jump here using some shortcuts. Here we go, and here we are with the partial autocorrelation. In this case, we clearly see that there is information in the day before, but apparently there is also relevant information at two, three—so one, two—then four and five months in the past, and also around ten months in the past. Twelve months, apparently, is not so relevant.

So if we count the lags—one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen—this means we’re looking at the previous month and the previous year. Apparently, we did not get a lot of unique information overall, but you do get unique information if you don’t just look one month back. If you look four or five months back, ten months back, and apparently thirteen months back as well, those lags were important.

If you connect this partial autocorrelation chart to the autocorrelation chart, it becomes much more meaningful. From the partial autocorrelation, you can say, okay, these specific lags contain unique information. That’s how you can phrase it. For the autocorrelation, it’s more like saying, okay, I can use all this information from the past and still retain information. You kind of connect both ideas, and then you’re in good shape.

You can still see some level of information around twelve or thirteen months here as well, but it becomes very, very small. If you really need to nitpick, if you need to pick just a few lags, you would go with these ones. These specific lags contain very unique information that you don’t find anywhere else.

And that’s it for us. This is how you do partial autocorrelation. It’s very, very quick, and I’ll see you in the next video.

# **Q) Python - Building a Useful Function Script**

Welcome back for this last video. I want us to build a script that you can use whenever you’re working with some kind of time series analysis. Of course, it’s not perfect and it’s not a one-size-fits-all solution, but it works as a solid starting point. It gives you a building block so you can take whatever you’re doing and begin by analyzing the data. No matter which technique you later apply, exploratory data analysis, manipulation, and visualization are more or less the same and don’t really change that much.

I’m going to take this script here and move it into the main one. Let me open it, and then I’ll start with the first cleanup. The idea is to keep what’s always relevant and remove what’s not. For the setup section, everything up to the path stays the same. The Python time series forecasting libraries remain unchanged. The Bitcoin price file becomes a generic “xxx.csv” so it can be reused. I’ll clear the selected outputs to keep things clean.

For the time series index, I’m going to keep it because it’s important. Usually, the column is called “date,” so there’s no issue there. I’ll remove this part and instead use the version where we set the index directly and parse the dates at the same time. That’s the one I want to keep, because most of the time we set the index column and parse dates together. If I only want the actual date column, I can simply delete the index and parsed dates part.

I’ll remove the remaining unnecessary pieces and keep things as clean as possible. The idea is to load the data and immediately put the date on the index. For exploratory data analysis, I’m not going to use rolling prices, so that part is out. For data visualization, I will keep it. I’ll plot the series without a title, which is usually better. I typically call my time series “y” because it’s easier to visualize, easier to work with, and I don’t have to rename it all the time. It also makes sense conceptually, since this is what we’re trying to predict. The extra parts here are removed.

For data manipulation, we’re not going to focus on that here, so that section is gone as well. When it comes to seasonality, this part will stay. I’ll clean it up and leave out the extra data frame pieces and labels. If I want to add a label later, I can always do that. We’ll keep the monthly plot, and I should also add a quarterly plot. So I’ll include the quarter plot, show it, and keep the seasonal decomposition. The default multiplicative model is fine.

Next, we add the autocorrelation. I’ll set the number of lags to 100, and if I ever need to change it, I can. Then I add the partial autocorrelation as well. Now we have both ACF and PACF in the script.

It’s worth reinforcing that I’m trying to keep this course up to date. Even when future warnings appear, the goal is to maintain a clean and usable script. There may be discrepancies across different versions of libraries, and that’s expected. This script is meant to be a useful code template. If something behaves slightly differently in your environment, that’s normal.

If you have any questions, let me know. I try to make sure everything works in one go, but keep in mind this is a course that’s close to 40 hours long. There are bound to be moments where things don’t connect perfectly. I really appreciate your patience. If anything confuses you, just ask—I’m here to help.

As a final note, it’s a bit sad for me when people say they don’t understand something, not because I think I’m the best explainer in the world—I definitely don’t—but because often they don’t ask questions. If you have questions, let me know. I answer everyone seriously. Just post it in the Q&A, and I’ll be there to help you out. I’ll see you in the next video.

# **R) Can you predict stock prices?**

Have you ever been bombarded with stock price predictions? If so, then this video is for you.

As we’ve seen, distinguishing between trend and seasonality in time series can seem straightforward at first. So why is forecasting still considered such a complex task? Why do organizations dedicate entire teams to it? The main challenge lies in modeling and interpreting errors. Forecasting is not just about fitting a model, but about understanding where it fails, why it fails, and how those errors can be explained and reduced.

A big part of improving predictions comes from incorporating relevant factors or regressors. These can include specific events, temperature changes, snowfall, overall economic conditions, and even public sentiment. Each of these elements can significantly influence forecasting accuracy, depending on the problem you’re trying to solve.

Another crucial aspect is the time horizon of the data. Suppose you have data spanning six or seven years. Do you really need all of it? Older data can introduce a lot of noise into your models because past conditions may no longer reflect what is happening today or what will happen in the future. This is why it’s essential to evaluate whether your data is still representative of future trends, and if it’s not, to make the conscious decision to exclude it from your analysis.

This brings us directly to stock market prediction. We are constantly flooded with articles claiming extraordinary returns using different strategies and algorithms. But remember, everyone looks like a genius in a bull market. The real question is whether these approaches can consistently outperform professional investment firms. In most cases, that’s highly doubtful.

Price movements can give the impression of a clear trend, but predicting when that trend will reverse is extremely difficult if there is no reliable pattern. Traditional forecasting models struggle in these situations, especially when sudden changes occur.

This unpredictability became painfully clear during the Covid-19 pandemic in March 2020. The pandemic completely disrupted existing trends and seasonal patterns, making many forecasting models ineffective overnight. It was a powerful reminder that external, unprecedented events can drastically change market dynamics.

In summary, forecasting—particularly with stock data—is a complex and nuanced task. It requires careful consideration of data relevance, thoughtful modeling of errors, and an awareness of external influences that can break even the most well-designed models.

# **S) What did we learn in this section?**

We have just finished the crossing line, and for this introduction to time series forecasting, it’s a great moment to pause and reflect on everything we’ve accomplished. It’s been quite a journey when you think about it.

We started from the very basics, simply trying to understand what time series data actually is. At first, it can feel unfamiliar, almost like learning a new language. But now, it feels natural. We’ve gone from confusion to confidence, and we can clearly analyze how data behaves over time.

Diving into Python was a major turning point. What may have felt intimidating in the beginning quickly became empowering. We learned how to slice and dice time series data with confidence, using Python not just as a tool, but as a reliable partner in solving real problems. Those libraries are no longer abstract concepts; they’re part of your everyday toolkit now.

We also made big progress in data visualization. Instead of staring at raw numbers, we learned how to turn them into meaningful stories. The charts we create now aren’t just visuals, they communicate insights in a way that makes sense to anyone looking at them.

Seasonality was another important concept we tackled. What once seemed tricky is now something we can recognize and interpret with ease. Whether it’s sales patterns or recurring trends, we know how to identify and reason about seasonal behavior in data.

Autocorrelation marked another milestone. It’s one thing to look at individual data points, but understanding how values relate to each other across time is a whole new level. And you’ve reached that level by learning how past values influence the present.

The discussion around predicting stock prices was especially eye-opening. It showed us the realities of forecasting and the complexity of financial markets. There’s no magic crystal ball, but there is careful analysis, critical thinking, and a lot of smart, hard work behind every meaningful prediction.

So take a moment to appreciate how far you’ve come. You didn’t just learn concepts—you applied them, analyzed data, and visualized insights along the way. That’s real progress.

# **T) CASE STUDY: Forecasting Gone Wrong**

Hey everyone, and welcome. Today I want to talk about something pretty interesting: what happens when forecasts don’t quite hit the mark. We all try to predict the future in one way or another, whether it’s the stock market, fashion trends, or even just the weather. But sometimes things don’t go as planned, and that’s actually okay. When predictions go sideways, that’s often when we learn the most.

Let’s look at some memorable forecast failures and what they can teach us. Take the rise and fall of fidget spinners. Remember those little spinning gadgets that suddenly seemed to be everywhere? One day nobody knew what they were, and the next day every kid in school or university had one. Retailers and manufacturers assumed they had struck gold and ramped up production aggressively. But the hype faded almost as quickly as it appeared, and suddenly stores were left with huge piles of unsold inventory. It was a classic case of confusing a short-lived craze with a long-term trend. The key lesson here is the importance of questioning whether a trend is truly here to stay or just a passing fad.

Another famous example is Long-Term Capital Management, or LTCM. This hedge fund was once considered elite, staffed by brilliant minds using highly sophisticated mathematical models to predict market movements. For a while, it looked like they had cracked the code, generating massive returns. Then the Russian financial crisis hit in 1998, completely outside what their models anticipated. The markets behaved in ways their assumptions couldn’t handle, and the fund collapsed dramatically. It was a stark reminder that no matter how advanced a model is, there will always be events it cannot foresee. Markets are complex and unpredictable, and sometimes they move in ways no algorithm can capture.

Then there’s Google Flu Trends, an ambitious attempt to predict flu outbreaks by analyzing what people searched for online. The idea was simple and clever: if more people are searching for flu symptoms, a flu outbreak might be underway. Initially, it looked like a breakthrough in using big data for public health. Over time, however, it became clear that the model was frequently overestimating flu cases, sometimes by a wide margin. Changes in user behavior and constant updates to Google’s own search algorithms distorted the signals. This case shows that big data and powerful algorithms still need careful calibration and constant reevaluation, especially when human behavior is involved.

Another intriguing example is the Hindenburg Omen, a complex technical indicator designed to predict stock market crashes. Based on certain market conditions, such as the number of stocks reaching new highs and lows at the same time, it aims to warn of an impending collapse. While it has occasionally preceded market downturns, it has also triggered false alarms that caused unnecessary panic, and at other times it failed to signal real crashes. This highlights the danger of relying too heavily on a single indicator without considering broader context and supporting evidence.

All of these stories point to the same underlying truth. The world is full of surprises, and no forecasting model, no matter how sophisticated, can account for everything. Whether we’re dealing with financial markets, public health, or consumer trends, uncertainty is always part of the equation. Forecasts are valuable tools, but they work best when paired with humility, critical thinking, and an understanding that the unknown will always play a role.

# **V) Section 5: Time Series Analysis Practice**

# **A) Data Loading and Index**

Welcome to this practical activity!

In this exercise, we’re going to work with Python, and I’ll demonstrate it using Colab. Of course, you can use any Python environment of your choice. Alternatively, if you have access, the Udemy workspace is also available.

Here’s what we’re working on: we have the weekly sales of a department, and our goal is to explore this data. You’ll find the dataset in the folder I’ve shared, and it’s also preloaded in the Udemy workspace. You’ll see a starter file and Lab 1, which contains the solutions. We’ll begin with the starter file.

First, connect to your drive. Make sure to adjust your directory path if it differs from mine. I’ve also prepared all the necessary libraries for you.

We’ll complete three tasks in this lab. The purpose of this initial exercise is to prepare the dataset for time series analysis.

Task 1: Load the department sales dataset into a DataFrame. Use:

import pandas as pd


df = pd.read_csv("department_iPhone_sales.csv", index_col="date", parse_dates=True)

Here, we make sure the date column is used as the index, which is important for time series analysis.

Task 2: Convert the index into a standard date format (YYYY-MM-DD), which makes it easier to work with. By default, Pandas may infer a different format, so we explicitly specify:

df.index = pd.to_datetime(df.index, format="%d-%m-%Y")

Now, the index shows the year, month, and day correctly.

Task 3: Set the frequency of the data to weekly. Since our dataset is recorded weekly, we tell Python:

df.index.freq = "W-FRI"

We use W-FRI because the sales dates fall on Fridays. This ensures Python correctly interprets the weekly spacing.

And that’s it! With these three steps, your dataset is now properly formatted for time series analysis: the dates are clean, indexed correctly, and the frequency is set.

# **B) Data Visualization for Time Series**

Let's do Lab Number Two. You can either continue on the starter Lab One file, which is the same, or I’ve also prepared the starter Lab Two just in case you are starting with this video.

For these exercises, we are going to focus on data visualization. The first task, Task 1, is to plot the weekly sales using the weekly_sales column. Additionally, we’ll do a bit of formatting—like adding a title to the chart—to make it more readable.

To do this, we can use:

plt.plot(df['weekly_sales'])
plt.title("Weekly Sales")
plt.show()

This is probably the easiest initial component. You’ll notice, however, that the x-axis may look a bit off or crowded. To fix this and improve the visualization, we can adjust the figure size:

plt.figure(figsize=(10, 4))
plt.plot(df['weekly_sales'])
plt.title("Weekly Sales")
plt.show()

Now, we can clearly see the overall evolution of weekly sales. This completes Task 1, where we customized the chart with a proper size and a title.

Looking at the chart, you might notice several spikes—spike, spike, spike—and it draws our curiosity. There’s a column called is_holiday, and the next task will leverage this.

Task 2 involves adding a vertical line whenever is_holiday is set to True. Specifically, we want a vertical dashed red line on the chart.

To implement this, we iterate through the DataFrame rows. For each row, if is_holiday is True, we use plt.axvline to draw the vertical line:

for date, row in df.iterrows():
    if row['is_holiday']:
        plt.axvline(x=date, color='red', linestyle='--', linewidth=0.5)

Here:

color='red' makes the line red.

linestyle='--' makes it dashed.

linewidth=0.5 keeps it thin so it doesn’t dominate the chart.

After plotting, you can see that many spikes in weekly sales appear to align with holidays. Some spikes, like those in May, don’t perfectly align with holidays, which indicates other factors may also contribute. Overall, the red dashed lines help highlight a seasonality component in the data related to holidays.

With this, we have completed Lab Two, which had two tasks:

A simple plot of weekly sales with a title and customized size.

Enhancing the plot by adding vertical red dashed lines whenever there is a holiday, showing how holidays may impact sales.

This visualization is a great way to connect time series spikes with external factors before diving deeper into analysis or forecasting.

# **C) Exploratory Data Analysis for Time Series**

Let's do Lab Number Three. You will also find a starter file for this lab. We have three tasks, and the focus here is more on exploratory data analysis, with an emphasis on autocorrelation and seasonal decomposition. Basically, we want to see what kind of information is already in our data from a trend and seasonality perspective.

Task 1 is to plot the autocorrelation of the data with 60 lags. We can do this using the ACF function:

from statsmodels.graphics.tsaplots import plot_acf
plot_acf(df['weekly_sales'], lags=60)
plt.show()

Looking at the chart, the first lag has the most information. Lags one through six contain some information, and then there’s a noticeable spike at lag 52. This makes sense because it represents roughly one year before. Overall, the strongest signals are from recent weeks, while older lags have less relevance.

Task 2 is to plot the partial autocorrelation. Unlike autocorrelation, which looks at total correlations, the partial autocorrelation isolates the unique information at each lag by removing the influence of earlier lags.

from statsmodels.graphics.tsaplots import plot_pacf
plot_pacf(df['weekly_sales'], lags=60)
plt.show()

Here, we notice a slightly different picture. One year ago doesn’t carry as much unique information, but there is strong unique information one week, two weeks, and seven weeks before. The remaining lags are less relevant from a statistical perspective. This helps identify the most meaningful time intervals in the data.

Task 3 is seasonal decomposition, where we focus on a multiplicative model with a period of 52 weeks (one year):

from statsmodels.tsa.seasonal import seasonal_decompose
decomposition = seasonal_decompose(df['weekly_sales'], model='multiplicative', period=52)

We can improve visualization by adjusting the figure size:

fig = decomposition.plot()
fig.set_size_inches(12, 8)
plt.show()

Looking at the decomposition:

The trend initially goes down, then up, and stabilizes, but keep in mind the first and last six months don’t show a reliable trend due to the way seasonal decomposition works.

The seasonality appears unusual, likely influenced by external factors like holidays.

The residuals hover around one, with some spikes, reflecting irregular components in the data.

While the seasonality is strong, it’s influenced by external elements, so it might appear a bit odd. Nevertheless, this gives us a clear breakdown of trend, seasonality, and residuals in the dataset.

With this, we have completed Task 3 and the lab. I hope you enjoyed this exploration, and I’ll see you in the next video.

# **VI) Section 6: Exponential Smoothing & Holt-Winters**

# **A) Game Plan For Exponential Smoothing and Holt-Winters**

I am very excited to walk you through the world of exponential smoothing and Holt-Winters. In this video, we'll kick off with the agenda that we'll cover throughout this section. I have a very fun case study lined up. Think of it as the goal we need to achieve. It's about customer complaints, and it’s basically as real as it gets. We'll use this case study to apply the skills we’ve learned in a way that mirrors what you would encounter in the business world.

Next up, let's talk about Python. Whether you're new to it or already familiar, we'll be using it extensively. This is how we get hands-on experience. So please prepare yourself for some coding action—we’ll go deep and program everything we need to do.

We won’t just stick to one type of exponential smoothing. Oh no! We’ll explore simple, double, and triple methods, and each smoothing method will get its own spotlight in our Python sessions. This is where the magic happens—where the theory meets practice.

We also need to learn how to measure errors in time series forecasting. And because data comes in all shapes and sizes, we’ll learn how to handle weekly data, daily data, and more granular data. The more granular the data, the more complex it becomes, but this is a skill we need to master.

Last but not least, we’ll wrap up this section with a discussion on the pros and cons of exponential smoothing and Holt-Winters. It’s important to understand what a technique can and cannot do.

So, get ready to learn a lot and hopefully have some fun. By the end of this section, you’ll be able to handle Holt-Winters and exponential smoothing like a pro.

# **B) CASE STUDY BRIEFING: Customer Complaints**

So picture this. We have a challenge that we need to solve. Imagine there is a company called Telco Wave, a very big player in the telecom world. And they’re facing a real puzzle, a challenge. Their customer complaints are all over the place. Some weeks it’s smooth sailing, other weeks it’s total chaos. And guess what? They have asked us to figure it out.

So it’s our job to predict these unpredictable swings. Why, you ask? To help Telco Wave become better at customer service and to showcase how we can use data to actually solve real problems.

Here’s the problem statement. Telecom is really facing this issue because they are getting more and more complaints. They are literally scratching their heads over how many customer service reps they need for each week. If the prediction is wrong, resources are wasted, and customers end up unhappy. This isn’t just a numbers game—it’s about bringing order into chaos.

Therefore, we need to craft a strategy. But before we dive into any complex solutions, we need to understand the basics. We need to get to know the data. What’s behind these fluctuations? What hidden patterns might we be missing? We need to dissect the whys behind these numbers. This is where data analysis comes in.

Exploring the data is a big deal. If we understand it well, Telco Wave can shift from playing catch-up—wasting resources and frustrating customers—to being in control.

By the end, the goal is to empower them to match their workforce perfectly with what the customers need. Fewer complaints will fall through the cracks, more customers will be satisfied, and the business will be healthier, which ultimately means more profits.

# **C) Python - Exponential Smoothing Set Up**

Let's kick off Exponential Smoothing, which is really one of those foundational forecasting models. In this video, we’re going to focus on setup. You are going to take the useful code templates that you built before, and in case you haven’t, you’ll find one here that’s final. You can start from there, of course, but do it with me.

You’ll find one template there, or you can use the one that you built yourself. Then I’m going to do Control C and copy it into time series analysis → exponential smoothing and holt-winters, and let’s open it and see how far we can get in this video. The goal is to either completely do the setup or at least get very close to it. I also don’t want to have a super long video.

I’m going to call the file holt-winters. We’ll start by mounting the drive. This will prompt you to give permission to connect to Google Drive. Once that’s done, we can proceed.

While this is connecting, I want to introduce the dataset. It’s about weekly customer complaints, and the business scenario is simple. Imagine you work in an e-commerce company—or really, any company with customer support—and you need to figure out how many customer support people you need. Most customer supports work with short contracts. Big companies often hire agencies to provide a certain number of people for a week or month. To plan properly, you need to know the expected number of complaints so you can allocate the right workforce. This is where time series analysis and exponential smoothing comes into play.

Next, we copy the path for the dataset and add it to our script. For now, we won’t import any extra libraries; we’ll do that later as needed. The dataset is called weekly customer complaints. It has a few columns, but we will focus on complaints for now. We cannot use independent variables with Holt-Winters or any other exponential smoothing model, so the complaints column is our main focus. The time variable is called week.

After running the initial setup, we have week as the index and complaints as the main column. I like to rename the time series variable to y. This is done by renaming the column: data_frame.rename(columns={'complaints': 'y'}). Now the series is ready for analysis.

Next, we attempt to plot the time series. Initially, we might encounter a “no numeric data to plot” error. This happens because the complaints column contains commas and is recognized as an object rather than a numeric type. To fix this, we remove the commas and convert the column to an integer: data_frame['y'] = data_frame['y'].str.replace(',', '').astype(int). Now the column is properly numeric, and we can plot it.

After plotting, we see that complaints increase over time, with higher spikes and an upward trend. This suggests multiplicative seasonality—the seasonality grows with the level of the series. While the trend is apparent, the spikes indicate specific events or fluctuations. To fully understand the seasonality, we will need to perform seasonal decomposition.

This completes the setup for our exponential smoothing analysis. In the next video, we will continue exploring the data and applying the smoothing methods.

# **D) Python - Exploratory Data Analysis**

Welcome back. Let's continue here. First, we need to understand our seasonality. If we just run the model as-is, we're going to get an error. This is because our data is weekly, but we are trying to generate a monthly plot. Therefore, we need to resample the data to a monthly frequency. The suggestion is to use M, though this may show a future warning. To properly handle this, we should use month end. Once we do that, we have our seasonal data.

Looking at the monthly data, we can observe that there are noticeable bottoms in February and March, as well as in August and September. Peaks are visible around November and a bit in December. This gives us a better understanding of our data. Overall, Q4 (October to December) appears to be the strongest, while Q2 is moderate, and Q1 and Q3 are the weakest quarters.

We can also analyze seasonality by quarter. Similar to before, we need to resample the data, this time using quarter end and calculating the mean. The results confirm that Q4 is the strongest quarter, followed by Q2, then Q3, and finally Q1 as the weakest quarter.

Next, we move on to seasonal decomposition. Since we have weekly data, the period is set to 52, corresponding to the 52 weeks in a year. First, we try an additive decomposition. The seasonal cycles are visible but not extremely smooth; there are noticeable ups and downs. The seasonal component fluctuates by around 100–200 units. The residuals also indicate some variation, especially in March. The trend component appears somewhat S-shaped.

From observing the data, it looks like a multiplicative seasonality might be more appropriate. This is because the seasonal spikes seem to increase as the trend grows, making the amplitude of the cycles larger over time. So we run a multiplicative decomposition. The trend remains largely the same, and the seasonal cycles do not change dramatically, but the multiplicative effect aligns with the increasing amplitude pattern.

As we continue exploring and measuring the models, we will determine which approach—additive or multiplicative—performs better and yields the best results.

Moving on, we examine autocorrelation and partial autocorrelation. By plotting the autocorrelation, we notice that there is significant information in the first 25 periods. There is also a noticeable spike around 52 weeks, representing one year in the past. For the partial autocorrelation, we see strong signals from the previous month, the last week, two weeks prior, and so on. This pattern continues with relevance for about 16 weeks, and then we again observe a signal around 52 weeks. Overall, the recent past and one complete seasonal cycle from the past year seem most informative.

From this analysis, we can conclude that a model incorporating seasonality is needed. Specifically, a Holt-Winters model would be suitable, and this is something we will explore in the upcoming sections.

Finally, let's briefly focus on the time series frequency. To check the frequency of the time series, we can inspect DataFrame.index. This will extract key information about the index, including length, name, and frequency (initially set to None). We can then change the frequency as needed. For example, our weekly data starts on a Monday in January 2018. By setting the weekly frequency to W-MON, we can confirm the change by checking DataFrame.index again, which should now reflect W-MON.

# **E) Training and Test Set in Time Series**

Welcome back. Let me introduce a very important topic in time series: splitting the data into a training set and a test set. This concept is simple and straightforward, but extremely powerful.

Imagine that you have a dataset represented by a blue rectangle. In a typical scenario, you might randomly split it, leaving 80% for training and 20% for testing. The main idea is to build a model using the training data and then evaluate it on the test data. This provides an unbiased way to assess the model’s performance.

However, time series is a very different beast. Unlike regular datasets, information on a given day is meaningless without the context of surrounding days. Moreover, in time series, we usually aim to predict the future. So, when splitting time series data, the practice is to remove the last periods. For example, you might remove the last observation, which then becomes part of the test set. The remaining data—represented here as yellow balls—becomes the training set. Importantly, the training data is not shuffled; it retains its chronological order. The light blue portion represents the test set.

Another important point is that the test set should reflect the number of periods you expect the model to predict in practice. For instance, if you are building a model to forecast the next four weeks, you should evaluate it using four-week periods. Similarly, if your forecasting horizon is three months, your test set should span three months.

Lastly, for the training data, it is recommended to include at least two full periods of data. For weekly data, this means having at least two full years, and ideally three, to capture clear seasonal and trend patterns. The goal is to provide enough data to identify robust patterns, which ultimately leads to reliable forecasts.

# **F) Python - Training and Test Set**

Welcome back. Let's talk about training and test sets. The first thing we need to understand is our goal. Our goal here is to predict the next 13 weeks. It’s important to note that we always measure and assess our model based on the business goal.

To introduce this concept, we will split the data into a training set and a test set. In this case, our period is 13 weeks. The training set will consist of all data up until the last 13 weeks—everything except the last 13 periods. The test set will then consist of everything starting from the last 13 weeks.

If we examine the training set, we still have all the data, which is fine. However, for practical purposes—such as using double or triple exponential smoothing—we often only work with the target variable, Y. So we subset the data to include only the columns we need and remove everything else that isn’t required.

There are multiple ways to subset and split your data. One approach is simply:

train = df[:-periods]   # all data except the last 'periods'
test = df[-periods:]    # the last 'periods' of data

Another way (which I had prepared for a different setup) could be using index positions:

train, test = df.iloc[: -periods], df.iloc[-periods:]

The key takeaway is not necessarily the exact method, but that you understand the concept and purpose of splitting the data. Once you manage this, you’re doing it correctly.

# **G) Simple Exponential Smoothing**



# **H) Python - Simple Exponential Smoothing**

# **I) Double Exponential Smoothing**

# **J) Python - Double Exponential Smoothing**

# **K) Triple Exponential Smoothing aka Holt-Winters**

# **L) Python - Triple Exponential Smoothing aka Holt-Winters**

# **M) Measuring Errors for Time Series Forecasting**

# **N) Python - MAE, RMSE, MAPE**

# **O) Python - Predicting The Future**

# **P) Python - Daily Data**

# **Q) Python - Working on the Useful Code Script**

# **R) Holt-Winter Pros and Cons**
