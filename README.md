# Master-Time-Series-Analysis-and-Forecasting-with-Python-2026-Notes
This Repository contains my "Master Time Series Analysis and Forecasting with Python 2026" Course Notes from Udemy

**I) Time Series Analysis and Forecasting with Python**

**A) Time Series Analysis and Forecasting with Python**

**B) Course Introduction**

**C) Overview of the AI Time Series Assistant**

**D) Diogo's Introduction and Background**

**E) Unlimited Updates and Enhancements 2026**

**II) Section 2: Part 1 - Time Series Analysis**

**A) Time Series Analysis Overview**

**III) Section 3: Python for Time Series Analysis**

**A) Game Plan for Python for Time Series Analysis**

**B) **Load and Explore Data****

**C) Subsetting Stores and Basic Aggregations**

**D) Working with Datetime Index and Weekday Patterns**

**E) Standardizing Sales and Comparing Weekday Patters**

**F) Analyzing Sales on a Specific Weekday**

**G) Removing Outliers and Re-Assessing Intra-Week Seasonality**

**H) Analyzing the Impact of Promotions**

**IV) Section 4: Introduction to Time Series Forecasting**

**A) Game Plan for Introduction to Time Series Forecasting**

**B) What is Time Series Data?**

**C) Python - Libraries and Data**

**D) Python - Time Series Index**

**E) Python - Exploratory Data Analysis Part 1**

**F) Python - Exploratory Data Analysis Part 2**

**G) Python - Data Visualization**

**H) Python - Data Manipulation Part 1**

**I) Python - Data Manipulation Part 2**

**J) Seasonal Decomposition**

**K) Python - Seasonal Plots**

**L) Python - Seasonal Decomposition**

**M) Auto-Correlation**

**N) Python - Auto-correlation**

**O) Partial Auto-Correlation**

**P) Python - Partial Auto-Correlation**

**Q) Python - Building a Useful Function Script**

**R) Can you predict stock prices?**

**S) What did we learn in this section?**

**T) CASE STUDY: Forecasting Gone Wrong**

**V) Section 5: Time Series Analysis Practice**

**A) Data Loading and Index**

**B) Data Visualization for Time Series**

**C) Exploratory Data Analysis for Time Series**

**VI) Section 6: Exponential Smoothing & Holt-Winters**

**A) Game Plan For Exponential Smoothing and Holt-Winters**

**B) CASE STUDY BRIEFING: Customer Complaints**

**C) Python - Exponential Smoothing Set Up**

**D) Python - Exploratory Data Analysis**

**E) Training and Test Set in Time Series**

**F) Python - Training and Test Set**

**G) Simple Exponential Smoothing**

**H) Python - Simple Exponential Smoothing**

**I) Double Exponential Smoothing**

**J) Python - Double Exponential Smoothing**

**K) Triple Exponential Smoothing aka Holt-Winters**

**L) Python - Triple Exponential Smoothing aka Holt-Winters**

**M) Measuring Errors for Time Series Forecasting**

**N) Python - MAE, RMSE, MAPE**

**O) Python - Predicting The Future**

**P) Python - Daily Data**

**Q) Python - Working on the Useful Code Script**

**R) Holt-Winter Pros and Cons**

**VII) Section 7: HOLT-WINTERS CAPSTONE PROJECT: Air miles**

**A) Capstone Project Presentation**

**B) Python Solutions: Task 1 and Task 2**

**C) Python Solutions: Task 3 and 4**

**D) Python Solutions: Task 5 and 6**

**VIII) Section 8: ARIMA, SARIMA and SARIMAX**

**A) Game Plan for ARIMA, SARIMA and SARIMAX**

**B) CASE STUDY BRIEFING: Predicting Daily Revenues**

**C) Python - Setting Up ARIMA**

**D) ARIMA**

**E) Auto-Regressive**

**F) Integrated**

**G) Python - Stationarity**

**H) Moving Average**

**I) Python - ARIMA**

**J) ARIMA in Action**

**K) SARIMA**

**L) Python - SARIMA**

**M) SARIMA in Action**

**N) SARIMAX**

**O) Python - SARIMAX**

**P) SARIMAX in Action**

**Q) Cross-Validation for Time Series**

**R) Python - Cross-Validation**

**S) Parameter Tuning**

**T) Python - Setting the Parameters**

**U) Python - Parameter Tuning**

**V) Python - Parameter Tuning Results**

**W) Q&A Highlight: Handling Future Data in Forecasting**

**X) Python - Predicting The Future Set Up**

**Y) Python - Predicting The Future**

**Z) SARIMAX Pros and Cons**

**IX) Section 9: PART 2: MODERN TIME SERIES FORECASTING**

**A) Modern Time Series Forecasting Overview**

**X) Section 10: (Facebook) Prophet**

**A) Game Plan for Facebook Prophet**

**B) Structural Time Series and Prophet**

**C) CASE STUDY BRIEFING: Bike Sharing**

**D) Python - Directory and Libraries**

**E) Python - Preparing Data**

**F) Python - Exploratory Data Analysis**

**G) Dynamic Holidays**

**H) Python - Holidays**

**I) Prophet Model Parameters**

**J) Python - Prophet Model**

**K) Python - Regressor Coefficients with ChatGPT**

**L) Python - Cross-Validation**

**M) Python - Performance Metrics**

**N) Python - Fixing 2012-10-29 with ChatGPT**

**O) Python - Feature Engineering**

**P) Python - Parameter Tuning Set Up**

**Q) Python - Parameter Tuning**

**R) Python - Parameter Tuning Outcome**

**S) Python - Predicting The Future Set Up**

**T) Python - Tuned Prophet Model**

**U) Python - Forecasting**

**V) Python - Prophet Data Visualization with ChatGPT**

**W) Prophet Pros and Cons**

**XI) Section 11: Capstone Project: Prophet**

**A) Project Introduction**

**B) Python - Challenge Solutions Part 1**

**C) Python - Challenge Solutions Part 2**

**D) Python - Challenge Solutions Part 3**


# **I) Time Series Analysis and Forecasting with Python**

# **A) Time Series Analysis and Forecasting with Python**

You ever feel like you're the keto vanilla ice cream of the business world?
Boring and disappointing that you're still doing forecasting the old way like a rolled ice cream when everyone now talks about a -- pistachio flavor?

Well, not anymore.

In this course, you'll be at the forefront of time series forecasting from time series analysis and diving deep into advanced forecasting models.
I'll guide you through every step.

You will cover everything from simple exponential smoothing to cutting edge models like LinkedIn, Silver Kite, Prophet, and Amazon Cronos.

Think you're just a small fish in a big pond?

Let's change that with hands on exercise, real world challenges, and capstone projects.
You won't just learn, you'll do.

Revenues.
Electricity.
People.
Temperatures.

If it moves, we predict.

By the end you'll be forecasting trends with the precision of a sniper, turning your yearly revenue into your monthly income.

And who's going to lead you through this journey?

Well.

Hi.

I'm Diogo, your guide, mentor, and now ice cream coach with years of experience in data science and a passion for turning complex concepts into actionable insights.

I practice what I preach.

Jump in and let's make this year your breakthrough.

This is your moment.

Make it count.

Enroll now and let's crush it together.

# **B) Course Introduction**

I'm very happy that you picked this course to learn time series analysis.
It's the fifth year that this course has been live, and I truly think it's the most complete out there.

And there's absolutely no video that were in the, let’s say, first two editions that are currently live now.
So it’s currently always, always updating.

What I want to show you throughout this video is every single change since the 2025 version to now, the 2026 version.
This walkthrough will help you understand exactly what has improved and why it matters.

The key changes start with much crisper videos.
I really try to make sure that all videos are shorter and more focused.

We also now have deeper explanations and better explanations overall.
The goal is not just speed, but clarity.

Coding is now done with AI in, I think, most of the videos by now.
This allows us to spend more time looking at documentation and understanding what is what and why we are doing it, without the hassle of writing all the code manually.

Of course, we are very critical about it.
In the end, the code used is the one that I like, not necessarily the one that, in our case, Jamie and I are told to use.

We also add new projects and topics based on popular demand.
So the bottom line is, if you want something, tell me.

If there are a few people that want the same thing, I’ll make it happen.
And that’s a promise.

I also introduced an AI assistant.
You now have a time series analysis assistant that you can take anywhere with you and use in your job or during interviews.

This is a print screen of the assistant.
All the materials of the course are there—honestly, everything is there.

It’s just about asking the right questions.
And the assistant responds, I think, in my tone.

So if you like the way I talk or the way I use words, you’re going to like this as well.

Now, let’s look at what the course was in 2025.
At that time, we had five parts.

These were time series analysis, modern time series models and forecasting, deep learning for time series, and advanced content for time series.
We also had a Python appendix.

The Python appendix was for those who were not very familiar with Python but still wanted to learn.
There was a crash course at the end.

These were all the sections that were available back then.
I don’t want to spend too much time listing them, but they’re all there.

The question then becomes: what did we change?
This is where the remakes come in.

One important highlight is a new part called the Time Series Graveyard.
These are techniques that existed before but have not been updated by the teams maintaining them.

That’s why I’m calling it the graveyard.
This section also exists because of your requests.

In the past, I removed some sections.
Some people later asked if they could still access them.

So now, instead of removing them completely, they live in the Time Series Graveyard.
I wouldn’t advise you to focus on them, but it’s your choice.

This section includes everything that was either remade or added.
Anything marked as new represents a true addition.

We added new practice exercises for time series analysis.
We also added new intermittent forecasting and classification for time series.

For sections that are not marked as new, their Python tutorials were fully remade.
This was necessary because new changes in the libraries required updates.

As a result, the course is now 100% up to date.
Nothing is outdated or obsolete.

This is how the 2026 version of the course is now organized.
It consists of six parts.

These are time series analysis, modern time series, deep learning, advanced content, the graveyard, and the Python appendix.

The course is also now split between sections and projects or exercises.
Depending on your goals, you can choose how deep you want to go.

If you want structured learning, you can focus on the sections.
If you want to build a portfolio, you’ll find plenty of projects and exercises.

One important thing to note is that the course is actually shorter than it was one year ago.
This is despite having a lot of new content.

The reason is simple: the videos are crisper and better edited.
Using AI for coding also makes learning much faster.

This creates a smoother and more efficient learning experience.
That’s something I’m genuinely excited about.

The 2026 version is live.
So let’s get started.

# **C) Overview of the AI Time Series Assistant**

In the previous lecture, I gave you a link to the AI time series assistant, and in this video I want to give you a brief introduction to it. I’m going to call this version one of the assistant. I focused a lot on getting the core functionalities and the architecture right so that I can build on it later. There are quite a few things that could still be improved, and that’s exactly what I’m looking for when you share your feedback.

In this video, I want to introduce what it can do, what it cannot do, and the other actions available. It’s always going to be a time series course assistant, and it has a collapsible bar where you can see its capabilities, the core topics it covers, and also what it cannot do. For example, it cannot give medical advice, it cannot run code, and it cannot browse the web. You’ll also see a “ready to predict the future” message and some initial information explaining what it can and cannot do.

Let’s say we enter a message, for instance, “explain time series analysis,” and then click on send. You’ll notice that it shows when it’s thinking, both in the chat itself and at the top, where it indicates that it’s running. This assistant is going to complement the AI assistant from Udemy. There are a few things they have in common, especially when it comes to the videos themselves, because both have access to the transcripts. In fact, everything from the Udemy AI assistant comes from those transcripts, so if you want something specific to a given lecture, that’s where it’s useful.

For example, if you want a summary of a lecture, the Udemy assistant is very good because everything is directly linked to your current video. However, this time series assistant is not linked to your current video experience, so that’s one area where Udemy’s assistant is better. On the other hand, this assistant has some clear advantages. First, it’s always going to be up to date. For instance, we’re currently using GPT-5 already, whereas the Udemy assistant, based on things like its use of words such as “delve,” still feels very much like GPT-4.

At the same time, this assistant has all the course material loaded into it. That includes all the Python code, all the PDFs, and everything else from the course. So if you want something really tailored to the course—especially things you want to use, reuse, and take into your professional life—this is the one to use. It’s also not really tied to Udemy, which means you can keep it open and ask questions while you’re working on your own projects.

This tool is meant for you to use and reuse. Of course, there are many things that can still be improved. There’s an option to give feedback, and while I’d obviously love it if you love it, what I really care about is that you share what you like and what you would improve. This could be things like the colors, the layout, or any other aspect. Again, this is version one.

If you’ve been with the course for a long time—especially since the course originally dates back to 2021—you know that I’m always improving it. There will be a version two, version three, and so on, because I’m very focused on making this the best course possible.

While you’re here, you can also ask things like, “Give me the code for exponential smoothing,” press control-enter, and the assistant will think and respond. Alongside this, I want to emphasize that the feedback section is really important. Sometimes people ask for personal help through the feedback, but I can’t help you that way. If you need my help directly, you should always use the Q&A section. I answer questions there almost every day—there’s maybe one day a month when I don’t check it.

The Q&A is always the best way to reach out to me. I do gather feedback from this tool, but I don’t check feedback every day. I usually review it weekly or monthly, make sure everything is working, and then act on it after a couple of months to improve the tool. One example of feedback is that the Udemy assistant allows you to add images, which is really cool, whereas this one currently cannot. That’s something to keep in mind for future improvements.

You’ll also find all the code used in the course here. This is the same code we use throughout the course, and it should always be up to date because it reflects the latest course materials. If you ever find any issues, please let me know, because sometimes it can produce hallucinations, which I can’t fully control.

If at any point you don’t want to continue a conversation and you want to start fresh, you can reset the conversation. That will start a new session. Everything here is session-based only—there’s no database in the backend, and nothing is stored.

If this is something you love and use a lot, there’s an option to donate. This is completely optional, but I want to be transparent that this tool does cost money to run. The cost for one to ten people is basically nothing, but when you get to thousands of users, it becomes more significant. So if it genuinely makes a difference in your life, you can support it, but again, it’s absolutely optional.

At the end of the day, this is yours. Even if you hate me, you can bookmark it. There’s no login, and it’s an open tool that you can use and reuse. It’s here for you to keep. I personally find it very cool when people use my work, so if you do use it, let me know.

# **D) Diogo's Introduction and Background**

Hi there, I’m Diogo, and I’m thrilled to welcome you to this course. Before we get started, let me share a bit about myself so you know who’s guiding you through this journey.

I hold a Master of Science in Management from ESMT Berlin, one of the top business schools in Europe, where I specialized in business and analytics. My professional career has been centered around using data to solve complex business challenges. I’ve worked on projects ranging from sales planning involving billions of euros to A/B tests where companies had to invest hundreds of thousands of euros. I’ve truly been around the block and have had a lot of hands-on experience.

Beyond teaching, I’m also a startup founder. My company, which you can find at Join Betacom, aims to leverage the power of data to help restaurants around the world. We analyze vast amounts of data to provide actionable insights, such as optimizing menus or determining the best pricing strategies for their products. And if you’re interested in my startup, feel free to reach out and drop me a line.

I’m here not just to teach, but to genuinely support your learning journey. If at any point you feel that this course isn’t the right fit for you, don’t hesitate to reach out. I’m more than happy to guide you toward a learning path that suits you best, even if that means recommending other resources or courses that better align with your learning style.

Before we wrap up, I’d like to extend a personal invitation for you to connect with me on LinkedIn. Simply search for Diogo Alves de Rezende, and let’s keep the conversation going. I’m always eager to engage with my students and grow our professional networks together.

Once you complete this course and earn your certificate, I encourage you to share it on LinkedIn. I make it a point to repost these achievements and celebrate your success, just as I’ve done for many of my students. It’s a great way to gain visibility and start building your professional reputation in the business and analytics community.

My goal is for you to succeed and feel empowered to make a real impact. Whether you’re here to advance your career, pivot into a new industry, or innovate within your own business or company, I’m excited to help you achieve those dreams.

# **E) Unlimited Updates and Enhancements 2026**

Hey, before we start, let me talk about the current status of this course and what’s coming next.

All the content you’re about to dive into has been pre-checked and updated to ensure it’s relevant and accurate for 2026. I know how fast technology evolves, and that’s why I’m committed to keeping this course up to date with the latest developments and insights, so you can stay ahead of the curve.

That said, in a world that’s constantly changing, there’s always a chance that something might slip through the cracks. If you notice any outdated information or anything that doesn’t feel quite right, please don’t hesitate to let me know. Your input is invaluable in maintaining the quality and relevance of this course.

In addition, if there’s any specific content or material that you’d like to see added, I’d love to hear from you. Your suggestions help shape the future of this course.

In the next lecture, I’ll share a form where you can report anything that’s incorrect, offer suggestions, or request additional resources. This course is a collaborative journey, and your feedback plays a key role in making it the best it can be.

Thank you for being an active part of this learning community. I’ll see you in the next video.

# **II) Section 2: Part 1 - Time Series Analysis**

# **A) Time Series Analysis Overview**

Welcome to the time series analysis part of the course. This is where you take your data skills to the next level, learning how to make your data work for you and predict future trends with accuracy. Let’s get started.

Have you ever wondered how companies like Amazon and Netflix always seem to know what’s coming next? It all comes down to mastering time series forecasting. By the end of this section, you’ll be applying the same techniques to your own business and career, turning raw data into accurate, actionable predictions.

We’ll begin by understanding what time series data is and why it plays such a critical role in forecasting. You’ll work with real-world data in Python and set up all the essential tools needed for time series analysis. This foundation will allow you to build robust models and generate precise predictions.

Before forecasting effectively, it’s essential to know your data inside out. You’ll perform exploratory data analysis to uncover hidden patterns and structures in your time series. Through visualization and data manipulation techniques, you’ll learn how to interpret your data and prepare it properly for forecasting.

You may notice that certain trends repeat over time—this is known as seasonality. You’ll learn how to decompose your data into seasonal, trend, and residual components. This breakdown will help you model your data more accurately and significantly improve your predictions.

Understanding how past data points influence future values is another key concept. You’ll explore autocorrelation and partial autocorrelation to measure these relationships. This knowledge is essential for selecting, tuning, and validating your forecasting models.

Next, we’ll dive into a range of forecasting techniques. You’ll start with simple exponential smoothing, which works well for short-term predictions. From there, you’ll move on to the Holt-Winters method to handle seasonality. You’ll also work with more advanced models such as ARIMA and SARIMAX to capture complex patterns in time series data.

Imagine being able to predict stock prices or customer demand with confidence. You’ll develop these skills through hands-on exercises and real-world challenges. This section is not just about theory—it’s about applying these techniques to real data and understanding their results.

You’ll also explore what happens when forecasting goes wrong. By studying real case studies where predictions missed the mark, you’ll learn to recognize common pitfalls and improve your forecasting approach.

Finally, you’ll bring everything together in a capstone project focused on forecasting airline miles. You’ll prepare the data, build forecasting models, and evaluate their performance. By the end of this project, you’ll have a strong, practical example of your forecasting skills to showcase professionally.

You’re not just learning how to forecast—you’re mastering a powerful skill that can transform your career. See you in the next video.

# **III) Section 3: Python for Time Series Analysis**

# **A) Game Plan for Python for Time Series Analysis**

In this section, you are going to use Python as a tool to understand time series. This is not really an academic exercise. We’re going to work with real data, real questions, and we’re going to aim for real decisions.

We’re going to move fast, stay focused, and build intuition step by step. The goal is to truly understand time series, not just in theory, but in practice.

I can tell you already that you don’t need to be a master of Python. You only need enough to slice data, spot patterns, test ideas, and eventually explain what’s actually going on in the business. That’s really the core objective here.

If you already feel comfortable with Python, that’s great—you’ll be sharpening your instincts. If, however, you’re rusty or starting fresh, don’t worry. There’s a full Python crash course at the end of this course where you can practice more or even start from zero at your own pace.

# **B) **Load and Explore Data****

You are going to find this inside Python for Time Series, which itself is inside Time Series Analysis, and then Python for Time Series Analysis. There, you will find a file called lab starter one.

And here we go.

So we are here. I need to reconnect. This is basically taking our script and saying, “Hey, this is live.” Then I would mount the drive here. Of course, if you’re not using Google Colab, you don’t need to do this step. This is just for those who are using it. Whichever workspace you’re using, just go for it.

In case you’re using Colab, let me show you. You go to Drive → My Drive. This is so that you get the path. Inside My Drive, you’ll find Python Time Series Forecasting. Then inside Time Series Analysis, you will find Python for Time Series Analysis. I will copy the path here. Here we go. Control. Of course, nothing changes, and here we go.

So this is the part where I do this with you. Of course, it’s very specific for Colab. And now we can go.

Task number one is to import the pandas library. All right, let’s do this. Import the pandas library. It’s very simple:
import pandas as pd.

So pandas is the library, and pd is the short form that we are going to call. Cool. Shift + Enter. Task number one is done.

Then task number two is to load the train.csv file into a DataFrame called df, and preview the first rows. You’re going to find the dataset in your materials. It’s a very cool dataset. It has a lot of stores, the day of the week, and it’s a very rich dataset for us to explore and practice some Python.

And here we go. DataFrame. So I’m going to call it dataframe = pandas.read_csv("train.csv"). There was an extra dot here, so let me remove it.

Then I do dataframe.head(). In case you’re using the Jupyter functions of Colab or whichever it is, the comments are going to give quite a few things away. I would always encourage you to double-check and make sure that whatever you’re doing makes sense. In the end, or in case you can, just try to do it on your own. That’s always the best way to practice.

Of course, when you’re actually doing the work, go for it. Go fast. As for practicing, doing it yourself is a tiny bit better.

Now we look at the dataset. We have store, day of week, date, sales, customers, whether it was open, whether there was a promo, whether we have a state holiday, and a school holiday.

The dataset is from a very well-known store, Rossmann. It’s all in German, but it’s basically something that sells things like cleaning products, beauty products, and so on. And of course, at least in Germany, there’s something very specific: some stores are closed on Sunday. That’s something we need to consider.

But if we look at it here, let’s look at something. We see a dtype warning for column seven. Remember, Python indexing starts at zero, so we have columns 0, 1, 2, 3, 4, 5, 6, 7. So the state holiday column has something weird going on.

We can specify the dtype option on import or set low_memory=False. This is clearly something we need to investigate. So I’d say the first step is to look at information about the DataFrame.

To do that, we use dataframe.info(). This provides some information. We can see that, for instance, state_holiday is currently an object. And when we look at it, it has zeros, but it’s an object type, which means it has characters or letters inside. That’s something we definitely need to investigate.

This leads us to task number four. We’re going to print the unique values in state_holiday and their counts.

The way we do this is dataframe["state_holiday"].value_counts(). And we should get something. If you’re not using a notebook style environment, you’d have to use print. I personally have a big preference for notebook styles because of the input-output flow—it’s much easier to explore data this way.

So let’s remove the print because this looks way nicer. We can see that we have a zero, and then we have a different type of zero, and then we have A, B, and C. There is something odd here.

I’m going to assume that A, B, and C are types of holidays, but these two different zeros suggest that something went wrong during data entry. That’s how I’d interpret this right now.

This leads us to task number five. We are going to binarize this set. Instead of having zero, zero, A, B, and C, we’re going to have just zero and one. The A, B, and C values will become ones, and both types of zero will become zeros.

Let’s do this. I’ve found that the simplest way is to use the following format. We take dataframe["state_holiday"] and use .isin(["A", "B", "C"]). Then we convert this to integers using .astype(int). This makes sure that whatever matches A, B, or C becomes a one.

I find this approach intuitive because we’re simply saying whether the value is in A, B, or C. The astype part may not be intuitive at first, but it’s a very simple way to convert booleans into zeros and ones.

Cool. Then we run this and replace the column in the same variable. There was one extra quote there, so we remove it.

Now let’s check again by running value_counts() on state_holiday, just like before. Control + Enter, and here we go. We now only have zeros and ones.

# **C) Subsetting Stores and Basic Aggregations**

When we look at the data, we can see that we have stores, and because there are a lot of entries, it’s not immediately clear how many stores we actually have. It’s usually very helpful to understand the structure of the data you’re analyzing.

So we’re going to count this. Let’s make it nice and readable by printing it using an f-string. We print something like: “There are” and then the length of dataframe.store.unique(), followed by “stores”.

When we run this, we see that there are 1,115 stores. That’s quite a lot for the sake of our analysis.

As we start exploring the data or even doing data visualization, visualizing 1,115 stores is simply not reasonable. So there are two things we’re going to do. First, we’re going to randomize a subset to see how things work. Second, we’re going to simplify our analysis from here on out.

What we’re going to do is subset the data to ten random stores, using a random seed of 1502, and then store the result. Let’s do this.

We start with the original DataFrame and sample from the store column. Step by step, we first sample ten stores. When we run this, you’ll notice that every time we run it, the results are the same. That’s because we set a random state.

The random state makes sure that the results I get and the results you get are the same. It doesn’t matter how many times we run it. If you change the random state, the stores will change, but as long as we keep it at 1502, the output remains consistent.

So now that we have these ten stores, we take the DataFrame and filter it. We go to the store column and use isin() to check whether each row belongs to one of these selected stores. When we run this, we get a series of booleans—True and False.

The next step is to use this boolean mask to retrieve only the rows where the value is True. This tells the DataFrame to keep only the rows corresponding to those ten stores. When we do this, we can see stores like 24, 47, and others appearing in the filtered data.

At this point, we want to store this result. So we save it as dataframe_ten. We can quickly preview it to confirm everything looks correct.

One important best practice here is to use .copy(). This ensures that we’re working with a separate copy of the data and that any modifications we make won’t unintentionally affect the original DataFrame. This is just good hygiene when working with pandas.

Now that dataframe_ten is properly set up, we can move on.

Task number three is to compute the mean sales for each of the ten stores and sort them in descending order. Let’s do this.

We take dataframe_ten, group it by store, then select the sales column and compute the mean. Once we have the result, we sort the values in descending order.

When we look at the output, it’s not immediately obvious which store is the biggest just by scanning the numbers, so sorting helps. After sorting, we can clearly see that store number 4 has the highest average sales, while store 794 has the lowest.

There’s quite a big difference here—roughly a threefold difference in average sales. And remember, this is just within our subset of ten stores.

Now, one last thing that I find particularly interesting is the following: for each store, we want to find the row corresponding to the day with the all-time highest sales.

In other words, we want to know which specific day each store achieved its maximum sales.

To do this, we take dataframe_ten, group by store, and focus on the sales column. Instead of computing the mean, we now use idxmax(), which gives us the index of the row where sales were highest for each store. We could also look at idxmin() if we wanted the worst day.

Once we have this, we can optionally sort the results in descending order to see the biggest peaks first.

When we inspect the output, we see something remarkable. For example, store 864, which averages around 3.5K in sales, had one day where sales were close to one million. That’s roughly 300 times its average daily sales, which is absolutely insane.

# **D) Working with Datetime Index and Weekday Patterns**

Task number one is to set the date column as the index, and then preview the first rows.

To give a bit of insight here, when we work with time series data, we care about time. Setting time as the index enables us to manipulate the data in a much easier and more natural way. This is especially important for visualization, because Python then understands that we are dealing with data evolving over time. Without this, Python doesn’t really know that the data is temporal, and I’ll show you that in a moment.

For now, we set the date column as the index. The way we do this is by using dataframe_ten.set_index("date", inplace=True).

Let’s preview a few rows to see what we get. We can see that the date is now shown in a consistent format. That looks good.

But now let’s look at what Python actually thinks about our date column. If we go back and inspect the information, we see that the date is still an object. An object means characters or strings, not a real date.

So we inspect dataframe_ten.index. When we look at it, we see that it has a start, but it’s still considered an object. The name is “date”, but that name itself is meaningless—it’s just a label.

What we need to do is convert this into a proper datetime object. To do that, we use pandas and apply to_datetime on the index. Once we run this, Python now understands that this index represents dates.

At this point, Python knows that we’re working with time, and this allows us to properly explore and manipulate the data from a time series perspective.

Cool.

Now let’s look at something interesting. Even though we’re not heavily using the index yet, a useful exercise is to compute the average sales per day and per store.

Let’s do this.

We take dataframe_ten and use groupby. At this point, you can probably already see how relevant grouping is. We group by day of week and by store. Then we select the sales column and compute the mean.

Here we go.

Now, I know—I know—it’s not very easy to interpret this output just by looking at the table. That’s fine. That’s going to be addressed in task number four.

Before moving on, I want to highlight something important. Instead of using the existing day_of_week column, we could have used dataframe_ten.index.dayofweek, and the result would be exactly the same. In our case, the day-of-week variable already exists, which makes things easier.

But when you work with time series data, you can create all sorts of time-based variables directly from the index—day of week, month, year, and so on. This makes feature engineering very powerful and very simple once your index is properly set.

All right.

So now we move on to visualization.

We want to visualize the average sales per weekday and per store using a line plot.

The simplest way to do this is to reuse the code we already have. I’m a big fan of reusing code whenever possible.

The first thing we need to do is unstack the grouped result. Once we unstack, we can already see that this becomes easier to compare across weekdays—0, 1, 2, 3, and so on. Day 6 corresponds to Sunday.

Since we’re dealing with Germany, this makes sense because many stores are closed on Sundays. Still, even with this structure, it’s not very easy to visualize just by looking at numbers.

So the next step is to call .plot().

When we do that, the visualization becomes much clearer. Now we can actually see the patterns.

Looking at the plot, one thing immediately stands out. All stores have zero sales on Sunday except for one. This is something interesting and consistent with what we saw in the table.

However, there’s another issue. The magnitude of sales differs a lot between stores. Some stores reach sales around 10,000, while others are much lower. This makes comparisons tricky, because the scale dominates the visualization.

And this is where the exercise really starts.

I want you to start thinking about this: when we want to understand how sales evolve during the week, we’re essentially looking at a seasonal pattern. To truly compare these patterns across stores, we should also start thinking about standardizing the data.

# **E) Standardizing Sales and Comparing Weekday Patters**

In order to properly compare behavior across different stores, we need to standardize the data. Standardization ensures that there is a common denominator among the stores, so that we are truly comparing apples with apples.

The standardization formula itself is very straightforward. It works as follows: we take the value, subtract the average, and divide by the standard deviation. This is the classic z-score formulation.

So this is step one. The main challenges here are twofold. First, we need to build a function. This is a very standard Python task and a great way to practice writing functions. Second, we need to apply this function correctly to our data, making sure that we zoom in on each store individually. In other words, we want to compute the mean and standard deviation per store and standardize sales within each store.

This brings us to task number one.

We need to define a function that standardizes a numeric series using z-scores, meaning value minus mean divided by standard deviation. Let’s do this.

We start by defining a function called standardize, and we pass a series as the input. Inside the function, we return the series minus the series mean, divided by the series standard deviation. This is very straightforward.

Now that the function is defined, we need to apply it to our data.

What we need to do is take our DataFrame and group it by store. This ensures that we are focusing on one store at a time. Then we focus on the KPI we care about, which is sales.

When we do this, we group by store, select sales, and apply a transformation using our standardize function. At first, it may show as a generic grouped series because we haven’t applied the transformation yet. But once we apply transform(standardize), the values are standardized per store.

After running this, we can see that the values are now standardized. This allows us to really work with and compare patterns across stores much more easily.

Recall that standardized values are centered around zero, and most values typically lie between -2 and 2. This makes interpretation and comparison much more intuitive.

This leads us to task number three. But before we do that, we need to store the standardized values.

We assign the standardized sales back into our DataFrame using .loc across all rows, and we create a new column called sales_standardized. The name “STD” here refers both to standard deviation and to the normalization itself.

Once we run this, the new column is created. We can quickly preview a few rows using .head() to confirm that everything looks correct.

Now we move on to computing the mean of the standardized sales.

We take dataframe_ten and group by day of week and store. Then we select the sales_standardized column and compute the mean. This gives us the average standardized sales per weekday and per store.

This naturally leads us to task number four.

We take the result, unstack it, and then plot it. This allows us to visualize the standardized weekday patterns much more clearly.

However, there are a few issues here. Nine out of ten stores have no sales on day seven, which is Sunday. This completely skews the visualization. From an analysis perspective, this is not ideal because zeros don’t really carry meaningful information in this context.

One thing that clearly stands out is the brown series, store 353. It shows extremely strong seasonality on Sundays. That makes sense, because if most other stores are closed, this store benefits disproportionately.

This is something we can infer from the data.

If we look at the other series, we see a different pattern. Sales tend to decrease day by day until Saturday, showing a clear weekday seasonality. That’s one conclusion we can draw.

At the same time, Sunday completely distorts the picture and makes it difficult to analyze the remaining patterns properly. We clearly need to clean this up further.

Still, let’s continue exploring.

Let’s take a closer look at Sundays specifically. Are stores always closed? Is there even a single Sunday where some of them are open? The same question applies to the other stores as well.

These are things we can visualize, explore, and then use to draw our own conclusions.

# **F) Analyzing Sales on a Specific Weekday**

I have to say, I only listed one task here, but this could have been done in multiple ways. What we need to do is filter the data for day seven and then visualize it. So this will be step number one and step number two at the same time.

First, let’s import what we need for visualization. We import matplotlib.pyplot as plt. That’s step number one.

Next, we create a new DataFrame called dataframe_d7. We start from dataframe_ten. There are many ways to do this. One way is to filter where the day of the week is seven and then make a copy. That’s one valid approach.

Another option would have been to use the index directly, since we’ve already set it to datetime. That’s another possibility. In this case, we’ll stick with using the existing variable and filtering based on the day of the week.

Now we move on to plotting.

We start by creating a figure using plt.figure(). We set the figure size to 12 by 6, which is a fairly standard size.

Next, we loop through the data. For each store and its corresponding data in dataframe_d7 grouped by store, we plot a line. We plot the date, which is stored in the DataFrame index, on the x-axis, and sales on the y-axis. For each line, we set the label using an f-string so that it shows the store number.

This loop allows us to plot each store’s Sunday sales on the same chart.

We close the parentheses and then display the plot. This is already enough to generate the visualization. It’s also a nice introduction to matplotlib if you’ve never used it before, because it shows the basic structure of creating a figure, plotting data, and labeling it.

Now, here’s a cool thing that you might or might not have noticed. Let’s quickly look at dataframe_d7 itself. You’ll see that the earliest dates are from 2015, and the later dates are from 2013. In other words, the data is reversed.

However, because Python knows that we’re dealing with dates, this doesn’t matter at all. When we plot the data, everything is automatically ordered correctly along the time axis.

That was just a quick side note.

Now let’s clean up the plot a bit. We add a title, “Sales on Sunday”. We label the axes with date and sales. We also add a legend so that we can identify each store.

I also like to add plt.tight_layout(). This usually tidies things up a bit and makes the plot easier to read, which I personally appreciate.

When we look at the plot, I feel it’s a bit too tall, so we reduce the figure height from 6 to 5. That small adjustment makes it look better.

Now, what do we see?

We see that every single store except one is always closed on Sundays. There are no exceptions at all. This tells us that by including Sundays in our analysis, we’re potentially introducing noise—especially when we standardize the data.

For store 353, there is one Sunday where the store was closed, which explains a drop. But aside from that, this store has recorded sales on every single Sunday, and those sales are growing over time. That’s definitely interesting to observe.

And that’s it.

This is how we do a quick visualization to explore a specific pattern in the data. I would say this exercise is on the more difficult side, because it includes a for-loop and multiple steps. But if you understand this, you’ll be able to handle virtually every visualization throughout this course.

# **G) Removing Outliers and Re-Assessing Intra-Week Seasonality**

Our conclusion so far is that we clearly have store 353, which behaves very differently from the others. There’s really no point in comparing it directly with the rest. At the same time, we also have day seven, which is Sunday, and this day is kind of “poisoning” our data because it introduces a lot of noise.

Because of this, task number one is to remove all observations for day of week seven and for store 353. Doing this will give us a much cleaner DataFrame, allowing us to continue exploring seasonality in a more meaningful way.

Cool, let’s do this.

We start from dataframe_ten. We want to keep only the rows where the day of week is not seven and the store is not 353. Since we have two conditions, we need to combine them.

First, we filter dataframe_ten where the day of week is not equal to seven. Then, we add a second condition where the store is not equal to 353. Finally, we make a copy of the result to avoid any unwanted side effects.

We store the result in a new DataFrame called dataframe_clean.

Once that’s done, we can take a quick look using .head(). This preview doesn’t show anything dramatic, because we’ve removed rows rather than added new ones. The main purpose here is simply to confirm that we still have data and that the operation worked as expected.

Now that we have a clean DataFrame, the next step is to re-standardize the sales column within each store, but this time using dataframe_clean.

We essentially repeat the same process as before. We take the sales column, group by store, and apply the standardization transformation. The result is stored again in the sales_standardized column.

If we preview a few rows using .head(), we can immediately see that the standardized values have changed. For example, values that were previously around 1.5 and 1.1 are now closer to 1.65 and 1.16. This makes sense, because we’ve changed the underlying data by removing Sundays and the outlier store.

This brings us to task number three.

We now take dataframe_clean and group it by day of week and store. Then we select the sales_standardized column and compute the mean. This gives us the average standardized sales per weekday and per store, using the cleaned data.

As mentioned earlier, we can store this result. Let’s call it intra_week.

Now we move on to task number four, which is visualization.

To visualize the intra-week seasonality, we take intra_week, unstack it, and then plot it. Compared to before, the view is much clearer. Earlier, everything almost looked like a straight line. Now we can actually see meaningful differences and patterns.

We can also customize the plot directly within pandas. For example, we can set the figure size to 12 by 6, add a title such as “Intra-week Seasonality of Standardized Sales”, and adjust the layout. If we want to remove or fine-tune certain axes, we can do that as well.

Reducing the figure height slightly—from 6 to 5—makes the visualization a bit cleaner and easier to read.

Now let’s interpret what we see.

There’s one store, store 267, that shows very strong Saturday performance, which is quite unique. In general, Thursday to Saturday seem to be the strongest days for most stores. From Monday to Tuesday to Wednesday, sales tend to decrease, and then there appears to be some stabilization.

Overall, Monday is often the strongest day, Saturday the weakest—except for that one store. We also see a general decrease until Wednesday, followed by a flatter pattern for the rest of the week.

One important takeaway here goes beyond just seasonality. This exercise demonstrates the process of time series analysis. We take a sample of data, analyze it, and identify problems. Then we ask questions like: “What’s causing this distortion?” In this case, Sunday was the issue, so we removed it.

Next, we notice a store that behaves completely differently. That store clearly belongs to a different category and should be analyzed separately. Ideally, we would later compare stores that are open on Sundays with those that are not, because they may not be directly comparable.

This is how we move step by step toward insight: analyze, detect anomalies, clean the data, reanalyze, and visualize the results. And ideally, at the end of this process, we present clear visualizations that support our conclusions.

# **H) Analyzing the Impact of Promotions**

One of the most interesting questions—especially in revenue planning or demand planning—is understanding the impact of promotions on revenue. This is something we are going to explore slowly here, just a little bit, from a more data-exploratory perspective.

Let’s start with the first task. We want to compare the average sales on promotion days versus non-promotion days. To do this, we use a function you’re probably already tired of by now: groupby. We group by the promo variable and then compute the mean. This immediately gives us something concrete to compare—average sales when promotions are active versus when they are not.

At this point, you might notice something odd happening, especially if you’re working in Colab. You may realize you’re doing something wrong, or Colab might not be cooperating properly. When that happens, it’s totally fine—and often necessary—to restart the runtime and rerun everything from scratch. That’s exactly what we do here.

After restarting and rerunning the pipeline, we see the results clearly. Average sales on promotion days are around 7991, while on non-promotion days they are around 4.4k. That’s already interesting. Here, I’m intentionally switching back to using the full data frame instead of one of the cleaned or filtered versions from before. For this particular analysis, working with the complete data frame feels more natural and helps illustrate a few additional concepts.

Now we move to task number two. Instead of looking at promotions globally, let’s break this down per store. We want to compare promotion versus non-promotion sales at the store level. To do this, we group by both store and promo, select sales, compute the mean, and then unstack the result. This gives us a table where, for each store, we can directly compare average sales with and without promotions.

We store this result in a new object called promo_uplift. This data structure now contains the mean sales per store, split by promotion status. We can inspect it using .head() or similar methods to confirm everything looks correct.

Task number three is where things get more interesting. For each store, we want to compute the relative uplift in sales during promotion days compared to non-promotion days. The idea is simple: we take the difference between promotional and non-promotional sales and divide it by the non-promotional baseline. In other words, (promo − non_promo) / non_promo.

This relative format is very useful because it puts all stores on a common scale, similar to standardization. Even if stores have very different absolute sales levels, relative uplift allows us to compare them fairly. We compute this relative difference and add it back to our promo_uplift structure. After that, we can again inspect the results to make sure everything looks reasonable.

At this stage, a very natural and insightful question arises: in which stores is promotion actually making a meaningful difference? To answer this, we look for the stores with the largest relative uplift. We take the computed uplift values and use nlargest(5) to identify the top five stores in terms of promotional elasticity.

This reveals some very interesting results. For example, stores like 198 and 607 stand out, and one store—store 108—shows an uplift of around 2.25x. That’s a very significant increase and clearly indicates that promotions are extremely effective in that store.

Task number five is simply the flip side of this analysis. If we can find the highest promotional elasticity, we should also look for the lowest. Using nsmallest(5), we identify the stores where promotions have little to no impact on sales. This immediately raises important business questions: what’s happening in those stores? Why don’t promotions work there?

Of course, there are many possible explanations. There could be other factors we’re not accounting for, such as how often promotions are run, when they are run, or how large they are. Promotions aren’t just a binary yes-or-no variable. They have magnitudes—depth of discount, breadth of products included, and timing over time. These are what we call confounding factors, and they can strongly influence the observed effect of promotions.

Overall, this exercise shows how we can start uncovering meaningful insights step by step using relatively simple exploratory techniques. If you’d like to see more exercises like this—especially ones that dig deeper into real business questions—just let me know, and I’d be very happy to create them for you.

# **IV) Section 4: Introduction to Time Series Forecasting**

# **A) Game Plan for Introduction to Time Series Forecasting**

Welcome to this video where I’ll make this game plan for our introduction to time series forecasting.

The thing is that we are going to talk about this new type of data. Well, not really new, but it could be new for you, which is time series data. Have you ever heard of it?

It’s all about looking at how things like stock prices or the weather change over time. And guess what? We are going to tackle it with Python. So don’t worry if you are new to this, I’ll guide you every step of the way, and I promise you it will be fun.

First off, let’s discuss what time series data is and what this really means. Imagine that you’re keeping track of your daily coffee spending. That’s time series data. Anything that records how things change day by day or month over month—that is time series data.

Now the cool part: we are going to use Python for all of this. We’ll start with the basics, and you’ll be amazed at how much you can do with just a few lines of code. I think you’ll be quite happy with how much we can achieve just in this section, which is an introduction.

We’ll get our hands dirty by sorting and playing with data. It’s really like putting a puzzle together, figuring out how to do it. We’ll look at patterns and trends. For instance, can we understand why Bitcoin’s price skyrocketed last week? Or why do sales dip every July?

You’ll learn how to spot these patterns, and of course, we’ll draw some graphs. Not just any graphs, but ones that really tell a story. You’ll learn how to turn numbers and dates into cool visual stories that anyone can understand.

Ever wonder if you can predict stuff like stock prices? We’ll talk about that too. It’s a bit like trying to guess the end of a movie, but with data and trends.

And to wrap it up, we’ll look at real-world examples where forecasting didn’t go as planned. It’s really about looking at someone else’s mistakes—and trust me, there’s really a lot to learn there.

# **B) What is Time Series Data?**

In this video, we are going to dive deep into time series data, and I’m also excited to introduce you to a particularly intriguing data set: the Bitcoin price data.

This data set will be the central focus of our tutorials. It tracks the daily price of Bitcoin spanning nearly a decade, from 2014 to 2023.

Now, why Bitcoin price data? Bitcoin, as a pioneer of cryptocurrencies, has a rich and dynamic data set. Its market is known for volatility, rapid price changes, and significant trends, making it an excellent subject for time series analysis.

By studying this data set, you’ll gain insights not just into Bitcoin’s price movements, but also into broader financial market dynamics and investment behavior.

Now let’s understand the essence of time series data. Time series data is unique, and I really want you to understand why. It’s like a chronological story, where each data point is a moment in time, neatly lined up from the oldest to the newest.

You’ll often find this data captured at consistent intervals—think daily, weekly, or monthly snapshots of data.

If we explore its wide-ranging applications, time series data isn’t just about finance. Imagine it being used in weather forecasting, where it helps predict rainfall or temperature trends. In economics, it’s crucial for analyzing GDP growth, and in healthcare, it’s used for monitoring patient heart rates over time.

Now let’s go deeper into the unique statistical tools used in time series analysis. Time series analysis introduces some fascinating statistical concepts. Together, we’ll look at autocorrelation—understanding how a data point is related to its past.

In fact, time series data is very unique because we use data from the past to predict the future. Moreover, we’ll discuss seasonality, identifying patterns that repeat over time. These concepts are key to accurate forecasting and trend analysis.

As we step into this journey through time series data, I encourage you to think about how this knowledge could enhance your own projects. What questions do you have? Do you see any practical applications for what you are learning?

Feel free to reach out in the Q&A or the student communities. I’m here to help. Until the next video—have fun!

# **C) Python - Libraries and Data**

Welcome to our first Python tutorial.

In this video, we are going to set up everything when it comes to our Python script. I have stored my materials here in my drive because we are going to use Google Colaboratory. You are welcome to use anything else, but I would strongly recommend that you do this with me.

That said, the materials are there for you, and of course, your IDE is your IDE. If you are new to Python, especially, I strongly encourage you to follow along, but I’m always here to help in case you have any issues.

If you decide to pick something else, you’ll find in the materials that there are several folders. We’re going to start with the time series analysis, and then with the introduction to time series analysis. We’re going to click on New, then More, and then Google Colaboratory—either create and share or just create.

If you have not shared it with anyone, it will just be “Create.” In case you’re having issues finding all the materials, they are in lecture number three or four of this course. You’ll be able to find everything there. There’s a link to my website, and on my website, there’s a big download button.

Now I’m going to change the title to Introduction to Time Series Analysis. You’ll notice that there are several things here. This is the table of contents, and we’ll build on this by creating headings and subheadings.

There’s also something here called Secrets, which we’re not going to use in this course. Then there’s Files—this is how we connect to Google Drive, and we’re going to use this extensively.

Let me mount the drive already. Connecting to Google Drive is called mounting Google Drive. You click on the drive icon, connect to Google Drive, and sometimes you’ll get a piece of code that you need to run.

If you’re wondering what that piece of code might be, it’s:

from google.colab import drive
drive.mount('/content/drive')

This is the code used to mount the drive. It doesn’t happen every time that you get this piece of code, which is why I’m sharing it—especially since this is the first time.

Once the drive is mounted, you’ll see something called drive. You click on drive, then go to My Drive. I have personally stored the folder on the homepage of the drive—the main part of the drive.

You’ll see Python Time Series Forecasting, then Time Series Analysis, and then Introduction to Time Series Analysis. You go to the three dots, copy the path, and then click back on code.

Now we’re going to set the directory. This is how we always connect to that specific directory, where in this case we have a couple of CSVs that are going to be our datasets. To do that, you use %cd to change the directory, and then you paste the path using Ctrl + V.

Here we go.

Let me also add a section here—this is going to be the Setup. To see this, we go to the table of contents, and you’ll notice that Setup now appears there.

Next, I definitely want us to import libraries and data. For importing libraries, we’ll import pandas as pd, numpy as np, and matplotlib. For now, this is going to be it. Each time we require a specific function or library, I’ll import those later. This way, we’re building everything step by step.

Then we’re going to load the dataset using pandas.read_csv. We’ll start with the Bitcoin price dataset. This is a good dataset for daily data—bitcoin_price.csv.

Then we use DataFrame.head(). This allows us to preview the first five rows of the data.

Here we go. We have seven KPIs. We have the date, starting from 2014, and then we have open, high, and low. Open is the price at the start of the day, high is the maximum, and low is the minimum.

Then we have the close, which is the value at the end of the day, and the adjusted close. The adjusted close accounts for things like dividends or splits. In this case, there’s nothing like that, but the adjusted close is the value we typically use because it’s the most relevant indicator of the value at the end of the day.

Finally, we have the volume, which is the trading volume for that specific day.

Now, what I want us to do is to close this video. In the next one, we’re going to return and explore something called the time series index. We’ll see how to do it, and that’s going to be it for now.

# **D) Python - Time Series Index**

In this video, we’re going to focus on the Time Series Index. This is a very important topic because when you’re dealing with the index—and you actually make the index out of dates—it makes your life much easier when it comes to time series analysis. Whether it’s plotting, forecasting, or the analysis itself, everything becomes quite straightforward.

Not all libraries that we are going to work with require this step, and I’ll show you which libraries do and which ones do not. However, for the sake of doing some pre-analysis, the main libraries that we work with—such as pandas, matplotlib, and later on, Statsmodels—use the date as the index. This is why it’s very important that you master this part of time series analysis.

What we’re going to do is convert the date column into a datetime object and then set the date as the index. These are two different steps, so let’s give it a go.

The first thing we need to focus on is the date format. In our case, the date is already in the correct format, which is year-year-year-year, month-month, day-day. This format is expressed using %Y-%m-%d. The capital Y means we are using four digits for the year, followed by the month and the day.

We can even see that tools like Google Gemini suggest using pandas.to_datetime. We set the column as date and specify the format. For our dataset, the date is already in the correct format, but it’s still a good practice to check.

Let me check this one. Okay, this one is fine as well. But in the next section, you’ll see that the date is not in the correct format. The bottom line is that you can always handle this by specifying the current format, and pandas will transform it into the correct one.

If we run DataFrame.head(), nothing is really going to change visually. The important step is setting the date as the index. To do that, we use set_index, specify the date column, and use inplace=True. Inplace means that the change is applied directly to the DataFrame, and the date column itself is removed.

Let me run this. Here we go—now the date is our index.

There are a few different ways to do this, and I’ll show you another one in a moment. But first, I want you to see what this allows us to do. For example, if we want to select a specific day from our DataFrame, we can use .loc and reference the index directly.

We specify a date, such as 2021-11-09, and when we run it, we get all the values for that specific row.

Now, another thing I want to show you is that we can also set the index at the time of importing the data. Let me call this DataFrame one. We again use pandas.read_csv with the same Bitcoin price CSV file. This time, we specify index_col='date' and parse_dates=True. This ensures that the date is parsed correctly and set as the index right away.

This approach gives us exactly the same result.

Before we close this video, I want to show you one last thing. You can always resample time series data to a different time granularity.

What does this mean? Imagine we have daily data, but daily values might be too noisy. Maybe weekly data would make more sense for our analysis. This is completely possible.

To do this, we take the DataFrame, use .resample(), specify 'W' for weekly, and then apply an aggregation such as .mean(). As you can see, this gives us weekly data.

You could also use the median, the maximum, or any other aggregation you can think of. If you can think of it, it probably exists—it’s just a matter of knowing what to apply.

Instead of weekly, you could also resample monthly, quarterly, or yearly. It’s really about deciding how to transform daily data into another time granularity.

This always depends on the business requirement. Do you care about daily data? If yes, then keep it. If weekly data is more relevant to your business problem, then go with weekly. It’s always about defining the business problem first, and then letting the analytics follow.

# **E) Python - Exploratory Data Analysis Part 1**

In this video, we’re going to talk about exploratory data analysis. Here we go.

It’s very important that we always, always understand the data that we are working with. In this case, we want to understand its cycles, its growth, and how it is developing. This is one of the fundamental things that really distinguishes a good analyst, scientist, or engineer from a mediocre one.

It all comes down to purpose. What are we doing? What is this for?

I’m going to cover a couple of things that are extremely simple, but also very common in time series analysis—one of them being the rolling average. This is particularly important when we start doing feature engineering, such as combining variables or averaging them. In this case, rolling averages create smooth curves, which are very useful for analysis.

Let’s do it.

Our first step is to generate a seven-day rolling average for the closing price. This is very straightforward. The way we do it is by taking the DataFrame, selecting the close column, and then applying the rolling method.

We specify a window of seven days, and then we decide how we want to aggregate—here, we use the mean. Once we do that, we get the rolling average. Of course, the first few values are missing because we don’t yet have a full seven-day window, but after that, the values appear.

What I think is particularly relevant is plotting two things together. Before doing that, I want to store this rolling average in a variable. I’ll call it the seven-day closing average. Now this value is stored and ready to use.

Next, I want to plot both the closing price and the seven-day rolling average together. Once we do that, we can clearly see the difference between the raw data and the smoothed curve.

Let me make sure we actually show the plot. Sometimes, especially in Jupyter environments, plots don’t display correctly unless we explicitly call plt.show(). This helps avoid issues with axis labels and rendering.

You can see quite a lot of data here, so let’s zoom in. A very easy way to do this is by using .loc and specifying a year, for example 2023.

Now we can clearly see what’s happening. The orange line represents the seven-day closing average. You’ll notice that it’s always slightly delayed compared to the actual closing price. This makes sense—it takes time for new values to influence the average.

That’s exactly how rolling averages work. You can shift values if you want, and I’ll show shifting later, but that’s not the nature of a rolling average. The more days you include in the window, the more delayed the curve becomes. Seven days is a relatively small window, so the delay is manageable, but it’s still there.

The next thing I want to show you is also related to rolling, but it’s more about aggregation. Let’s say we want to find the month with the highest average closing price.

To do this, we resample the data on a monthly basis and calculate the mean. Then we focus on the close column and look for the index of the maximum value. This gives us the month with the highest average closing price.

You might see a warning that the letter M is deprecated and that you should use ME for month-end, but the logic remains the same. Once we run this, we get our result.

This is one way to explore the data. You can also preview different parts of the dataset. For example, if you want to see the last five rows, you can use DataFrame.tail(). This is essentially the opposite of DataFrame.head().

And that’s it for this video.

You might notice that I mention things like data manipulation, seasonality, autocorrelation, and partial autocorrelation. Realistically, this entire section is dedicated to giving you techniques to get to know your time series better.

The goal is that, by the end, you have a toolbox of methods you can use to understand any time series you’re working with.

# **F) Python - Exploratory Data Analysis Part 2**

In this video, we are going to continue with the exploratory data analysis.

One thing that’s very interesting in time series analysis is looking at day-over-day or period-over-period comparisons. For example, you take one day and then look at the percentage change compared to the previous one. This is exactly what we’re going to do here.

We’re going to compute the percentage change for the closing price variable. There is a built-in function in pandas that allows us to do this very easily.

We always start by selecting our variable, so we take DataFrame['Close']. Then we apply the percentage change function, which is already available. When we do this, we immediately get the percentage change for each day.

The first day is, of course, not available because we don’t have a previous day to compare it to. Essentially, the calculation works by dividing the current day’s value by the previous day’s value. For example, one day divided by the day before it. This type of calculation is extremely common in financial data and is very helpful because it connects the dots between consecutive periods.

Personally, I like to multiply this value by 100 so that it’s easier to visualize as a percentage. I find that having the values expressed this way makes interpretation much more intuitive.

We then store this under a new column in the DataFrame, something like daily_returns_100pct, so it’s always clear that the values have been multiplied by 100.

Once we have this, we can start exploring the data. For instance, we might want to check which days had more than a 10% change in price.

To do that, we take the DataFrame and filter it by selecting the daily returns that are greater than 10%. This gives us all the days where the price increased by more than 10%.

But then you might think—what about days where the price dropped by more than 10%? Those are not included in this filter.

The solution is very simple. We take the absolute value of the daily returns and then check which values are greater than 10%. This way, we capture both large positive and large negative changes.

Now we’re looking at all the days that experienced more than a 10% change in either direction. For this dataset, you can see that there are around 97 days with more than a 10% change.

That’s massive and clearly shows how extremely volatile this data is.

That’s it for this video. In the next one, we’re going to focus on data visualization and explore this dataset visually.

# **G) Python - Data Visualization**

In this video, we are going to explore data visualization. Of course, we have already done some visualization earlier, but now I want to explore a few additional parameters that you can add to make your visualizations cleaner and more visually appealing.

Let’s take a look.

We will start with the very basics, as always. For example, plotting the daily closing price. We simply go to our DataFrame, select the close column, and call .plot().

We’ve done this before, but let’s improve it a little. For instance, we can add a title. We just specify something like “Daily Closing Price”, and now the plot looks more informative and polished. This is one simple way to enhance your visualization.

Next, let’s explore something slightly different. We’ve already looked at rolling averages earlier, but now let’s apply it to a different variable.

We’ll create a 30-day rolling average for volume. To do this, we start with our DataFrame, select the volume column, apply a rolling window of 30, and then compute the mean. This gives us a new variable representing the 30-day rolling volume.

Now let’s try to plot both the closing price and the 30-day rolling volume together.

If we try to plot them directly on the same chart, it doesn’t really make sense because the magnitudes are completely different. The volume values are much larger, which causes the closing price to appear almost like a flat horizontal line.

So how do we fix this?

The solution is to use two different y-axes.

We start by plotting the 30-day rolling volume and enabling the legend. Then we add plt.show() to display the plot properly.

Next, we plot the closing price on a secondary y-axis. We specify secondary_y=True and also enable the legend. It’s important to remember that True must be capitalized so that Python recognizes it as a boolean value.

Finally, we label the y-axis for the closing price. This is how we correctly visualize two variables with very different magnitudes on the same plot.

Now we can actually start connecting the dots.

By looking at the visualization, we can begin to see potential relationships between volume and price. While it’s sometimes difficult to interpret visually, in general, when volume goes up, the price also tends to go up. This kind of relationship becomes much clearer when both variables are plotted together with separate axes.

As a final step, let’s briefly look at correlation.

We can compute the correlation between the closing price and the 30-day rolling volume. This gives us a numerical measure of how strongly these two variables are related.

If we print the results properly and use the .corr() function, we can see that the values are the same, just formatted slightly differently depending on how we display them.

It’s important to note that this correlation is the Pearson correlation, which is the default in pandas. Pearson correlation is often not ideal for time series data, but for our purposes, it still gives us a good intuition.

Just as an FYI, you might want to explore Spearman correlation, which is more commonly used for time series. However, I don’t want to go down the rabbit hole of statistics here.

Even with Pearson correlation, we’re still about 90% correct, and the overall conclusion does not change: the two variables are strongly connected. In fact, we don’t even strictly need the correlation value here, because the visualization already tells us the story.

That’s it for data visualization.

In the next video, I want us to focus on data manipulation. This is a specific topic that I’d like us to dive into next.

# **H) Python - Data Manipulation Part 1**

The first step is to identify missing values. The way we do this is by using our DataFrame, calling isnull(), and then applying sum(). This allows us to count the number of missing values per column.

Once we do this, we can clearly see that some of the columns we are working with contain null values. At that point, the real challenge becomes: how do we actually fill these missing values?

There is no single correct solution here. The approach you choose very much depends on the context of the problem. What I want to do is show you a few common techniques, and then based on your own use case, you can decide which one makes the most sense.

The first technique is to fill missing values using the next available observation.

For example, let’s say Day 29 is missing, but Day 30 has a value. In this case, Day 29 would take the value of Day 30. This approach is very useful when you want to maintain continuity in the data, or when you believe that something went wrong on a specific day and the next value is a reasonable replacement.

Again, this is just one option, and whether it makes sense or not depends entirely on your situation.

Let’s take a concrete example: the 30-day rolling volume.

We take the 30-day rolling volume series and apply fillna() with the method set to backward fill (bfill), and we try to do it in place.

When we do this, we may encounter an error or warning. This is actually an important topic to address.

The warning says that a value is trying to be set on a copy of a DataFrame or Series through chained assignment using an in-place method. It also mentions that this behavior will change in pandas 3.0.

This is one of the reasons I’m explicitly recording and explaining this, because these kinds of issues can easily show up in real projects. At the moment, we’re using pandas version 2.2, but future versions will be stricter.

The warning essentially tells us that instead of modifying a column in place using chained assignment, we should reassign the result back to the column explicitly.

So rather than using an in-place operation, we should do something like:
assign the filled result back to the DataFrame column itself.

When we try to follow this approach, we might run into another issue. For example, using method= inside fillna() on a Series may result in a deprecation warning. The message tells us that using method is deprecated and that we should instead use ffill() or bfill() directly.

This is another example of pandas evolving over time and why it’s important to keep your code up to date.

So instead of using fillna(method="bfill"), we directly use bfill().

Once we do this correctly, everything works as expected. This is a good example of how you can take a warning message, read it carefully, and fix your code step by step.

At this point, it’s worth clarifying the difference between backward fill (bfill) and forward fill (ffill).

Backward fill means that a missing value is filled using the next available value. This matches the earlier example where Day 29 takes the value of Day 30.

Forward fill does the opposite: it fills the missing value using the previous available value. In that case, Day 29 would take the value of Day 28, assuming it exists.

In practice, I personally tend to use backward fill more often, because in many time-series scenarios it makes more intuitive sense. But again, this depends entirely on your data and your assumptions.

Another common technique for handling missing values is interpolation.

Let’s take a different example: the 7-day closing average. We already know that rolling averages naturally introduce missing values at the beginning of the series.

Instead of forward or backward filling, we can interpolate these values.

To do this, we take the 7-day closing average series and call .interpolate() on it. We no longer use inplace=True, and the operation completes successfully.

This is how interpolation works in practice.

Whenever you use interpolation, it’s very important to check the documentation. I’m a big fan of going through the pandas docs, especially when working with something new or nuanced. It does take time to get used to, but it’s absolutely worth it.

In this case, we are working with a Series, which is one-dimensional data. A DataFrame would be two-dimensional, but that distinction doesn’t change much for interpolation in this scenario.

Looking at the pandas Series interpolate() documentation, we see that the default method is linear interpolation.

Linear interpolation assumes that the data points are equally spaced, which is true for daily time-series data. In most cases, linear interpolation will be sufficient.

The key difference between interpolation and forward or backward fill is that interpolation looks at both sides of the missing value. It considers the value before and the value after, and then computes a reasonable estimate in between.

Forward fill and backward fill, on the other hand, only look at one side of the equation.

To summarize:

Forward fill (ffill) uses the previous value

Backward fill (bfill) uses the next value

Interpolation uses both before and after values

Each approach has its place, depending on the problem you’re trying to solve.

That’s it for now. I spent some extra time here because missing values are a critical part of time-series data manipulation.

We’re going to come back to data manipulation and explore it further in the next video.

# **I) Python - Data Manipulation Part 2**

In this video, we’re going to focus on the index and also on feature engineering. Let’s get started.

The first thing we’re going to do is focus on the index. If we fetch the index from our DataFrame, this is as simple as calling dataframe.index. When we do that, we get all of the dates associated with our time series.

You may notice that the frequency is currently set to None. We’ll talk about frequency later, but for now, that’s perfectly fine.

What’s important here is that the index already contains a lot of valuable information. Because these are dates, we can actually create multiple features directly from the index. For example, we might want to know whether a given observation falls on a Monday, whether it’s a weekend, which month it belongs to, or which year it comes from.

All of this information already exists inside the index.

For instance, if we access the day of the week, we’ll see values like 0, 1, 2, 3, 4, 5, and 6. These correspond to Monday through Sunday.

Our goal here is to extract time variables and store them as new features.

Let’s start by extracting the year.
We create a new column called year, and assign it the value dataframe.index.year.

Next, we can extract the month by creating a month column and assigning dataframe.index.month.

We can do the same for the day of the month by creating a day column and assigning dataframe.index.day.

We can continue with more granular features. For example, we may want to extract the day of the week numerically. We do this by creating a column and assigning dataframe.index.dayofweek.

Sometimes, instead of numeric values like 0, 1, or 2, we want the actual day names, such as Monday, Tuesday, and so on. To do that, we create another column and assign dataframe.index.day_name(). This one requires parentheses.

There’s also another numeric representation we can use, which is weekday. This is essentially the same as day of week, but it’s still useful to store explicitly. We create a column and assign dataframe.index.weekday.

At this point, if we take a quick look at the DataFrame using head(), we can see that at the end we now have several new columns: year, month, day, day of week, weekday name, and weekday numeric.

As a final time-based feature, let’s create a weekend indicator.

We define a new column called is_weekend. This column will be a boolean that tells us whether a date falls on a weekend or not. The result is True for weekends and False otherwise.

This is actually very useful, because booleans are often easier to work with in models.

If you prefer to have this feature as a binary variable instead of True and False, you can convert it to integers using astype(int). This gives you 0 and 1, which is often ideal for machine learning models.

You can even add a comment to remind yourself that this conversion turns the feature into a binary representation.

All of this work is done using the index, but it also falls under feature engineering, which is what I want to focus on next.

There are many ways to perform feature engineering, and we’ve already covered some of them. For example, rolling averages—like a 30-day rolling average—are also a form of feature engineering. They smooth the data and help capture trends.

However, one of the most common and most important feature engineering techniques in time series is the creation of lagged variables.

So what are lagged variables?

Think about a real-world example. You’re browsing the web and you see an ad. You think it looks good, but you don’t buy the product that day. Instead, you purchase it the next day.

From a modeling perspective, the influence of the ad doesn’t happen immediately—it happens with a delay.

If we don’t include lagged variables in our model, we fail to capture this delayed effect. Lagged variables allow us to model situations where the impact of a regressor occurs in the future, rather than at the same time.

In other words, the value today may influence outcomes tomorrow.

Creating lagged variables is extremely simple.

For example, if we take the close column and apply shift(1), we create a lag-1 variable. This means that today’s row now contains yesterday’s closing price.

If we look at the values, we can see that what originally appeared on one date now appears on the next date.

If we want to go further, we can create a lag-2 variable by using shift(2). This shifts the values back by two periods.

As you can see, building lagged variables is extremely easy and very powerful.

We can store these as new columns, such as close_lag_1 and close_lag_2. If we preview the DataFrame again using head(), we’ll see these new lagged features added at the end.

All of this opens up a wide range of possibilities when it comes to modeling. As you start to understand your problem better, you begin to think about which variables influence outcomes and how those influences unfold over time.

This is where time series analysis and forecasting really begin to take shape.

# **J) Seasonal Decomposition**

In this video, I’m going to introduce you to seasonal decomposition.

The general idea behind seasonal decomposition is that we separate time series data into three distinct components: the trend, the seasonality, and the error term (also called the residual).

Let’s look at each of these components individually, starting with the trend.

The trend represents the general direction of the time series over time. You can imagine this as a smooth curve showing whether the data is generally increasing, decreasing, or remaining stable.

One important thing to keep in mind is that a trend can change over time, but it does not change every single day. If it did, it would no longer be considered a trend.

Next, we have seasonality.

Seasonality refers to recurring, cyclical patterns that repeat at regular intervals. A classic example is a time series that tends to be higher during the summer months and lower during the winter months.

You can imagine a chart where the data follows a repeating seasonal curve. This curve is cyclical, remains fairly consistent over time, and has a certain amplitude between its peaks and troughs.

Finally, we have the error term, also known as the residual.

The error term represents everything that is not explained by the trend or the seasonality. Ideally, this component behaves like random noise and does not exhibit any clear pattern. You can think of it as a random walk with no structure.

Now let’s zoom in specifically on seasonality.

One important thing to understand is that there are two main types of seasonality.

The first type is additive seasonality.

Additive seasonality is characterized by constant seasonal fluctuations. For example, this could mean that we always add 10 units in July or subtract 50 units in December.

In this case, the size of the seasonal effect stays the same regardless of whether the trend is low, medium, or high. If you were to plot this, you would see the same seasonal pattern repeating with the same amplitude over time.

The second type is multiplicative seasonality.

Multiplicative seasonality occurs when the seasonal cycles are proportional to the trend. Instead of thinking in terms of absolute units, we think in percentages.

For example, the data might increase by 10% in July or decrease by 50% in December. In a chart, you would observe that the size of the seasonal fluctuations increases as the trend increases. Even though the pattern repeats, the amplitude grows over time.

Now you might ask: why does this distinction matter?

By understanding whether your data exhibits additive or multiplicative seasonality, you can make better forecasts and better-informed decisions. The type of seasonality directly influences how models interpret patterns and predict future values.

Another important question is: how do we identify which type of seasonality our time series has?

Unfortunately, there is no statistical test that can definitively tell us whether the seasonality is additive or multiplicative. However, we do have two practical options.

The first option is data visualization.

By plotting the time series, we can visually inspect whether the seasonal fluctuations remain constant over time or whether they grow and shrink proportionally with the trend. This is something we’ve already started doing in earlier videos.

The second option—and this one is very powerful—is to focus on model performance.

This means building two different models: one assuming additive seasonality and another assuming multiplicative seasonality. We then compare their performance and see which one better fits the data.

Ideally, we should use both approaches every time.

Visualization helps us form an initial intuition before modeling, while model performance helps us validate that intuition afterward. In practice, the second approach is often preferred because it is results-driven.

In other words, the first option is about making an assessment before modeling, and the second option is about evaluating the results after modeling.

Of course, we’ll try both approaches.

But for now, let’s move on and see how to check for seasonality and plot it in Python.

# **K) Python - Seasonal Plots**

One of the most important topics in time series analysis is seasonality, which refers to the cyclical patterns that occur over time. These recurring curves appear again and again across specific time intervals, such as months or quarters.

There are many different ways to identify, analyze, and model seasonality, and throughout this course, I’ll show you several approaches. For now, however, we are going to focus purely on visualizing seasonality, which is often the first and most intuitive step.

There is one specific set of functions that I want to introduce for this purpose. To keep everything organized, we’ll start by importing them.

From statsmodels.graphics.tsa.plots, we are going to import two functions: month_plot and quarter_plot. These are specifically designed to visualize seasonal patterns.

In addition, although we won’t use it in this video, we will also import seasonal_decompose from statsmodels.tsa.seasonal, as it will be important later in the course.

To begin, let’s focus on monthly seasonality.

One of the simplest ways to analyze monthly seasonality is to take the monthly values of a variable and observe how they behave over time. This approach provides a clear visual indication of whether seasonal patterns exist.

We start by selecting the closing price from our data frame. Then, we resample the data to a monthly frequency using month-end ("M") and calculate the mean for each month.

Once this is done, we apply the month_plot function to the resampled data.

The resulting visualization contains two important components.

The black line represents the actual observed values for each month over time. In the case of Bitcoin, this line trends upward overall, but with large fluctuations due to volatility. Each point on this line corresponds to an actual monthly value.

The red line represents the average of the averages. In other words, for each month (January, February, and so on), it shows the average value across all years. This red line is what we use to identify seasonal patterns.

In this particular case, we do not observe strong seasonality. The red line does not show clear peaks or troughs across months, indicating that Bitcoin does not have a pronounced monthly seasonal pattern.

To clean up the output and ensure that only a single plot is displayed, we use plt.show().

We can also improve readability by adding a y-axis label, such as “Closing Price”, which makes the visualization easier to interpret.

Next, we can explore quarterly seasonality using a similar approach.

Once again, we start with the closing price, but this time we resample the data using quarter-end ("Q") instead of month-end. After calculating the mean, we apply the quarter_plot function.

This visualization makes seasonal differences more apparent. With the exception of Q3, which appears unusually flat or lower, the quarterly seasonality becomes clearer. This kind of insight can signal that further investigation is worthwhile.

At this point, it’s useful to look at a different data set that exhibits clearer seasonality.

For this purpose, we import a second data set containing monthly revenue for a chocolate company. This data set is already aggregated at a monthly level, which makes it ideal for seasonal analysis.

We read the CSV file using pandas.read_csv, specifying the date column as the index and enabling parse_dates=True to ensure the dates are handled correctly.

Because this data is already monthly, we do not need to resample it. We can directly apply the month_plot function to the revenue column.

This time, the seasonality is much more apparent. We can clearly see seasonal bottoms occurring during certain months and strong peaks during others.

For example, November stands out as a peak month, which aligns well with real-world events such as Black Friday, Singles’ Day, and holiday shopping seasons.

If we wanted to visualize quarterly seasonality for this data, we could easily do so by resampling using quarter-end frequency and applying the quarter_plot function.

However, we’ll stop here for now. There is much more to explore when it comes to seasonality, and we will continue building on this foundation in the next video.

# **L) Python - Seasonal Decomposition**

Now let’s take a look at one of the most important techniques in time series analysis: seasonal decomposition.

The general idea behind seasonal decomposition is that we take a time series and split it into three separate components. These components are the trend, the seasonal cycles, and the noise (or residuals), which represents whatever remains unexplained after removing trend and seasonality.

To start, we apply seasonal decomposition to our time series data. In this case, we are working with the adjusted close price from our data frame. We use the seasonal_decompose function and then call .plot() to visualize the result.

When we run this, we obtain a plot that shows four panels: the original series, the trend, the seasonal component, and the residuals. This gives us a structured way to understand what is driving the time series.

To make the visualization cleaner and easier to work with, we store the decomposition output in a variable called decomposition. From there, we generate the plot and explicitly set the figure size. A size of 18 by 10 generally works well and makes the components easy to read.

When we reduce the figure size significantly, for example to 5 by 5, we notice that the seasonal cycles are very difficult to see. Everything appears compressed, and the seasonal structure is effectively hidden. This highlights how important visualization choices are when working with decomposition.

At this point, we need to talk about an important parameter in seasonal decomposition: the period.

The seasonal_decompose function needs to know the length of the seasonal cycle. If the data index does not have an explicit frequency, or if we want to override the default behavior, we must specify the period manually.

This is critical because seasonal decomposition only supports one seasonality at a time.

This limitation is important to understand. For example, with daily data, we may have weekly seasonality (7 days), yearly seasonality (365 days), and potentially even monthly effects. However, classical seasonal decomposition forces us to choose only one of these.

This is a known limitation of early time series techniques. More modern models can handle multiple seasonalities, but here we must be explicit about which one we want to analyze.

Since we are working with daily data, one reasonable choice is to set the period to 365, which captures yearly seasonality.

When we do this, we start to see a seasonal pattern emerge. The seasonal component is not perfectly smooth, but it is consistent across time. The trend component shows longer-term movements up and down, and the original series reflects the raw adjusted close values.

If instead we set the period to 7, which corresponds to weekly seasonality, the decomposition becomes much less insightful. The seasonal and trend components do not reveal meaningful structure, and the output is not very informative.

This is expected, especially for stock market data. Stock prices are widely known to behave like a random walk, meaning they do not exhibit strong or reliable trends or seasonality. Because of this, predicting stock prices consistently is extremely difficult, if not impossible.

As a general rule, if someone claims they can reliably predict stock prices, they are likely trying to sell you something.

Now let’s contrast this with a different data set: the chocolate company revenue data.

This data is monthly and contains a single variable: revenue. We apply seasonal decomposition again, this time using a period of 12, since the data is monthly.

We start with an additive model, and the results immediately make more sense. The trend clearly increases over time, the seasonal component shows a repeating annual pattern, and the residuals are relatively small.

However, when we look closely at the revenue data itself, we notice something important. The seasonal spikes become larger over time. Early in the series, the seasonal fluctuations are small, but as revenue grows, the seasonal peaks and troughs grow as well.

This behavior suggests that an additive model may not be ideal.

In cases where seasonal fluctuations increase proportionally with the trend, a multiplicative seasonality model is more appropriate.

When we apply a multiplicative decomposition, the seasonal component is expressed as percentages rather than absolute values. A value of 1.0 represents a neutral effect. Values below 1 indicate a negative seasonal impact, and values above 1 indicate a positive seasonal impact.

For example, a value of 0.8 means revenue is about 20% lower than average due to seasonality, while a value of 1.4 indicates roughly a 40% increase. In this data set, months like November typically show strong positive seasonal effects, which aligns well with real-world business patterns.

This interpretation makes multiplicative seasonality far more intuitive for growing time series like revenue.

With that, we’ll stop here for now. There is still much more to explore when it comes to seasonal decomposition, and we’ll continue building on this in the next video.

# **M) Auto-Correlation**

In this video, I’ll introduce a very important and interesting concept called autocorrelation.

The basic idea behind autocorrelation is to understand whether information from the past can help us predict the future. To do this, we correlate the values of a time series with its own past, or lagged, values and observe how strongly they are related.

Imagine two axes where we plot the observation 
𝑦
y at time 
𝑡
t against 
𝑦
y at time 
𝑡
−
1
t−1. This represents the relationship between the current value and the value from the immediately previous period. If the data points show a clear upward trend, this indicates a positive correlation. Suppose we compute this correlation and get a value of 0.8.

Now we create a main graph where the correlation value is shown on the y-axis and the number of lags is shown on the x-axis. The point corresponding to lag 1 would be plotted at 0.8 on this graph.

Next, we move to lag 2. Here, we correlate the time series with values lagged by two time units. Let’s imagine the correlation value for lag 2 is 0.6. We plot this value on the same graph as well.

One key idea to understand is that as the lag increases, the correlation usually decreases. This is because the information comes from further in the past and is therefore less relevant for predicting the present. As a result, the correlation values tend to get smaller as the lag increases.

If we continue this process for higher lags, the correlation values may become very small or even negative, but they generally move closer to zero. By analyzing this pattern, we can understand how far back in time we can go and still find useful information for prediction.

Of course, this does not give us the complete picture by itself, but it is a very important piece of information when working with time series data.

To summarize, an autocorrelation plot helps us determine whether there is useful information in the past values of a time series and how long that information remains relevant.

Now let’s apply this concept and see how it works in practice. Until the next video.

# **N) Python - Auto-correlation**

We’re now going to work with autocorrelation using the statsmodels library, which is great because it not only computes autocorrelation but also plots it for us. Here, I’m following a common suggestion, which is to plot both the autocorrelation and the partial autocorrelation, although we still need to properly learn what partial autocorrelation is.

The good thing is that this is very easy to do. In general, we don’t care much about the exact numerical values of the correlations, because we’re not going to directly use those numbers anywhere. What really matters is how the plot looks, what it tells us, and the intuition behind it. That’s what we’re going to focus on.

Let me activate the environment first. I actually need to activate everything else as well. Alright, everything is activated now. I jumped to the bottom of the script. If you’re wondering why I needed to activate everything again, it’s because I’m re-recording this video. Sometimes recordings don’t go as planned, and I need to redo them to keep the content and the Python code as up to date as possible. Anyway, let’s move on.

The first step is to plot the autocorrelation of the Bitcoin adjusted close price. I have a suggested snippet here, but I’m not going to use it immediately because I want to control the figure size. So I first define the figure using plt.figure and set the size. I’ll go with a width of 12 and a height of 6. Then I plot the autocorrelation for the adjusted close values.

One important thing we still need to specify is the number of lags. Let’s set it to 100 and see what this gives us. Once we run it, we get the plot.

What we see here is quite striking. The level of correlation in Bitcoin is absolutely massive. There is a very strong correlation across many lags, which is honestly quite uncanny. Now, if we want to be a bit more critical and properly assess this, it almost looks like a linear curve. That immediately raises an important question: is this actually relevant?

In other words, if we look at lag 100, what information does lag 100 really contain that was not already present in lag 99? That’s the key question. What this plot is indicating is that all of these lags contain information, and possibly even relevant information. You could technically take lag 100 and still observe a strong correlation. But the real issue is uniqueness. What unique information does lag 100 bring that was not already captured by lag 99, lag 98, and so on?

This is exactly where partial autocorrelation comes into play. I’m spoiling it a bit here, but that’s essentially what partial autocorrelation answers. It tells us the correlation at a specific lag after removing the effects of the previous lags. In other words, it helps us understand whether the correlation at lag 100 is truly new information, or whether it only exists because the correlation was already present at lag 99, which itself depended on lag 98, and so forth.

In many cases, what we see at higher lags is not really new or relevant information. It’s just inherited from earlier lags. We’ll confirm this more clearly once we look at partial autocorrelation.

Now let’s shift our focus to a different dataset. If we inspect the df_choco dataframe, we can see that it has only 60 entries. Because of that, it wouldn’t make sense to use 100 lags here. Instead, something like 20 lags is much more reasonable.

So we now plot the autocorrelation function for the revenue KPI in df_choco, using 20 lags. We follow the same approach as before, just changing the data and the number of lags. Once we plot it, the result looks very different.

Notice that we don’t see the same kind of linear decay here. Instead, the pattern goes down, up, down, up. Another very important thing to notice is the blue shaded area in the plot. This represents the confidence interval. As we move further into the past, this interval widens. That makes sense, because the further back we go, the fewer observations we have, and the more variability we expect.

What this plot is telling us is very insightful. We can clearly see seasonality in the data. There is strong correlation at the most recent lags, which reflects the trend. But we also see notable spikes around lag 5 or 6, and again around lag 12. This suggests seasonal behavior at roughly six-month and twelve-month intervals.

This aligns well with intuition. There is information repeating itself on a seasonal basis. For example, something meaningful is happening every six months and every twelve months. This gives us strong evidence that seasonality plays a significant role in this dataset.

Again, the natural question arises: what is the value added by a specific lag? For instance, what does month three contribute that was not already present in month two or month one? This is exactly the kind of question that partial autocorrelation is designed to answer.

Overall, this exercise highlights how important it is to understand our data deeply. Autocorrelation helps us see whether past values contain useful information and how far back that information remains relevant. In this case, it looks like information up to around 12 months in the past can help us predict the future.

# **O) Partial Auto-Correlation**

Now that we understand the autocorrelation, let’s move on to the partial autocorrelation, which is a very cool concept in the world of time series. Don’t let this scare you—it's not super complicated at all. It’s actually really handy, and I’m going to break it down for you in very simple terms. So let’s kick it off.

What is this PACF, or partial autocorrelation function, all about? Imagine you’re trying to understand the relationship between your coffee consumption today and how much coffee you drank a few days ago. But here’s the twist: you want to know this relationship without the influence of all the days in between. That’s exactly what partial autocorrelation does. It tells you the direct relationship between your data points at different times, while removing the effects of the points in between.

Now remember the autocorrelation function. That’s like looking at the total correlation between a time series and its lagged values, including both direct and indirect effects. It’s similar to asking, “How does my coffee consumption today relate to all the past days?”

Partial autocorrelation, on the other hand, is more specific. It’s like asking, “How does my coffee consumption today relate specifically to three days ago?” while completely ignoring the days in between. So while autocorrelation gives you the overall picture, partial autocorrelation zooms in on specific, direct relationships.

When you plot the PACF, you’ll usually see bars at each lag, just like you do with the autocorrelation plot. If a bar stands out significantly, it means there is a noteworthy direct relationship at that particular lag. If these bars drop off quickly, it suggests that only recent values have a direct effect on current values. If they tail off slowly or oscillate, it indicates that older values still have a direct influence.

You might now wonder why PACF is needed when we already have ACF. The autocorrelation function starts the story by showing all correlations, but it can include noise from indirect correlations. The partial autocorrelation completes the story by isolating only the direct correlations, giving you a much clearer picture of how each point in time directly influences another.

# **P) Python - Partial Auto-Correlation**

Welcome back, and welcome to partial autocorrelation. Partial autocorrelation is very simple in terms of application. Just like before, we’re going to compute the PACF for Bitcoin adjusted close. There we go, and we’re going to do the exact same thing as we did earlier. Even Gemini catches on very quickly if you know what to do, and you can put it in comments and get this done very, very fast.

The only difference between this code and the ACF code is this “P” here. That’s literally the only difference. Now, look at this chart. It’s quite different, right? If you look at it closely, it’s just so different compared to the autocorrelation plot.

What this chart is telling us is that the only information that is actually relevant, the only information that is truly unique, comes from the day before. And this is kind of crazy when you think about it. You saw earlier that we had so much information, but basically all of that information was always connected to the information that happened just before it. There was no unique information beyond that. Of course, you do see some values here that go outside, but really, for Bitcoin—and please take this seriously—if you’re trying to predict Bitcoin, the only relevant information is what happened the day before. This is a very clear conclusion from this chart.

Let me actually write it down. The only relevant information for the price of Bitcoin is what happened the day before. There we go.

Now we can do the PACF for revenue in the data frame. Let me jump here using some shortcuts. Here we go, and here we are with the partial autocorrelation. In this case, we clearly see that there is information in the day before, but apparently there is also relevant information at two, three—so one, two—then four and five months in the past, and also around ten months in the past. Twelve months, apparently, is not so relevant.

So if we count the lags—one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen—this means we’re looking at the previous month and the previous year. Apparently, we did not get a lot of unique information overall, but you do get unique information if you don’t just look one month back. If you look four or five months back, ten months back, and apparently thirteen months back as well, those lags were important.

If you connect this partial autocorrelation chart to the autocorrelation chart, it becomes much more meaningful. From the partial autocorrelation, you can say, okay, these specific lags contain unique information. That’s how you can phrase it. For the autocorrelation, it’s more like saying, okay, I can use all this information from the past and still retain information. You kind of connect both ideas, and then you’re in good shape.

You can still see some level of information around twelve or thirteen months here as well, but it becomes very, very small. If you really need to nitpick, if you need to pick just a few lags, you would go with these ones. These specific lags contain very unique information that you don’t find anywhere else.

And that’s it for us. This is how you do partial autocorrelation. It’s very, very quick, and I’ll see you in the next video.

# **Q) Python - Building a Useful Function Script**

Welcome back for this last video. I want us to build a script that you can use whenever you’re working with some kind of time series analysis. Of course, it’s not perfect and it’s not a one-size-fits-all solution, but it works as a solid starting point. It gives you a building block so you can take whatever you’re doing and begin by analyzing the data. No matter which technique you later apply, exploratory data analysis, manipulation, and visualization are more or less the same and don’t really change that much.

I’m going to take this script here and move it into the main one. Let me open it, and then I’ll start with the first cleanup. The idea is to keep what’s always relevant and remove what’s not. For the setup section, everything up to the path stays the same. The Python time series forecasting libraries remain unchanged. The Bitcoin price file becomes a generic “xxx.csv” so it can be reused. I’ll clear the selected outputs to keep things clean.

For the time series index, I’m going to keep it because it’s important. Usually, the column is called “date,” so there’s no issue there. I’ll remove this part and instead use the version where we set the index directly and parse the dates at the same time. That’s the one I want to keep, because most of the time we set the index column and parse dates together. If I only want the actual date column, I can simply delete the index and parsed dates part.

I’ll remove the remaining unnecessary pieces and keep things as clean as possible. The idea is to load the data and immediately put the date on the index. For exploratory data analysis, I’m not going to use rolling prices, so that part is out. For data visualization, I will keep it. I’ll plot the series without a title, which is usually better. I typically call my time series “y” because it’s easier to visualize, easier to work with, and I don’t have to rename it all the time. It also makes sense conceptually, since this is what we’re trying to predict. The extra parts here are removed.

For data manipulation, we’re not going to focus on that here, so that section is gone as well. When it comes to seasonality, this part will stay. I’ll clean it up and leave out the extra data frame pieces and labels. If I want to add a label later, I can always do that. We’ll keep the monthly plot, and I should also add a quarterly plot. So I’ll include the quarter plot, show it, and keep the seasonal decomposition. The default multiplicative model is fine.

Next, we add the autocorrelation. I’ll set the number of lags to 100, and if I ever need to change it, I can. Then I add the partial autocorrelation as well. Now we have both ACF and PACF in the script.

It’s worth reinforcing that I’m trying to keep this course up to date. Even when future warnings appear, the goal is to maintain a clean and usable script. There may be discrepancies across different versions of libraries, and that’s expected. This script is meant to be a useful code template. If something behaves slightly differently in your environment, that’s normal.

If you have any questions, let me know. I try to make sure everything works in one go, but keep in mind this is a course that’s close to 40 hours long. There are bound to be moments where things don’t connect perfectly. I really appreciate your patience. If anything confuses you, just ask—I’m here to help.

As a final note, it’s a bit sad for me when people say they don’t understand something, not because I think I’m the best explainer in the world—I definitely don’t—but because often they don’t ask questions. If you have questions, let me know. I answer everyone seriously. Just post it in the Q&A, and I’ll be there to help you out. I’ll see you in the next video.

# **R) Can you predict stock prices?**

Have you ever been bombarded with stock price predictions? If so, then this video is for you.

As we’ve seen, distinguishing between trend and seasonality in time series can seem straightforward at first. So why is forecasting still considered such a complex task? Why do organizations dedicate entire teams to it? The main challenge lies in modeling and interpreting errors. Forecasting is not just about fitting a model, but about understanding where it fails, why it fails, and how those errors can be explained and reduced.

A big part of improving predictions comes from incorporating relevant factors or regressors. These can include specific events, temperature changes, snowfall, overall economic conditions, and even public sentiment. Each of these elements can significantly influence forecasting accuracy, depending on the problem you’re trying to solve.

Another crucial aspect is the time horizon of the data. Suppose you have data spanning six or seven years. Do you really need all of it? Older data can introduce a lot of noise into your models because past conditions may no longer reflect what is happening today or what will happen in the future. This is why it’s essential to evaluate whether your data is still representative of future trends, and if it’s not, to make the conscious decision to exclude it from your analysis.

This brings us directly to stock market prediction. We are constantly flooded with articles claiming extraordinary returns using different strategies and algorithms. But remember, everyone looks like a genius in a bull market. The real question is whether these approaches can consistently outperform professional investment firms. In most cases, that’s highly doubtful.

Price movements can give the impression of a clear trend, but predicting when that trend will reverse is extremely difficult if there is no reliable pattern. Traditional forecasting models struggle in these situations, especially when sudden changes occur.

This unpredictability became painfully clear during the Covid-19 pandemic in March 2020. The pandemic completely disrupted existing trends and seasonal patterns, making many forecasting models ineffective overnight. It was a powerful reminder that external, unprecedented events can drastically change market dynamics.

In summary, forecasting—particularly with stock data—is a complex and nuanced task. It requires careful consideration of data relevance, thoughtful modeling of errors, and an awareness of external influences that can break even the most well-designed models.

# **S) What did we learn in this section?**

We have just finished the crossing line, and for this introduction to time series forecasting, it’s a great moment to pause and reflect on everything we’ve accomplished. It’s been quite a journey when you think about it.

We started from the very basics, simply trying to understand what time series data actually is. At first, it can feel unfamiliar, almost like learning a new language. But now, it feels natural. We’ve gone from confusion to confidence, and we can clearly analyze how data behaves over time.

Diving into Python was a major turning point. What may have felt intimidating in the beginning quickly became empowering. We learned how to slice and dice time series data with confidence, using Python not just as a tool, but as a reliable partner in solving real problems. Those libraries are no longer abstract concepts; they’re part of your everyday toolkit now.

We also made big progress in data visualization. Instead of staring at raw numbers, we learned how to turn them into meaningful stories. The charts we create now aren’t just visuals, they communicate insights in a way that makes sense to anyone looking at them.

Seasonality was another important concept we tackled. What once seemed tricky is now something we can recognize and interpret with ease. Whether it’s sales patterns or recurring trends, we know how to identify and reason about seasonal behavior in data.

Autocorrelation marked another milestone. It’s one thing to look at individual data points, but understanding how values relate to each other across time is a whole new level. And you’ve reached that level by learning how past values influence the present.

The discussion around predicting stock prices was especially eye-opening. It showed us the realities of forecasting and the complexity of financial markets. There’s no magic crystal ball, but there is careful analysis, critical thinking, and a lot of smart, hard work behind every meaningful prediction.

So take a moment to appreciate how far you’ve come. You didn’t just learn concepts—you applied them, analyzed data, and visualized insights along the way. That’s real progress.

# **T) CASE STUDY: Forecasting Gone Wrong**

Hey everyone, and welcome. Today I want to talk about something pretty interesting: what happens when forecasts don’t quite hit the mark. We all try to predict the future in one way or another, whether it’s the stock market, fashion trends, or even just the weather. But sometimes things don’t go as planned, and that’s actually okay. When predictions go sideways, that’s often when we learn the most.

Let’s look at some memorable forecast failures and what they can teach us. Take the rise and fall of fidget spinners. Remember those little spinning gadgets that suddenly seemed to be everywhere? One day nobody knew what they were, and the next day every kid in school or university had one. Retailers and manufacturers assumed they had struck gold and ramped up production aggressively. But the hype faded almost as quickly as it appeared, and suddenly stores were left with huge piles of unsold inventory. It was a classic case of confusing a short-lived craze with a long-term trend. The key lesson here is the importance of questioning whether a trend is truly here to stay or just a passing fad.

Another famous example is Long-Term Capital Management, or LTCM. This hedge fund was once considered elite, staffed by brilliant minds using highly sophisticated mathematical models to predict market movements. For a while, it looked like they had cracked the code, generating massive returns. Then the Russian financial crisis hit in 1998, completely outside what their models anticipated. The markets behaved in ways their assumptions couldn’t handle, and the fund collapsed dramatically. It was a stark reminder that no matter how advanced a model is, there will always be events it cannot foresee. Markets are complex and unpredictable, and sometimes they move in ways no algorithm can capture.

Then there’s Google Flu Trends, an ambitious attempt to predict flu outbreaks by analyzing what people searched for online. The idea was simple and clever: if more people are searching for flu symptoms, a flu outbreak might be underway. Initially, it looked like a breakthrough in using big data for public health. Over time, however, it became clear that the model was frequently overestimating flu cases, sometimes by a wide margin. Changes in user behavior and constant updates to Google’s own search algorithms distorted the signals. This case shows that big data and powerful algorithms still need careful calibration and constant reevaluation, especially when human behavior is involved.

Another intriguing example is the Hindenburg Omen, a complex technical indicator designed to predict stock market crashes. Based on certain market conditions, such as the number of stocks reaching new highs and lows at the same time, it aims to warn of an impending collapse. While it has occasionally preceded market downturns, it has also triggered false alarms that caused unnecessary panic, and at other times it failed to signal real crashes. This highlights the danger of relying too heavily on a single indicator without considering broader context and supporting evidence.

All of these stories point to the same underlying truth. The world is full of surprises, and no forecasting model, no matter how sophisticated, can account for everything. Whether we’re dealing with financial markets, public health, or consumer trends, uncertainty is always part of the equation. Forecasts are valuable tools, but they work best when paired with humility, critical thinking, and an understanding that the unknown will always play a role.

# **V) Section 5: Time Series Analysis Practice**

# **A) Data Loading and Index**

Welcome to this practical activity!

In this exercise, we’re going to work with Python, and I’ll demonstrate it using Colab. Of course, you can use any Python environment of your choice. Alternatively, if you have access, the Udemy workspace is also available.

Here’s what we’re working on: we have the weekly sales of a department, and our goal is to explore this data. You’ll find the dataset in the folder I’ve shared, and it’s also preloaded in the Udemy workspace. You’ll see a starter file and Lab 1, which contains the solutions. We’ll begin with the starter file.

First, connect to your drive. Make sure to adjust your directory path if it differs from mine. I’ve also prepared all the necessary libraries for you.

We’ll complete three tasks in this lab. The purpose of this initial exercise is to prepare the dataset for time series analysis.

Task 1: Load the department sales dataset into a DataFrame. Use:

import pandas as pd


df = pd.read_csv("department_iPhone_sales.csv", index_col="date", parse_dates=True)

Here, we make sure the date column is used as the index, which is important for time series analysis.

Task 2: Convert the index into a standard date format (YYYY-MM-DD), which makes it easier to work with. By default, Pandas may infer a different format, so we explicitly specify:

df.index = pd.to_datetime(df.index, format="%d-%m-%Y")

Now, the index shows the year, month, and day correctly.

Task 3: Set the frequency of the data to weekly. Since our dataset is recorded weekly, we tell Python:

df.index.freq = "W-FRI"

We use W-FRI because the sales dates fall on Fridays. This ensures Python correctly interprets the weekly spacing.

And that’s it! With these three steps, your dataset is now properly formatted for time series analysis: the dates are clean, indexed correctly, and the frequency is set.

# **B) Data Visualization for Time Series**

Let's do Lab Number Two. You can either continue on the starter Lab One file, which is the same, or I’ve also prepared the starter Lab Two just in case you are starting with this video.

For these exercises, we are going to focus on data visualization. The first task, Task 1, is to plot the weekly sales using the weekly_sales column. Additionally, we’ll do a bit of formatting—like adding a title to the chart—to make it more readable.

To do this, we can use:

plt.plot(df['weekly_sales'])
plt.title("Weekly Sales")
plt.show()

This is probably the easiest initial component. You’ll notice, however, that the x-axis may look a bit off or crowded. To fix this and improve the visualization, we can adjust the figure size:

plt.figure(figsize=(10, 4))
plt.plot(df['weekly_sales'])
plt.title("Weekly Sales")
plt.show()

Now, we can clearly see the overall evolution of weekly sales. This completes Task 1, where we customized the chart with a proper size and a title.

Looking at the chart, you might notice several spikes—spike, spike, spike—and it draws our curiosity. There’s a column called is_holiday, and the next task will leverage this.

Task 2 involves adding a vertical line whenever is_holiday is set to True. Specifically, we want a vertical dashed red line on the chart.

To implement this, we iterate through the DataFrame rows. For each row, if is_holiday is True, we use plt.axvline to draw the vertical line:

for date, row in df.iterrows():
    if row['is_holiday']:
        plt.axvline(x=date, color='red', linestyle='--', linewidth=0.5)

Here:

color='red' makes the line red.

linestyle='--' makes it dashed.

linewidth=0.5 keeps it thin so it doesn’t dominate the chart.

After plotting, you can see that many spikes in weekly sales appear to align with holidays. Some spikes, like those in May, don’t perfectly align with holidays, which indicates other factors may also contribute. Overall, the red dashed lines help highlight a seasonality component in the data related to holidays.

With this, we have completed Lab Two, which had two tasks:

A simple plot of weekly sales with a title and customized size.

Enhancing the plot by adding vertical red dashed lines whenever there is a holiday, showing how holidays may impact sales.

This visualization is a great way to connect time series spikes with external factors before diving deeper into analysis or forecasting.

# **C) Exploratory Data Analysis for Time Series**

Let's do Lab Number Three. You will also find a starter file for this lab. We have three tasks, and the focus here is more on exploratory data analysis, with an emphasis on autocorrelation and seasonal decomposition. Basically, we want to see what kind of information is already in our data from a trend and seasonality perspective.

Task 1 is to plot the autocorrelation of the data with 60 lags. We can do this using the ACF function:

from statsmodels.graphics.tsaplots import plot_acf
plot_acf(df['weekly_sales'], lags=60)
plt.show()

Looking at the chart, the first lag has the most information. Lags one through six contain some information, and then there’s a noticeable spike at lag 52. This makes sense because it represents roughly one year before. Overall, the strongest signals are from recent weeks, while older lags have less relevance.

Task 2 is to plot the partial autocorrelation. Unlike autocorrelation, which looks at total correlations, the partial autocorrelation isolates the unique information at each lag by removing the influence of earlier lags.

from statsmodels.graphics.tsaplots import plot_pacf
plot_pacf(df['weekly_sales'], lags=60)
plt.show()

Here, we notice a slightly different picture. One year ago doesn’t carry as much unique information, but there is strong unique information one week, two weeks, and seven weeks before. The remaining lags are less relevant from a statistical perspective. This helps identify the most meaningful time intervals in the data.

Task 3 is seasonal decomposition, where we focus on a multiplicative model with a period of 52 weeks (one year):

from statsmodels.tsa.seasonal import seasonal_decompose
decomposition = seasonal_decompose(df['weekly_sales'], model='multiplicative', period=52)

We can improve visualization by adjusting the figure size:

fig = decomposition.plot()
fig.set_size_inches(12, 8)
plt.show()

Looking at the decomposition:

The trend initially goes down, then up, and stabilizes, but keep in mind the first and last six months don’t show a reliable trend due to the way seasonal decomposition works.

The seasonality appears unusual, likely influenced by external factors like holidays.

The residuals hover around one, with some spikes, reflecting irregular components in the data.

While the seasonality is strong, it’s influenced by external elements, so it might appear a bit odd. Nevertheless, this gives us a clear breakdown of trend, seasonality, and residuals in the dataset.

With this, we have completed Task 3 and the lab. I hope you enjoyed this exploration, and I’ll see you in the next video.

# **VI) Section 6: Exponential Smoothing & Holt-Winters**

# **A) Game Plan For Exponential Smoothing and Holt-Winters**

I am very excited to walk you through the world of exponential smoothing and Holt-Winters. In this video, we'll kick off with the agenda that we'll cover throughout this section. I have a very fun case study lined up. Think of it as the goal we need to achieve. It's about customer complaints, and it’s basically as real as it gets. We'll use this case study to apply the skills we’ve learned in a way that mirrors what you would encounter in the business world.

Next up, let's talk about Python. Whether you're new to it or already familiar, we'll be using it extensively. This is how we get hands-on experience. So please prepare yourself for some coding action—we’ll go deep and program everything we need to do.

We won’t just stick to one type of exponential smoothing. Oh no! We’ll explore simple, double, and triple methods, and each smoothing method will get its own spotlight in our Python sessions. This is where the magic happens—where the theory meets practice.

We also need to learn how to measure errors in time series forecasting. And because data comes in all shapes and sizes, we’ll learn how to handle weekly data, daily data, and more granular data. The more granular the data, the more complex it becomes, but this is a skill we need to master.

Last but not least, we’ll wrap up this section with a discussion on the pros and cons of exponential smoothing and Holt-Winters. It’s important to understand what a technique can and cannot do.

So, get ready to learn a lot and hopefully have some fun. By the end of this section, you’ll be able to handle Holt-Winters and exponential smoothing like a pro.

# **B) CASE STUDY BRIEFING: Customer Complaints**

So picture this. We have a challenge that we need to solve. Imagine there is a company called Telco Wave, a very big player in the telecom world. And they’re facing a real puzzle, a challenge. Their customer complaints are all over the place. Some weeks it’s smooth sailing, other weeks it’s total chaos. And guess what? They have asked us to figure it out.

So it’s our job to predict these unpredictable swings. Why, you ask? To help Telco Wave become better at customer service and to showcase how we can use data to actually solve real problems.

Here’s the problem statement. Telecom is really facing this issue because they are getting more and more complaints. They are literally scratching their heads over how many customer service reps they need for each week. If the prediction is wrong, resources are wasted, and customers end up unhappy. This isn’t just a numbers game—it’s about bringing order into chaos.

Therefore, we need to craft a strategy. But before we dive into any complex solutions, we need to understand the basics. We need to get to know the data. What’s behind these fluctuations? What hidden patterns might we be missing? We need to dissect the whys behind these numbers. This is where data analysis comes in.

Exploring the data is a big deal. If we understand it well, Telco Wave can shift from playing catch-up—wasting resources and frustrating customers—to being in control.

By the end, the goal is to empower them to match their workforce perfectly with what the customers need. Fewer complaints will fall through the cracks, more customers will be satisfied, and the business will be healthier, which ultimately means more profits.

# **C) Python - Exponential Smoothing Set Up**

Let's kick off Exponential Smoothing, which is really one of those foundational forecasting models. In this video, we’re going to focus on setup. You are going to take the useful code templates that you built before, and in case you haven’t, you’ll find one here that’s final. You can start from there, of course, but do it with me.

You’ll find one template there, or you can use the one that you built yourself. Then I’m going to do Control C and copy it into time series analysis → exponential smoothing and holt-winters, and let’s open it and see how far we can get in this video. The goal is to either completely do the setup or at least get very close to it. I also don’t want to have a super long video.

I’m going to call the file holt-winters. We’ll start by mounting the drive. This will prompt you to give permission to connect to Google Drive. Once that’s done, we can proceed.

While this is connecting, I want to introduce the dataset. It’s about weekly customer complaints, and the business scenario is simple. Imagine you work in an e-commerce company—or really, any company with customer support—and you need to figure out how many customer support people you need. Most customer supports work with short contracts. Big companies often hire agencies to provide a certain number of people for a week or month. To plan properly, you need to know the expected number of complaints so you can allocate the right workforce. This is where time series analysis and exponential smoothing comes into play.

Next, we copy the path for the dataset and add it to our script. For now, we won’t import any extra libraries; we’ll do that later as needed. The dataset is called weekly customer complaints. It has a few columns, but we will focus on complaints for now. We cannot use independent variables with Holt-Winters or any other exponential smoothing model, so the complaints column is our main focus. The time variable is called week.

After running the initial setup, we have week as the index and complaints as the main column. I like to rename the time series variable to y. This is done by renaming the column: data_frame.rename(columns={'complaints': 'y'}). Now the series is ready for analysis.

Next, we attempt to plot the time series. Initially, we might encounter a “no numeric data to plot” error. This happens because the complaints column contains commas and is recognized as an object rather than a numeric type. To fix this, we remove the commas and convert the column to an integer: data_frame['y'] = data_frame['y'].str.replace(',', '').astype(int). Now the column is properly numeric, and we can plot it.

After plotting, we see that complaints increase over time, with higher spikes and an upward trend. This suggests multiplicative seasonality—the seasonality grows with the level of the series. While the trend is apparent, the spikes indicate specific events or fluctuations. To fully understand the seasonality, we will need to perform seasonal decomposition.

This completes the setup for our exponential smoothing analysis. In the next video, we will continue exploring the data and applying the smoothing methods.

# **D) Python - Exploratory Data Analysis**

Welcome back. Let's continue here. First, we need to understand our seasonality. If we just run the model as-is, we're going to get an error. This is because our data is weekly, but we are trying to generate a monthly plot. Therefore, we need to resample the data to a monthly frequency. The suggestion is to use M, though this may show a future warning. To properly handle this, we should use month end. Once we do that, we have our seasonal data.

Looking at the monthly data, we can observe that there are noticeable bottoms in February and March, as well as in August and September. Peaks are visible around November and a bit in December. This gives us a better understanding of our data. Overall, Q4 (October to December) appears to be the strongest, while Q2 is moderate, and Q1 and Q3 are the weakest quarters.

We can also analyze seasonality by quarter. Similar to before, we need to resample the data, this time using quarter end and calculating the mean. The results confirm that Q4 is the strongest quarter, followed by Q2, then Q3, and finally Q1 as the weakest quarter.

Next, we move on to seasonal decomposition. Since we have weekly data, the period is set to 52, corresponding to the 52 weeks in a year. First, we try an additive decomposition. The seasonal cycles are visible but not extremely smooth; there are noticeable ups and downs. The seasonal component fluctuates by around 100–200 units. The residuals also indicate some variation, especially in March. The trend component appears somewhat S-shaped.

From observing the data, it looks like a multiplicative seasonality might be more appropriate. This is because the seasonal spikes seem to increase as the trend grows, making the amplitude of the cycles larger over time. So we run a multiplicative decomposition. The trend remains largely the same, and the seasonal cycles do not change dramatically, but the multiplicative effect aligns with the increasing amplitude pattern.

As we continue exploring and measuring the models, we will determine which approach—additive or multiplicative—performs better and yields the best results.

Moving on, we examine autocorrelation and partial autocorrelation. By plotting the autocorrelation, we notice that there is significant information in the first 25 periods. There is also a noticeable spike around 52 weeks, representing one year in the past. For the partial autocorrelation, we see strong signals from the previous month, the last week, two weeks prior, and so on. This pattern continues with relevance for about 16 weeks, and then we again observe a signal around 52 weeks. Overall, the recent past and one complete seasonal cycle from the past year seem most informative.

From this analysis, we can conclude that a model incorporating seasonality is needed. Specifically, a Holt-Winters model would be suitable, and this is something we will explore in the upcoming sections.

Finally, let's briefly focus on the time series frequency. To check the frequency of the time series, we can inspect DataFrame.index. This will extract key information about the index, including length, name, and frequency (initially set to None). We can then change the frequency as needed. For example, our weekly data starts on a Monday in January 2018. By setting the weekly frequency to W-MON, we can confirm the change by checking DataFrame.index again, which should now reflect W-MON.

# **E) Training and Test Set in Time Series**

Welcome back. Let me introduce a very important topic in time series: splitting the data into a training set and a test set. This concept is simple and straightforward, but extremely powerful.

Imagine that you have a dataset represented by a blue rectangle. In a typical scenario, you might randomly split it, leaving 80% for training and 20% for testing. The main idea is to build a model using the training data and then evaluate it on the test data. This provides an unbiased way to assess the model’s performance.

However, time series is a very different beast. Unlike regular datasets, information on a given day is meaningless without the context of surrounding days. Moreover, in time series, we usually aim to predict the future. So, when splitting time series data, the practice is to remove the last periods. For example, you might remove the last observation, which then becomes part of the test set. The remaining data—represented here as yellow balls—becomes the training set. Importantly, the training data is not shuffled; it retains its chronological order. The light blue portion represents the test set.

Another important point is that the test set should reflect the number of periods you expect the model to predict in practice. For instance, if you are building a model to forecast the next four weeks, you should evaluate it using four-week periods. Similarly, if your forecasting horizon is three months, your test set should span three months.

Lastly, for the training data, it is recommended to include at least two full periods of data. For weekly data, this means having at least two full years, and ideally three, to capture clear seasonal and trend patterns. The goal is to provide enough data to identify robust patterns, which ultimately leads to reliable forecasts.

# **F) Python - Training and Test Set**

Welcome back. Let's talk about training and test sets. The first thing we need to understand is our goal. Our goal here is to predict the next 13 weeks. It’s important to note that we always measure and assess our model based on the business goal.

To introduce this concept, we will split the data into a training set and a test set. In this case, our period is 13 weeks. The training set will consist of all data up until the last 13 weeks—everything except the last 13 periods. The test set will then consist of everything starting from the last 13 weeks.

If we examine the training set, we still have all the data, which is fine. However, for practical purposes—such as using double or triple exponential smoothing—we often only work with the target variable, Y. So we subset the data to include only the columns we need and remove everything else that isn’t required.

There are multiple ways to subset and split your data. One approach is simply:

train = df[:-periods]   # all data except the last 'periods'
test = df[-periods:]    # the last 'periods' of data

Another way (which I had prepared for a different setup) could be using index positions:

train, test = df.iloc[: -periods], df.iloc[-periods:]

The key takeaway is not necessarily the exact method, but that you understand the concept and purpose of splitting the data. Once you manage this, you’re doing it correctly.

# **G) Simple Exponential Smoothing**

What is this all about?

Imagine you are checking weekly sales figures or weekly complaint counts. Some weeks are very busy, while others are quiet. What you are really trying to find is a steady rhythm in these numbers—a sense of the underlying level without getting distracted by short-term noise.

That is exactly where simple exponential smoothing comes in.

This method is not just a simple average. Instead, it gives more importance to what happened most recently. In other words, it “listens” more closely to recent data points while still respecting the past. This idea is built directly into the formula.

Let’s break that formula down conceptually.

The next forecast is calculated as the current level, plus alpha multiplied by the difference between the most recent actual value and the current level.

First, the current level.
This represents our baseline or starting point. It reflects where we believe the series stands based on everything we have seen so far.

Next, the recent actual value.
This is the latest observation—yesterday’s sales or last week’s numbers. It tells us what actually just happened.

Finally, alpha.
Alpha is the tuning parameter. When alpha is close to 1, we place a lot of weight on the most recent observation. When alpha is close to 0, we trust the historical level more and adjust very slowly.

Now let’s put this into a concrete example.

Suppose our current level is 100 cups sold. That is our baseline. Yesterday, however, we actually sold 120 cups. Let’s say alpha is 0.2.

With this setup, our forecast does not suddenly jump to 120. Instead, it moves up slightly, acknowledging that sales were higher than expected, but without overreacting to a single day’s spike.

Applying the formula gives us:
100 plus 0.2 times (120 minus 100), which results in a forecast of 104.

The values such as the current level and recent actual are computed by the model as it processes the data. Alpha, on the other hand, is typically chosen by the practitioner. For now, we won’t focus deeply on how to select alpha. The goal here is to understand the structure of time series forecasting. Later in the course, we will explore parameter tuning and how different alpha values affect results.

Now let’s think about how this applies to real data.

When we apply simple exponential smoothing to weekly sales data, the goal is to look past spikes—those weeks where sales suddenly surge or drop sharply. This method helps smooth out those fluctuations and gives us a broad sense of direction.

It is especially useful when you want a high-level view of where things are heading, rather than reacting to every short-term change.

To wrap things up, simple exponential smoothing is a very solid tool in a forecasting toolkit—but it is not a fortune teller. It does not capture strong trends or seasonal patterns. The equation itself is simple, and as a result, the outcome is also simple.

That simplicity is exactly why it is called simple exponential smoothing.

It helps smooth chaotic data, reduce noise, and highlight the underlying level of a time series.

Next, we’ll see how this works in Python. From there, we’ll build up step by step—from simple exponential smoothing to double and then triple smoothing in the following videos.

# **H) Python - Simple Exponential Smoothing**

Here is the same lecture rewritten in clean, normal output, lecture-style prose, with the meaning preserved and the flow clarified—no restructuring of intent, just clarity.

Alright, this is the video you’ve been waiting for—the point where we actually start doing some modeling.

To begin, we need to import a couple of functions. I’m going to import them upfront because as we move from simple to double and then to triple exponential smoothing, we’ll be using different functions. All of these live inside the statsmodels library, specifically under the time series analysis module and the Holt-Winters section.

From there, we import both ExponentialSmoothing and SimpleExpSmoothing.

Once that’s done, we’re ready to work with simple exponential smoothing. This function is very straightforward. If you look at the documentation, you’ll notice that it mainly requires the endogenous variable—that is, your time series data.

There is also an option to specify an initial level. By default, this is set to None, which means the model will infer it automatically. Conceptually, the initial level is just the first value of the time series. In our case, that happens to be 1750. It’s important to remember that while the initial level starts there, the level itself is not constant. It adapts and evolves throughout the time series.

All of the other parameters are optional, and for now, we won’t use them. We’ll focus only on passing in our time series.

So we apply simple exponential smoothing to our training data and then fit the model. That’s it—the model is now trained.

Once the model is fitted, we can inspect a summary of the results. Printing the summary gives us details such as the initial level and the smoothing parameter. You’ll see that the initial level is 1750, as expected. Again, keep in mind that this is only the starting point. The level itself is continuously updated as the model processes the data.

At this stage, it’s important not to overthink these coefficients. Exponential smoothing models are not designed to be highly interpretable. They are internal mechanisms that serve a specific purpose. The intuition is what matters: we use past values to predict the future, and we give more weight to recent observations.

For example, if the smoothing level is around 0.51, that simply means the model is weighting recent information slightly more than older information. It doesn’t carry deeper business meaning, and that’s perfectly fine.

Now let’s move on to forecasting.

We generate predictions by asking the fitted model to forecast a number of periods equal to the size of our test set. When we do this, you’ll notice something interesting: all the forecasted values are the same.

This is not a bug. This is exactly how simple exponential smoothing works.

Because the model only estimates a level—and no trend or seasonality—the forecast is flat. Each future value is based on the last estimated level. Since that level does not change once forecasting begins, all predicted values are identical.

You might be tempted to manually recreate the calculation using the last training value, the smoothing parameter, and the initial level. However, that won’t work correctly because the initial level is not the same as the final level used internally by the model. The true level is updated at every time step, and that internal value is what drives the forecast.

Next, let’s visualize the results.

We create a plot and set a reasonable figure size. Then we plot the training data, the test data, and the forecasted values. Finally, we add a title and a legend and display the plot.

When we zoom in on the results, the outcome becomes very clear. This is a terrible forecasting model.

And that’s okay.

Simple exponential smoothing is doing exactly what it is designed to do. It produces a flat forecast because it assumes no trend and no seasonality. Historically, this was one of the earliest forecasting models ever created.

Things will improve significantly when we move on to double exponential smoothing, which introduces a trend component, and then to triple exponential smoothing, which adds seasonality.

So trust the process. We are building this step by step.

If you have questions—especially about the level parameter—don’t worry too much about it. From a practical standpoint, it’s not something you need to interpret deeply. What matters is knowing how to fit the model and how to generate forecasts.

Remember: the flat forecast is expected, and it comes directly from the simplicity of the equation—a weighted previous value plus a level term.

That’s it for this video.

Until the next one—have fun.

# **I) Double Exponential Smoothing**

Let’s dive into double exponential smoothing.

You might notice the two stars next to the name and wonder what exactly makes it “double.” The answer lies in what the model is smoothing.

With simple exponential smoothing, we focused only on smoothing the data itself—essentially filtering out short-term noise. Double exponential smoothing adds a second layer. In addition to smoothing the level, it also models the trend in the data.

This means we’re no longer just averaging out highs and lows. We’re also capturing whether our sales or metrics are generally increasing or decreasing over time.

If we break down the mathematics conceptually, double exponential smoothing is built on two equations.

The first equation smooths the level. This part is very similar to what we saw with simple exponential smoothing. It represents our updated baseline and incorporates the most recent actual observation.

The second equation smooths the trend. Here, we measure how much the level changes from one period to the next. This change represents the direction and strength of the trend.

Because of this second equation, we introduce an additional parameter: beta.

Now let’s look at the components involved.

The smoothed level is our evolving baseline. It updates over time by incorporating recent actual values.

The smoothed trend captures how fast and in which direction the level is changing. It tells us whether the series is generally moving upward or downward.

Then we have alpha and beta, which act as tuning levers.

Alpha controls how much weight we give to recent observations compared to the existing level and trend. A higher alpha makes the model react more quickly to recent changes.

Beta controls how much weight we give to changes in the trend itself. A higher beta means the model adapts more quickly when the trend accelerates or slows down.

Let’s put this into context with an example.

Imagine your sales have been increasing overall, but with noticeable weekly ups and downs. Double exponential smoothing helps separate short-term fluctuations from the underlying upward movement. It allows us to see whether that growth is driven by a genuine trend rather than random variation.

As always, a word of caution.

While double exponential smoothing is excellent at capturing trends, it still does not account for seasonality. Regular patterns such as monthly or yearly cycles are not handled here. That’s something we will address next with triple exponential smoothing.

To wrap things up, double exponential smoothing smooths both the current values and the trend. It helps us understand whether we are “catching a wave” and, more importantly, which direction that wave is moving.

That’s enough theory for now.

Next, we’ll see how to implement double exponential smoothing in Python.

# **J) Python - Double Exponential Smoothing**

Now we’re going to work with double exponential smoothing in practice. We’ll build the model, generate predictions, and then visualize the results to see how well it performs.

Let’s get started.

We’ll use double exponential smoothing from statsmodels. To do that, we rely on the ExponentialSmoothing class. If you look at the documentation, you’ll see that the key input is the endogenous variable, which is simply our time series data.

We begin by building the model. We’ll call it something like model_double. The core input is our training data, which represents the observed time series.

What makes this double exponential smoothing is the inclusion of a trend component. This is where we explicitly tell the model to account for upward or downward movement over time.

For the trend type, we need to specify how the trend behaves. The most common options are additive and multiplicative.

An additive trend is typically used when the data follows a roughly linear pattern, where changes are fairly constant over time. A multiplicative trend is more appropriate when changes grow or shrink proportionally, such as exponential growth or decay.

In this case, the data appears fairly linear, so we use an additive trend.

We also explicitly set seasonality to none. That’s because we are not yet modeling seasonality—this will come later when we move to triple exponential smoothing (also known as Holt-Winters).

Once the model is defined, we fit it to the training data using .fit().

At this point, you can inspect the model summary if you want. However, in practice, the summary is usually not very insightful. The smoothing level and smoothing trend parameters don’t give us much actionable information. Exponential smoothing models are designed more for forecasting accuracy than interpretability, so it’s often fine to skip the summary altogether.

After fitting the model, we generate forecasts. We forecast the same number of periods as the length of the test set. This gives us a prediction series that we can directly compare against the actual values.

To really understand how the model behaves, visualization is essential.

We plot three things:

The training data

The test data

The forecast produced by the double exponential smoothing model

Once plotted, the behavior becomes much clearer.

What we see is that the model captures the overall trend quite well. As the trend rises and then stabilizes, the forecast reflects that movement. The predictions are smoother and more realistic than what we saw with simple exponential smoothing.

However, there is still an important limitation.

The model struggles with large spikes and dips in the data. These fluctuations are caused by seasonality, which this model does not yet handle. Double exponential smoothing accounts for level and trend, but not repeating seasonal patterns.

This explains why the forecast looks stable but fails to react to sharp periodic changes.

That missing piece—seasonality—is exactly what we’ll address next when we move to triple exponential smoothing. Once we add that third component, the model’s performance will improve significantly.

# **K) Triple Exponential Smoothing aka Holt-Winters**

Here is the same content rewritten into clear, structured, normal output, with smooth flow and no loss of meaning.

Let’s talk about triple exponential smoothing, more commonly known as the Holt–Winters method.

You can think of this as the big sibling of simple and double exponential smoothing. While simple smoothing handles level and double smoothing handles level plus trend, triple exponential smoothing is designed for data that includes trend and seasonality.

This is especially useful for patterns like customer complaints, sales, or demand data where we observe strong and repeating seasonal cycles. Ignoring seasonality in those cases leads to poor forecasts, which is exactly what we saw when using double exponential smoothing.

So how does Holt–Winters work?

In triple exponential smoothing, the time series is decomposed into three components:

Level
This is the baseline value of the series. It represents where the data is centered at any point in time and is the same concept we introduced with simple exponential smoothing.

Trend
This captures whether the data is generally increasing or decreasing over time. This component was added in double exponential smoothing.

Seasonality
This represents repeating patterns that occur at regular intervals, such as daily, weekly, monthly, or yearly cycles. This is what makes the method “triple.”

In other words:

Simple exponential smoothing → level

Double exponential smoothing → level + trend

Triple exponential smoothing → level + trend + seasonality

The Holt–Winters method uses three equations, one for each component. While we won’t go through the full mathematical equations here—since they become more complex—the intuition is straightforward.

First, the method updates the level, similar to simple exponential smoothing.
Next, it updates the trend, measuring how the level changes over time.
Finally, it adjusts for seasonality, accounting for recurring cycles in the data.

Just like the previous methods, Holt–Winters relies on smoothing parameters:

Alpha (α) controls the level

Beta (β) controls the trend

Gamma (γ) controls the seasonality

Each parameter determines how much weight is given to recent observations versus past information. These parameters can be tuned to improve model performance, but we’ll postpone parameter tuning until later in the course. For now, the focus is on understanding the structure and intuition behind the method.

From a practical standpoint, Holt–Winters becomes essential whenever your exploratory data analysis reveals strong seasonal patterns. If you see deep and regular cycles in your time series, double exponential smoothing will not be sufficient.

Common real-world examples include:

Forecasting electricity demand with daily or seasonal patterns

Predicting retail sales that spike during holidays

Modeling customer complaints that follow weekly or yearly cycles

In all these cases, incorporating seasonality is critical for accurate forecasting.

To conclude, Holt–Winters is a powerful and robust tool for modeling complex time series. By accounting for level, trend, and seasonality, it allows us to generate far more realistic and reliable forecasts.

Next, we’ll apply this method and see just how easy it is to implement in practice.

# **L) Python - Triple Exponential Smoothing aka Holt-Winters**

Now we’re going to work with triple exponential smoothing, also known as the Holt–Winters method. This technique is named after the two researchers who developed it: Holt and Winters.

At this point, we’re extending everything we’ve learned so far. We’ve already seen simple exponential smoothing (level), double exponential smoothing (level and trend), and now we’re adding the final piece: seasonality.

Let’s start by building the Holt–Winters model.

We define our model and specify the trend as additive. This makes sense because the overall movement in our data follows a fairly linear direction over time.

For seasonality, we choose a multiplicative structure. The reason for this is clear when we look at the data: as the trend increases, the seasonal peaks also become larger. This tells us that the seasonal effect grows proportionally with the level of the series, which is exactly what multiplicative seasonality is designed to capture.

Next, we specify the seasonal period. Since our data is weekly and there are 52 weeks in a year, we set the seasonal period to 52.

Once we fit the model, we immediately notice that the complexity increases. We now have smoothing parameters for:

The level

The trend

The seasonality

In addition, the model estimates initial seasonal values for each season. This added complexity is expected, because we are now modeling much richer behavior in the data.

After fitting the Holt–Winters model, we generate forecasts for the same number of periods as our test set.

To evaluate how well the model performs, we visualize the results by plotting:

The training data

The test data

The Holt–Winters forecast

When we focus on a single year—such as 2022—the improvement becomes very clear.

The model now captures not only the overall trend but also the seasonal cycles. Those recurring peaks and dips that were completely missed by simple and double exponential smoothing are now clearly reflected in the forecast.

This confirms what we suspected earlier: the missing piece was seasonality. By adding it, the model’s performance improves dramatically, even though the change in implementation is relatively simple.

The takeaway here is powerful. Incorporating seasonality can significantly improve forecasting accuracy when seasonal patterns are present in the data.

In the next step, we’ll move on to error measurement—how to quantify forecast performance and evaluate these models properly using Python.

# **M) Measuring Errors for Time Series Forecasting**

Now let’s talk about how to measure accuracy and errors in time series forecasting.

I’ll be honest—this may not be the most exciting topic, but it is one of the most important. No matter how sophisticated your model is, if you can’t measure its error, you can’t judge whether it’s actually useful.

The core idea behind error measurement is always the same, whether you’re working with regression or time series forecasting.

You have:

A model, which produces predictions (often visualized as a line)

Actual values, which represent what truly happened

The error is simply the difference between the actual values and the predictions. This difference is often referred to as the delta.

What changes is how we measure and summarize these differences.

Mean Absolute Error (MAE)

The first key metric is the mean absolute error.

Here, we calculate the difference between the actual value and the prediction, take the absolute value, and then average those values across all observations.

The absolute value is critical. It ensures that positive and negative errors do not cancel each other out.

For example:

If one error is +100 and another is −100, the average error would be zero

But the mean absolute error would still be 100

This makes MAE very interpretable. If the MAE is 2, it means that, on average, your predictions are off by about 2 units.

Root Mean Squared Error (RMSE)

The second major metric is the root mean squared error.

This metric also starts by computing the difference between actual and predicted values, but instead of taking the absolute value, it squares the differences, averages them, and then takes the square root.

Squaring the errors has an important effect:

Large errors are penalized much more heavily than small ones

This makes RMSE particularly useful when outliers matter and you want to strongly penalize large mistakes.

The downside is that RMSE is less interpretable than MAE. Once you start squaring and taking square roots, the final number doesn’t translate as intuitively into “average error.”

Comparing MAE and RMSE

Both metrics are widely used, and each has advantages.

MAE is easier to interpret and treats all errors equally

RMSE punishes large errors more strongly and is better when extreme mistakes are costly

In practice, it’s common to calculate both. However, when selecting or fine-tuning a model, RMSE is often preferred as the primary optimization metric because of its sensitivity to large errors.

Mean Absolute Percentage Error (MAPE)

The third metric is mean absolute percentage error, often abbreviated as MAPE.

Instead of measuring error in absolute units, MAPE expresses the error as a percentage.

This makes it very easy to interpret. For example, a MAPE of 10% means your predictions are off by about 10% on average.

However, MAPE has an important limitation.

It gives equal weight to all percentage errors, regardless of scale. For example:

Predicting 100 with an error of 10 results in a 10% error

Predicting 10 with an error of 1 also results in a 10% error

In many real-world scenarios, being off by 10 units on a large value is more significant than being off by 1 unit on a small value. MAE and RMSE reflect this difference, but MAPE does not.

Because of this, MAPE can sometimes be misleading, especially when values vary widely in magnitude.

What Is the Ideal Error?

This is one of the most common questions—and the answer is simple: there is no universal ideal error.

The acceptable level of error depends entirely on:

The business context

The use case

The tolerance of stakeholders

What matters most is improvement over time. As you use better data, better features, and better models, your error should decrease.

Determining whether an error is “good enough” is ultimately a business decision and should be discussed with stakeholders and senior decision-makers.

# **N) Python - MAE, RMSE, MAPE**

In this video, we’re going to do a very important task: analyzing our outcomes. Specifically, we’re going to measure the errors produced by our time series forecasting model.

To do this, we need a few specific functions. From sklearn.metrics, we’ll import the metrics required to evaluate our predictions. These include the root mean squared error (RMSE), the mean absolute error (MAE), and the mean absolute percentage error (MAPE).

Initially, there may be some confusion because earlier versions of sklearn did not include a direct function for RMSE. In those cases, people used the mean squared error and then applied a square root manually. However, now RMSE exists directly, so we can use it without additional tricks. Throughout the course, you may still see some references to mean squared error because it was commonly used before RMSE was added.

The key idea behind all these metrics is the same: we compare y_true (what actually happened) with y_pred (what our model predicted). This structure is always the same across all sklearn metrics. No matter which error metric you use, you pass in the actual values first and the predicted values second.

Once the functions are imported, we calculate and print RMSE, MAE, and MAPE using our test data and predictions. When we do this for our current model, we get results such as an MAE of around 366, an RMSE of around 424, and a MAPE of about 8 to 8.5%.

At this point, it’s useful to control how many digits we display. In practice, whether RMSE is 424.5 or 430 doesn’t really matter much. What matters is the scale of the error, not tiny decimal differences. So we usually limit the number of decimal places, especially for readability.

The same idea applies to MAE. For MAPE, since it represents a percentage, it makes sense to multiply it by 100 and display it as a percentage. We can also limit it to one decimal place, which is more than enough. For example, reporting a MAPE of 8.5% is much clearer and more useful than showing many decimal places.

Once this setup is done, we can start experimenting with different model configurations. For instance, we might switch the seasonality from multiplicative to additive and observe how the metrics change. When we do this, the results turn out to be much worse—MAPE jumps to around 12.7%, MAE increases to about 867, and RMSE to around 622. This clearly indicates that additive seasonality performs poorly for this dataset.

We can also experiment with different trend configurations, such as using a multiplicative trend. In this case, we might see a runtime warning during internal calculations, indicating numerical issues. However, despite the warning, the resulting error metrics may actually improve. This highlights an important point: we should be results-driven. If a model produces better evaluation metrics, it may still be worth considering, even if there are warnings, as long as the results are stable and sensible.

That said, these results are based on a single test set. Later on, we’ll cover cross-validation, where we measure errors across multiple test sets to ensure the model generalizes well.

The final step in this video is to build a reusable function that assesses the model and visualizes the results. This function takes the training data, test data, predictions, and an optional chart title. The title is not mandatory, allowing the function to be reused easily in different contexts.

Inside this function, we plot the training data, test data, and forecasted values on the same chart. We then add a title (if provided), a legend, and display the plot. After that, we calculate RMSE, MAE, and MAPE using the same metrics as before and print the results.

Once the function is defined, we can apply it directly by passing in the train set, test set, and predictions. This gives us both a visual comparison and numerical evaluation in one step. If we want to zoom in on a specific time range—such as starting from 2022—we can adjust the inputs accordingly. When we do this, we see very strong results, confirming that the model is performing well.

And that’s it for this video. This is how you evaluate a time series forecasting model, both visually and quantitatively. You now have a reusable function that you can apply to future models and datasets.

# **O) Python - Predicting The Future**

Sure 🙂
Here is the full explanation rewritten end to end, organized into clear multiple paragraphs, keeping all details intact and improving readability—without removing or adding meaning.

Welcome back. Let’s now move on to predicting the future.

At this stage, the idea is simple. Once you are satisfied with your modeling—once you’ve evaluated the errors, explored different configurations, and decided that the model is “good enough”—you move forward and use it to forecast future values. Throughout this course, we’ll continue to explore how models behave under different scenarios, but for now, we’ll take a straightforward approach.

The question we ask ourselves is: Are we satisfied with this model?
If the answer is yes, then we proceed to forecasting.

This step is meant to be easy. Since we’ve already compared models and visually inspected performance, we’re going to build a Holt-Winters model. The reason we choose Holt-Winters is simple: visually and quantitatively, it performed the best.

We build the Holt-Winters model using the complete dataset. We start by inspecting our DataFrame—just taking a quick look at the first few rows using .head() rather than displaying everything. From this DataFrame, we extract the target variable y and feed it directly into the model.

At this point, we choose between additive and multiplicative components. Based on our earlier analysis, multiplicative seasonality is already top of mind. There’s no need to overthink this here, especially since parameter tuning will be covered extensively later in the course.

To keep things simple, we copy the parameters we already identified as good and plug them into the model. Once the model is fit, we generate predictions and preview the first few forecasted values to confirm that everything looks reasonable.

Initially, it might seem like the predictions haven’t changed, which can be confusing. But this turns out to be a simple oversight—the model parameters were not actually changed. After rerunning everything correctly, it becomes clear that using additive seasonality results in a significant deterioration of performance, with MAPE jumping to around 12.7%.

This reinforces our earlier conclusion: multiplicative seasonality is the correct choice. It also aligns with what we observed visually—the seasonality in the data is clearly multiplicative.

Once the model is finalized, we proceed to forecasting. We generate a forecast for 13 periods ahead. This number is not random; it matches the length of the test set we used earlier. This alignment makes business sense—if we tested on 13 weeks, then predicting 13 future weeks is a logical next step.

After generating the forecast, we preview the first few values and then plot the training data alongside the forecast. Using plt.plot, we plot the historical y values and then overlay the forecast. This gives us a clear visual representation of how the model projects future behavior.

To make this reusable, we wrap the plotting logic into a function. We define a function called plot_future, which takes three inputs: the historical y values, the forecast, and an optional chart title (defaulting to None). Inside the function, we plot the data, apply the title if provided, and display the chart.

We then call this function using our DataFrame’s y, the forecasted values, and a title such as “Holt-Winters”. Just like before, if we want to zoom into a specific time range—say, from 2022 onward—we can apply .loc[2022:].

At first, this zooming doesn’t work as expected, which reveals a small bug in the function. The issue is that the function parameters were not fully dynamic. After correcting this and passing y properly, everything works exactly as intended.

In the end, it turns out that the model was right all along—the issue was simply in the function definition.

With that, we’re done. We’ve successfully predicted the future using our Holt-Winters model, and we’ve seen that doing so is actually very straightforward once the model is finalized.

Next, we’re going to shift focus to daily data, which is one of the most common and most challenging types of time series data. Daily data comes with its own quirks and considerations. To explore this properly, we’ll work with a Bitcoin dataset and see how forecasting works in that context.

# **P) Python - Daily Data**

We’ll start by loading the Bitcoin price dataset. To do this, we use pandas.read_csv and load the file named Bitcoin_price.csv. While loading the data, we set the index column to date and enable parse_dates=True. This ensures that the date column is properly recognized as a datetime index, which is exactly what we want for time series analysis.

From the dataset, we only need one KPI. Although there are many available, we’ll focus on the adjusted close price. This allows us to zoom in on exactly what we need without unnecessary columns. We store this series in a variable called daily_data. For convenience, we can also rename the series so it’s easier to work with going forward. If desired, all of this could even be done in a single line of code.

Once the data is loaded, it’s always good practice to do a quick preview to confirm everything looks correct. After that, we check the index. One important thing to verify with daily data is whether the frequency is properly set. Often, the frequency is missing, even if the dates are continuous. To ensure completeness and avoid issues later, we explicitly set the index frequency to daily. After doing this, the data is exactly in the format we want.

Next, we move into model assessment. We decide to set aside the last 30 days as our test set. We define periods = 30, then split the data accordingly. The training data includes everything except the last 30 observations, and the test data consists of those final 30 days.

At first, an error appears indicating “too many indexers.” This happens because we’re working with a Series rather than a DataFrame. Once we adjust the slicing accordingly, everything works as expected. We quickly verify the training and test data and confirm that the test set starts on December 1st, which is exactly what we want.

With the data split correctly, we proceed to build a Holt-Winters model for daily data. This is where an important discussion comes in. Daily data often contains multiple seasonal cycles. However, exponential smoothing—specifically Holt-Winters—only allows for one seasonal cycle, so we must choose carefully.

At this point, we need to decide what seasonal period makes the most sense. Should it be 365 days for yearly seasonality, or 7 days for weekly seasonality? In many cases, weekly (intra-week) seasonality is stronger and more important, so it’s common to start with a seasonal period of 7. Yearly seasonality can also be meaningful, but only if it’s very clearly defined. Ideally, we’d capture both—but Holt-Winters doesn’t allow that. More advanced models will, and we’ll explore those later.

We start by fitting the model to the daily data. During fitting, we see a convergence warning. This warning essentially tells us that the model is struggling to fit the data well. We can experiment by changing the trend type, for example trying a multiplicative trend, but we still see warnings. This isn’t entirely surprising—cryptocurrency data is extremely volatile and inherently difficult to predict.

Despite this, we proceed to generate predictions using the Holt-Winters model. Even here, the convergence warning persists. This reinforces an important reality: crypto prices are noisy, volatile, and very challenging for traditional forecasting models.

Next, we assess the model using our reusable model assessment function. We pass in the training data, test data, and the daily predictions. We also zoom in on the test period to better visualize the results.

Interestingly, the MAPE comes out at around 4.7%, which looks quite good at first glance. However, when it comes to stock or crypto prices, even small errors can be problematic. Trading involves taxes, transaction fees, slippage, and opportunity cost. In practice, errors often need to be closer to 1–2% to be truly useful.

This is a good moment for an important caution. If someone claims they can reliably predict the stock or crypto market, you should be extremely skeptical. Markets are influenced by countless unpredictable factors, and no model can consistently forecast them with high accuracy.

Visually, the results look okay. We see the test data alongside the forecast, and the model does a reasonable job of following the general movement. When we zoom further into the period starting from late November 2023, the performance still appears acceptable.

However, from a practical standpoint, this is where things become tricky. You might see the forecast indicating an increase, decide to buy, and then wait several days only to find that the price never reaches the predicted level. By the time the forecast turns downward, you’ve already missed opportunities or incurred losses. This highlights just how difficult real-world trading decisions are.

To further experiment, we change the seasonal period to 7 days and refit the model. Immediately, the results look very different. The MAPE increases to around 5.2%, and the forecast line changes significantly. This suggests that there isn’t a strong weekly seasonal cycle in the data, which makes sense—traditional stock data does not have a weekly cycle, and Bitcoin’s weekly behavior is debatable at best.

This experiment highlights an important takeaway: changing the seasonal period can drastically alter the model’s behavior. When using Holt-Winters, it’s crucial to experiment and let the data guide your decisions rather than relying on assumptions.

As we move forward, we’ll explore more advanced models that allow for multiple seasonalities and greater flexibility. That’s where things get really exciting. If this already feels interesting, the upcoming models will open up even more possibilities.

# **Q) Python - Working on the Useful Code Script**

Welcome back! To wrap up this video, let’s focus on building on our template. We’ll go to our main template file, specifically the useful code template, and start enhancing it. One important addition we can make is including extra libraries. I’ll add everything initially and then remove what’s unnecessary. For example, I’ll remove the Holt-Winters-specific libraries, since they are very model-specific. Everything else stays, and that gives us a clean, reusable template.

Next, we ensure proper renaming and organization. This includes adding placeholders like XXX for parts that may need future adjustments. Frequency handling is another key part—at some point we worked with time series frequency, so it’s useful to include that as a function in the template. This allows you to change the frequency dynamically, rename your time series to Y, set the index when importing data, and easily plot the series. These improvements make the template more flexible and user-friendly.

We also aim to improve the plotting and documentation. By passing the plotting and template sections through an AI or a documentation improvement step, the template becomes cleaner, easier to read, and more professional. The goal is to have a ready-to-use template that you can adapt for various projects while keeping the code organized and comprehensible.

Next, we include useful functions like model_assessment. Adding these directly into the template ensures that you can evaluate models, visualize train/test/forecast splits, and calculate error metrics quickly, without rewriting the code each time. One limitation I noticed in Colab is that building a simple library of reusable functions isn’t as smooth as in Jupyter or other environments. In Jupyter, you can build a lightweight library over time and import it easily. Colab doesn’t make this as seamless yet, but including functions in a template is a practical workaround.

We also keep the plot future function in the template. This allows for easy visualization of forecasts whenever we want to predict future data. By having both model_assessment and plot_future in the template, you now have a foundation for end-to-end time series analysis: importing data, preprocessing, building models, assessing them, and forecasting.

Finally, we save this as useful_code_template_final. This final version of the template is clean, reusable, and ready for use whenever you start a new time series project. It’s flexible enough to adapt as your workflow evolves, and over time you can continue to improve it. I hope you had fun building this with me, and I’m looking forward to seeing you in the next video!

# **R) Holt-Winter Pros and Cons**

Hey everyone! In this video, I’m going to walk you through the pros and cons of the Holt-Winters method, breaking it down into key advantages as well as its limitations.

As a general introduction, Holt-Winters is a favorite in the forecasting world. If you have a problem that isn’t overly complex but exhibits trend and seasonality, Holt-Winters does a very good job. Its first advantage is that it is simple to implement. The method is straightforward—you don’t need a PhD to get it up and running—which makes it accessible for many people. I also find it quite intuitive: the model’s logic, which revolves around the current level, the trend, and the seasonality, resonates easily.

The parameters we discussed—alpha, beta, and gamma—exist in the model, but for a simple implementation, we often don’t even need to tune them. This simplicity makes the model highly adaptable to changes. Because the method emphasizes recent past observations, it naturally adjusts to shifts in trends and patterns, making it responsive to new data.

However, there are some limitations to be aware of. First, Holt-Winters has only one seasonal component. For daily data, for example, you might need to choose between weekly or yearly seasonality, but it cannot account for both at the same time. This limitation is shared with older methods, including models from the ARIMA family, which can struggle with complex time series.

Another limitation is that Holt-Winters cannot incorporate external regressors. Factors such as weather, market conditions, or major events like Covid cannot be used to refine forecasts. The method relies entirely on historical time series data, which may not always be sufficient for accurate prediction in real-world scenarios. When multiple seasonalities or external drivers are important, Holt-Winters can fall short.

In conclusion, Holt-Winters stands out as a reliable and quick-to-use forecasting method. You can put it in place easily and quickly gauge how predictable your data is. That said, it’s important to recognize that it may not always be a perfect fit. For complex scenarios with multiple seasonalities or external influences, you may need more advanced models.

That wraps up this section. Until the next video, have fun exploring time series forecasting!

# **VII) Section 7: HOLT-WINTERS CAPSTONE PROJECT: Air miles**

# **A) Capstone Project Presentation**

Welcome to this capstone project.

In this video, I’m going to walk you through the six tasks that you need to complete. To take on this challenge, you essentially have two options for your working environment. You can either use your own setup—something like Google Colab, which is what I’ll be using—or you can use the Udemy workspace that’s provided. The Udemy workspace is mainly there so you can review and work through the tasks themselves, but all the necessary information is also explained here.
Alright, let’s get started.

The challenge you’ll be working on is centered around air miles data. It’s a very straightforward dataset that contains the number of air miles accumulated per month over a period of roughly ten years. There are six main tasks that you need to complete as part of this project. Along with that, you also have access to starter code, which is provided in the workspace. This starter code is also available to you outside the workspace, and it’s essentially the same in both places.
If you decide to use Google Colab, the main extra step you’ll need to take is setting up the environment. If you’re using another setup, that may not be necessary. In any case, you’ll find the required libraries, the dataset itself, and a few initial code snippets that are meant to help you get started. All of this is also preconfigured in the Udemy workspace, along with the dataset.
Now, let’s go through the tasks one by one.
The first task is to set the data frequency. This is clearly indicated in the instructions. You need to specify the frequency as monthly start. Completing this correctly is your first task.

After that, the second task is to visualize the data. The main purpose here is to really become familiar with the dataset—think of this as exploratory data analysis. In my own solution, I’ll be doing this step by step. For your part, you can keep it simple if you want, such as basic plots. You can also go further and include seasonal decomposition, autocorrelation and partial autocorrelation plots, monthly plots, quarterly plots, or a combination of these. I would recommend doing as many as possible, because each of these visualizations provides complementary insights into the data.

The third task is very straightforward. You need to split the data so that the last 12 months are used as the test set, and all the remaining data is used for training. The idea behind this is to have an unbiased way to evaluate the model. Typically, the test set should consist of the most recent observations, because that’s where, in real-world scenarios, we most want our forecasts to be accurate.

Task number four is where you build the model. Here, you’ll create a Holt-Winters model. You’ll need to specify the type of seasonality—whether it’s additive or multiplicative—set the seasonal period, and then fit the model to the training data.

Task number five is about forecasting. Using the fitted model, you’ll predict the next 12 months of air miles. You’re also encouraged to visualize the forecasted values along with the historical data, if you’d like, to better understand how the model is behaving.

Finally, the sixth and last task is model evaluation. Here, you assess how well your model performed. You can use mean absolute error, or you can use the three commonly used metrics: root mean squared error, mean absolute error, and mean absolute percentage error. You may also choose to visualize the errors, but the key objective is to measure the model’s error and clearly present it to the user.

So, to summarize, there are six tasks in total. You’ll find the solutions for these tasks in the task folder, with separate code files for task one through task six. There is also an overall script that brings everything together.

And that’s it. I hope everything is clear. If you have any questions, feel free to ask them in the next video. In the next session, I’ll start from the starter code and solve the capstone project step by step with you. I’ll see you in the next video.

# **B) Python Solutions: Task 1 and Task 2**

All right, let’s solve this together.

I went ahead and ran the initial setup. I set the directory in my Google Colab environment, and inside it we can find our data file, air.csv. I also set the index to be the date column, enabled date parsing, and specified dayfirst=True. What we now have is monthly data ranging roughly from 1996 to 2005, with about 113 entries in total. I then renamed the main series to Y.

Now let’s take a look at the tasks.

The first task is to set the data frequency and use MS (monthly start). Let’s give this a try. For task one, we set the index to monthly frequency using MS. That’s the very first step. I apply this directly on the DataFrame index, and once that’s done, everything looks good and ready to go.

With task one completed, we can move on to task two, which is data visualization and exploratory data analysis. The goal here is really to understand the data deeply.

The first visualization is a simple time series plot. We plot dataframe.Y and display it. After adjusting the figure size to make it clearer, we can immediately see a few important patterns. There’s a clear upward trend over time, and the data also appears to be strongly seasonal.

If we look closely, there’s a noticeable dip around 2001. This aligns with the September 2001 terrorist attacks, which understandably reduced people’s willingness to fly. This real-world event is clearly reflected in the data.

Next, we build a monthly plot. We plot the monthly seasonality of dataframe.Y and set the y-axis label to “Monthly Air Miles.” From this visualization, the seasonality becomes very clear. Air miles are highest roughly between March and August, while the lowest values appear between September and February.

To further confirm this pattern, we move on to a quarterly plot. We resample the data by quarter and then plot it. Initially, we run into a small issue because resampling requires an aggregation function, so we apply a sum. Once that’s fixed, the plot works as expected. From this view, Q1 shows the lowest values, Q4 is slightly higher, and Q3 has the highest seasonal peak.

Next, I like to perform seasonal decomposition. Since the data shows that as the trend increases, the magnitude of the seasonal fluctuations also increases, it suggests that a multiplicative model is more appropriate than an additive one. We perform seasonal decomposition with a period of 12 and set the model to multiplicative.

When we plot the decomposition, we can clearly see the upward trend, a dip around 2001, and then a recovery afterward. The seasonal component fluctuates roughly between 0.9 and 1.1, which means about plus or minus 10 percent seasonality. The residuals are centered around 1, which indicates a very reasonable and clean decomposition. Overall, this looks like a good fit for the data.

There are two more plots I want to include here. The first is the autocorrelation plot (ACF). When we plot the ACF for dataframe.Y, we see strong peaks around lag 12, as well as at multiples of that lag. This clearly indicates strong seasonality. We also see that most correlations die off quickly, which suggests that there isn’t an extremely strong long-term trend component.

Finally, we look at the partial autocorrelation plot (PACF). Here, we see significant information around lags of 6–7 months and again around 12–13 months. This also aligns very well with the seasonal nature of the data and further confirms our earlier observations.

With that, we’re done with task one and task two. In the next step, we’ll move on to task three, which is creating the training and test sets. I’ll see you in the next video.

# **C) Python Solutions: Task 3 and 4**

The next step is to move on to task three, which is creating the training and test sets.

For this task, we’re going to define the test set as the last 12 months of data, and the training set as everything that comes before that.

Let’s get started by setting up the test set. We take the DataFrame and slice it so that everything up to index -12 becomes the training data, and from -12 to the end becomes the test data.

This approach works quite well and is very clean. Once this split is done, task three is effectively completed, because that is all the task requires.

Next, we move on to task number four, which is to build the Holt-Winters model.

In this task, we build the Holt-Winters model using the time series data from the training period only. It’s important to note here that Holt-Winters does not allow external regressors, so we are working purely with the time series itself.

Now let’s build the Holt-Winters model. We create the model using exponential smoothing and pass in dataframe.Y as the input series.

For the seasonality, we initially consider a multiplicative form because earlier analysis suggested multiplicative seasonality. However, when we look at the trend more closely, it appears to be fairly linear.

Since linear trends typically work well with additive seasonality, we decide to go with additive seasonality instead.

After setting up these parameters, we fit the model by calling .fit(). Once this runs successfully, the model is built.

At this point, task four is complete. We’ll stop here for now and come back in the next session to wrap things up with tasks five and six. I’ll see you then.

# **D) Python Solutions: Task 5 and 6**

Task five is focused on forecasting.

In this step, we are going to generate predictions using the model we built earlier. Since task six is about accuracy assessment, we first need forecasts to compare against the test data. Specifically, we are going to predict the next 12 periods using the trained model.

We start by taking the fitted Holt-Winters model and calling the forecast method. To keep things clean and flexible, the number of steps is set automatically based on the length of the test dataset. This ensures the forecast aligns perfectly with the test period. We then store these predictions and label them clearly as Holt-Winters predictions.

At this point, we have our forecast values, which means we are ready to move on to task six: accuracy assessment.

For evaluating the model, we compute error metrics. You can absolutely use pre-built metric functions, and they work quite well. Over time, though, I tend to rely less on them. The reason is that in environments like Colab, especially when working alongside large language models, natural-language-driven exploration often feels more intuitive and flexible.

We assess the model using MAPE and RMSE as our primary metrics. After computing them, we display the results to the user. The MAPE turns out to be around 0.03, which corresponds to roughly a 3% error. That’s an extremely small error and indicates that the model is performing very well.

Next, we visualize the results. We plot the training data, the test data, and the model’s predictions on the same graph. When we look at the plot, we can clearly see that the predicted values are very close to the actual test values. This visual confirmation reinforces what the error metrics already told us.

You’re welcome to customize this plot further—styling, colors, labels, and so on—but even in its basic form, it communicates the model’s performance quite effectively.

Now, let’s do one final bonus step: forecasting the future. After all, this is really the main reason we build forecasting models in the first place.

To do this, we rebuild the Holt-Winters model using the complete dataset, not just the training portion. We fit the model on all available data and then forecast the next 12 months into the future. These become our future predictions.

Finally, we visualize the full historical data together with the future forecasts. This gives us a clear picture of how the model expects the series to evolve going forward.

This wraps up how we assess the model and how we generate future forecasts. Feel free to experiment with the model settings we defined earlier in task four—such as trend and seasonality types—and see how they impact accuracy. Exploring these variations is something we’ll dive into more deeply later in the course.

For now, the key takeaway from this project isn’t just the Holt-Winters model itself. It’s the full workflow: setting the correct frequency, performing solid exploratory data analysis, splitting the data properly, and evaluating results thoughtfully. These building blocks are the real foundation.

And with that, we’re done with the project. If you have any questions, let me know, and I’ll see you in the next video.

# **VIII) Section 8: ARIMA, SARIMA and SARIMAX**

# **A) Game Plan for ARIMA, SARIMA and SARIMAX**

In this section, we’re going to learn some very important foundational models in time series analysis: ARIMA, SARIMA, and related variants. These are models you absolutely need to understand. Alongside them, we’ll also master several core concepts that form the foundation of time series forecasting. Everything we’ve learned so far leads into this part of the journey.

This phase of time series analysis is really about strengthening your forecasting skills before we move on to more complex models. If you want to truly understand ARIMA and its extensions, you’re definitely in the right place. ARIMA stands for Autoregressive Integrated Moving Average, which is quite a mouthful. On top of that, we also have SARIMA and ARMAX, and we’re going to learn all of them, model by model.

We’ll initially put most of our focus on ARIMA, because it contains the majority of the core concepts you need to understand time series modeling. Once you truly understand ARIMA, moving on to SARIMA and ARMAX becomes much easier. For more advanced time series forecasting, especially when seasonality and external factors are involved, we’ll rely more heavily on SARIMAX. But again, if ARIMA makes sense to you, the rest will feel very natural.

It’s really important that by the end of the ARIMA section, you feel confident with it. If you don’t, come to me and I’ll help you. Getting ARIMA right makes everything that follows much simpler and more intuitive.

Of course, we won’t rely on ARIMA alone. ARIMA is the simplest of these models, and as we’ll see, it has limitations. In real life, most data is seasonal. A classic example is ice cream consumption—you don’t eat ice cream all year long. That’s the perfect illustration of why we need seasonal models like SARIMA. On top of that, SARIMAX allows us to incorporate external factors, which are often crucial.

To make this concrete, imagine I live in Berlin. It’s summer while I’m recording this, but it’s raining. Do I want ice cream? Absolutely not. Even though it’s summer, the external factor—rain—changes my behavior. That’s exactly why models with exogenous variables are so powerful.

Throughout this section, we’ll apply these models to real-world data. We’ll explore each model carefully, understand when to use which one, and learn how to choose the best model—typically the one with the highest accuracy. We’ll also spend time tuning our models using techniques like cross-validation and parameter optimization. We’ll really go into the details here.

There’s one more important thing I want to point out. This is probably the third or fourth iteration of this SARIMAX section. The course was launched a few years ago, and I’ve continuously updated it. This is actually one of the biggest updates so far. Previously, I was using the PMDARIMA library quite heavily. Unfortunately, its last update was in 2023, and parts of it no longer work properly, especially in Colab environments.

Because of that, we’re moving away from PMDARIMA. If at any point in this section something looks odd or broken, please let me know. I do my best to keep everything up to date, but I’m not perfect. I’ve remade almost all of the Python-related videos from scratch, and I’ve updated the others where it made sense. Some parts remain unchanged simply because ARIMA itself dates back to the 1960s, and the core ideas haven’t really changed.

Going forward, we’ll be using statsmodels, which we’ve already used earlier for things like seasonal decomposition and plotting. It’s a very strong and reliable library for time series analysis. While the older library had some convenient extra features, moving on is part of the natural evolution of the course—and that’s perfectly fine.

I’m genuinely excited about this section. We’re going to explore these models in depth, really push our understanding, and make sure you’re set up for success. Most importantly, I want you to succeed, and I’ll be here to help you every step of the way.

If you have any questions, let me know, and I’ll see you in the next video.

# **B) CASE STUDY BRIEFING: Predicting Daily Revenues**

In this video, I’ll introduce you to the case study for this section.

Imagine this scenario: you are the owner of a chocolate retail shop, and your goal is to predict your daily revenue. You might ask why this is so important. The answer is simple—understanding and managing cash flow is critical for any business. If you know how much you are likely to earn, you can plan in advance how much stock you need and how many people should be working in the store.

When you picture yourself running this shop, it becomes clear why forecasting matters. Every single day requires planning. Forecasting allows you to prepare for what’s coming, so your shop can stay one step ahead instead of reacting at the last minute. By predicting daily revenue—whether it’s for the next day or the next 30 days—you can make much smarter decisions.

This directly leads to fewer unsold chocolates and better staffing decisions. You don’t want excess inventory sitting on the shelves, and you also don’t want employees standing around without work. Idle staff means wasted money, which is a loss for the company. Accurate forecasts help ensure that people are working when they are actually needed.

As a result, inventory management also improves. You order the right amount of chocolate at the right time, reducing waste and improving efficiency. Overall, this means the business runs more smoothly—and in this case, a bit more sweetly as well.

So what is your role in this case study? You are acting as an analyst or data scientist. Your goal is to analyze past sales data, identify patterns, and forecast future sales. You can also take special events into account, such as weekends or holidays. For example, Valentine’s Day is a huge event for chocolate sales, and ignoring it would lead to poor forecasts.

This case study is especially exciting because it clearly shows how data can influence real business decisions. We’re not just applying algorithms for the sake of it. We’re learning about consumer behavior, understanding seasonality, and finding the right balance between demand and supply.

By the end of this case study, you’ll see firsthand how to forecast the future using ARIMA, SARIMA, SARIMAX, and external regressors. You’ll understand how these models can be applied in a real-world scenario, in a way that closely mirrors what you would do in a day-to-day job as a data professional.

Most importantly, you’ll see just how relevant and impactful this skill is. Forecasting isn’t just theoretical—it can directly affect business performance and decision-making.

That’s it for the introduction. I hope this motivates you. Let’s get started, and I’ll see you in the next video. Have fun!

# **C) Python - Setting Up ARIMA**

Welcome to this first tutorial on ARIMA.

Inside the Time Series Analysis section, you’ll find a folder dedicated to ARIMA, SARIMA, and SARIMAX. In that folder, you’ll also find a starter file. This is a very simple setup that I prepared for you, containing some of the functions we’ve already built and used earlier.

One of the first things you’ll notice is that we connect to the drive. This is something you may need to adjust depending on your setup. You’ll need to mount your drive and possibly change the folder path, but by now you should already be comfortable doing that.

Let’s briefly review the functions and libraries used here. We import pandas and matplotlib, and we also bring in several plotting and modeling utilities from statsmodels. Some of these, like seasonal decomposition, we’ve already used. Others are commented out for now and will be activated later when we need them, such as SARIMAX. We also see imports like ParameterGrid, which we haven’t used yet, and NumPy, which is a standard dependency you’ll see often.

After running the setup, we load the chocolate sales dataset. The index column is the date column, which is why it’s set as the index. We also enable date parsing. However, there’s an important additional detail here: the date format. Because the dates in the CSV file are day-first, we explicitly set dayfirst=True. This ensures the dates are interpreted correctly.

It’s very important to always double-check the date format. If the date is parsed incorrectly, everything downstream—seasonality, trends, forecasts—will be wrong. In this dataset, the original date format could easily be misinterpreted as month-first, so correcting this is a crucial step.

The dataset contains revenue, discount rate, and coupon rate. These are fairly straightforward. The discount rate refers to the visible discount applied to a product, while the coupon or voucher rate is applied later, typically at checkout. These are different mechanisms and often influence customer behavior in different ways.

To make this more concrete, think of an online store like Amazon. A discount might be displayed directly on the product page, such as “15% off,” while a coupon might apply an additional percentage reduction during checkout. These two concepts are handled differently and can have different impacts on sales.

Some preprocessing has already been done in the dataset. For example, the revenue column originally contains commas, which causes it to be read as an object rather than a numeric type. To fix this, we remove the commas from the strings and then convert the values to floats. Once that’s done, the revenue column becomes numeric, which is exactly what we want.

Next, we set the data frequency to daily and rename the main target variable to Y. Setting the correct frequency is always important in time series analysis. We also briefly review the possible frequencies that could be used, depending on the dataset.

Now we move into exploratory data analysis. When we visualize the data, we can see that revenue is generally growing over time. There are also clear spikes at certain times of the year, which strongly suggests seasonality. These spikes repeat year after year, reinforcing the idea that seasonal effects are present.

Looking at the monthly plot, we can clearly see the seasonal ups and downs. Revenue tends to be higher in the second quarter, lower in the first and third quarters, and then rises again in the fourth quarter—especially in November, which shows a very strong seasonal spike.

The quarterly plot confirms this observation. Q1 and Q3 are relatively similar, Q2 acts as a transition, and Q4 consistently shows higher values. This kind of confirmation across multiple plots is exactly what we want during EDA.

We then perform seasonal decomposition using a multiplicative model. The reason for choosing multiplicative seasonality is the presence of large spikes—the size of the seasonal effect grows with the level of the series. The trend component shows steady growth that eventually stabilizes, while the seasonal component exhibits strong recurring spikes. The residuals still contain some large values, indicating that there is room for improvement in the model.

Next, we look at the autocorrelation function. This clearly shows strong seasonal behavior, with spikes at regular intervals such as 7, 14, and 21 days. These repeating patterns confirm that the data is deeply seasonal and that a seasonal component will be essential in our modeling.

The partial autocorrelation plot provides even more insight. We see strong relationships with values from one day ago, two days ago, five days ago, seven days ago, and even up to two weeks back. This again highlights the strong seasonal and short-term dependencies in the data.

At this stage, we may start guessing initial parameters for our models, such as autoregressive and seasonal terms. However, it’s important to stress that visualization is just the starting point. These guesses are not final decisions.

The key takeaway is that while visualization helps us build intuition, our final parameter choices should be driven by data and metrics, not just visual inspection. As we progress through the tutorial, we’ll move away from guessing and toward data-driven decisions. This brings clarity, confidence, and a much stronger foundation for our models.

We’ll stop here for now. There’s a lot more to learn, and the next steps will be both detailed and fun. I’ll see you in the next video.

# **D) ARIMA**

In this video, we are going to cover the ARIMA concepts, and don’t worry if it feels a bit complicated at first. I’ll break everything down in a very simple and intuitive way so you can follow along comfortably.

ARIMA stands for Autoregressive Integrated Moving Average, and it is one of the most popular models in the forecasting world. The reason for its popularity is that it is relatively easy to apply, and the intuition behind it is also straightforward once you understand the basics.

Apart from ARIMA, there is also an extended version that deals with seasonality. This additional layer makes the model extremely useful for data that follows seasonal patterns, such as monthly sales, yearly demand, or weekly traffic. On top of that, ARIMA can also be enhanced by including external factors, known as exogenous regressors, which allow us to incorporate outside influences into our forecasts.

Although all of this might sound complex at first, the good news is that ARIMA is actually very easy to work with in practice. For this particular video, however, we will focus only on the core ARIMA model.

ARIMA has three main components.

The first component is Autoregressive. This part is essentially about looking at the past to predict the future. In other words, we use previous values of the time series to forecast what comes next. If past values have a strong influence on future values, this component captures that relationship.

The second component is Integrated, and this is where stationarity comes into play. A stationary time series is one where the mean, variance, and covariance remain constant over time. Simply put, stationarity means the data follows a consistent pattern that can be predicted. The integrated part of ARIMA helps us transform non-stationary data into stationary data, usually through differencing. We will cover this topic in detail, and in the practice videos, we will place special emphasis on stationarity and the integrated component.

The third and final component is the Moving Average, and this part is quite clever. Instead of only using past values, the model also looks at past errors. Yes, the mistakes the model made in earlier time steps are treated as valuable information and are used to improve future predictions.

From a more mathematical perspective, an ARIMA model is typically written as an equation. In this equation, y(t) represents the value of the time series at time t, which is what we are trying to forecast. The alpha term is the constant or intercept, acting as a baseline starting point. Then we have coefficients for the autoregressive part, which indicate how much influence previous values such as y(t−1) have on the current forecast. We also include coefficients for the moving average part, which determine how much influence past errors should have.

Finally, there is the error term, which represents everything that is not explained by the constant, the autoregressive terms, or the moving average terms.

Even though this may seem mathematically heavy, the core idea is actually simple. A forecast at time t is made up of a baseline value, plus information from the most recent past values, plus information from the most recent errors.

In the next videos, we will zoom in on each of these components individually and build a strong intuition around them.

# **E) Auto-Regressive**

After we have a good grasp of ARIMA, let’s zoom in on each of its components, starting with the Autoregressive part, or AR for short.

In many ways, the autoregressive component is the heartbeat of ARIMA. Understanding it properly is absolutely crucial if you want to understand ARIMA as a whole. So let’s break it down step by step and see what it’s really about.

The core idea behind autoregression is very simple: the past influences the present. Imagine you are trying to predict how much coffee you’ll drink tomorrow. A very reasonable place to start would be to look at how much coffee you drank over the past few days. If you drank a lot yesterday and the day before, chances are you’ll drink a similar amount tomorrow. This intuition is exactly what the AR component captures.

In technical terms, the AR component looks at past values of the time series and uses them to predict the next value. In our coffee example, past daily coffee consumption helps us forecast tomorrow’s consumption.

From a more mechanical perspective, the autoregressive model works using lags. A lag is simply a previous data point in the time series. You can think of a lag as a step backward in time. A lag of 1 means we are looking at yesterday’s value to predict today. A lag of 2 means we are looking at the value from the day before yesterday, and so on.

Within the ARIMA framework, the autoregressive part is represented by the P in the model notation. The value of P tells us how many lagged values of the series we are going to use. For example, if we choose an ARIMA model with P = 2, it means we are using the last two observations—lag 1 and lag 2—to predict the future.

In this case, the model would look something like this: today’s coffee consumption is equal to a constant term (often called alpha) plus a coefficient multiplied by yesterday’s consumption, plus another coefficient multiplied by the consumption from the day before yesterday, and so on. The number of coefficients depends on how many lags we include. With two lags, we have two different coefficients—one for yesterday and one for the day before yesterday.

The constant term, alpha, acts as a baseline level, while the coefficients determine how strongly each past value influences the current prediction. These coefficients are usually denoted by symbols such as φ (phi), δ (delta), or ω (omega), depending on notation, but the idea is always the same: they measure the influence of past values on the present.

Autoregression is fundamental to time series modeling because time series data is all about patterns over time. In many real-world problems—such as coffee consumption habits or daily chocolate revenue predictions—the past contains valuable information about the future. If a problem had no autoregressive component at all, it would mean that the past provides no useful signal for prediction, which rarely makes sense in time series analysis.

If you’re wondering whether autoregression has something to do with autocorrelation, the answer is yes. They are closely related. Autocorrelation tells us how strongly current values are related to past values, while autoregressive models explicitly use that relationship inside the ARIMA framework. In a sense, they describe the same underlying information from two different perspectives.

So, in a nutshell, the autoregressive component uses lagged values of the time series to help predict the future. That’s the essence of AR.

Next, we’ll move on to the Integrated part and see what that’s all about

# **F) Integrated**

Let’s now move on to the second component of the ARIMA model: Integrated.
This part plays a key role in making our predictions as accurate and reliable as possible.

So what does Integrated actually mean, and how does it relate to stationarity?

A good way to think about a stationary time series is to imagine a reliable train moving at a steady pace. Its speed doesn’t suddenly change, and neither do its characteristics. In statistical terms, a stationary time series has a constant mean, constant variance, and constant covariance over time. In simple words, it doesn’t swing wildly. Its patterns are stable, and that stability makes it much easier to predict what will happen next.

The challenge is that, in practice, most time series data is not stationary. This is where the concept of differencing comes in.

If our data is not steady—for example, if it has a clear upward or downward trend, like we often see with financial data such as Bitcoin prices—we need to apply differencing. Differencing helps smooth out the data. What we do is subtract one day’s value from the previous day’s value. The result is a new series that is much more stable and, therefore, more predictable.

We’ll explore this visually in the practical tutorial, where I’ll show you what a raw time series looks like and how it changes after differencing is applied. For now, it’s enough to understand the idea: differencing removes trends and stabilizes the data.

Now let’s connect this back to ARIMA.
The Integrated component tells us how many times we need to difference the data to make it stationary. If we difference the data once and that’s enough to stabilize it, then the model has an I value of 1. In other words, we would be working with an ARIMA model where I = 1.

To build intuition, imagine four different time series:

The first has a fairly stable pattern over time. This one would be considered stationary.

The second steadily grows over time. This means its mean changes, so it is not stationary.

The third shows fluctuations with changing amplitudes—think of seasonal sales that peak and dip at different levels each year. Since the variance changes, this series is also not stationary.

The fourth has cycles of different lengths and irregular ups and downs. Because there is no consistent structure, this series is not stationary either.

In real-world scenarios, this becomes tricky because most real-world data is not stationary. Financial data, revenue data, sales data, and almost anything that represents a flow over time usually changes its behavior. These series don’t follow a clean, stable pattern that’s easy to predict.

That’s exactly why differencing is so important in ARIMA. It transforms messy, time-varying data into something more manageable—something we can actually model and forecast.

A natural question at this point is: How do we know if our data is stationary?
There is a formal statistical test for this called the Augmented Dickey–Fuller (ADF) test. It allows us to check whether the mean, variance, and covariance are constant over time. In the next video, we’ll walk through this test and I’ll show you how to automate the process so you don’t have to rely on guesswork.

From experience, a practical mindset is to assume that most time series are not stationary and to rely on differencing as a tool to reveal usable patterns. This idea of differencing is quite unique to ARIMA—most other models don’t explicitly include it as a built-in concept.

The good news is that this process is largely automated in practice, so you don’t need to overthink it. We’ll see how straightforward it actually is when we apply it step by step.

Let’s put all of this into practice in the next video.

# **G) Python - Stationarity**

We are now going to look at stationarity, which is a core concept behind the Integrated (I) part of ARIMA. This is something we need to understand and inspect, just like all the other parameters. However, it’s important to be very clear about one thing from the start: we are not going to make final modeling decisions based on a single chart or a single test. Instead, we look at the problem globally.

I want to show you how stationarity works and why it matters. It’s an important topic in time series analysis overall, but in practice, it’s usually not the final deciding factor. In the beginning, we may use it as guidance because it gives us useful information, but later on, what really matters is that we test many combinations of parameters and let the data and metrics decide. That’s ultimately how ARIMA modeling works.

So let’s focus on stationarity. As mentioned earlier, we imported the ADF test, which is our statistical test for stationarity. This test comes from statsmodels, and we apply it directly to our target time series.

We run the ADF test on DataFrame.y, and the output gives us several values. At first glance, it may look overwhelming, but we are going to focus on just one thing: the p-value. The p-value is what allows us to test a hypothesis and make an inference about our data.

To understand the p-value, we need to talk briefly about hypotheses. Every statistical test has:

a null hypothesis (the status quo), and

an alternative hypothesis (what we want to check against the status quo).

For the Augmented Dickey–Fuller test, the null hypothesis is that the time series has a unit root, which means it is not stationary. The alternative hypothesis is that there is no unit root, meaning the series is stationary.

This is where things can feel a bit counterintuitive. We are testing for stationarity, but the null hypothesis actually assumes non-stationarity. So interpretation is crucial:

If the p-value is greater than 0.05, we fail to reject the null hypothesis → the time series is not stationary.

If the p-value is less than or equal to 0.05, we reject the null hypothesis → the time series is stationary.

In our case, the p-value is around 0.1, which is greater than 0.05. That means our original time series is not stationary.

Now let’s look at what happens under the hood when we treat a time series as non-stationary in ARIMA. The model applies differencing. Practically, this means subtracting the previous value from the current value.

So we create a new series:
y_diff = DataFrame.y.diff()

This operation takes today’s value and subtracts yesterday’s value. Since the first observation has no previous value, we drop the resulting NaN. If you manually check the math between two consecutive values, you’ll see that it matches exactly—this is just simple subtraction.

Once we have the differenced series, we run the ADF test again on y_diff. This time, the p-value becomes very small, close to zero. That tells us that the differenced series is stationary.

So what does this mean for us?

It means that our original time series is not stationary—which is completely normal. In fact, most real-world time series are not stationary. They usually have trends, seasonality, or both. If you just look at our data visually, you can already see a trend and seasonal patterns. Both of these violate the assumptions of stationarity.

Stationarity depends on three things:

constant mean

constant variance

constant covariance

If a time series has a trend or seasonality, these conditions are almost always violated. So it is very fair—and very realistic—to assume non-stationarity in most cases.

That’s exactly why ARIMA includes the Integrated component. Differencing allows us to transform complex, real-world data into a form that is more stable and easier to model.

We’ll stop here for this video. There is still a lot more to learn, and things will become even clearer as we move forward and start combining all these pieces together.

# **H) Moving Average**

Now it’s time to cover the final piece of the ARIMA puzzle: the moving average (MA) component. I’ll explain this in a simple, intuitive way, using the same coffee example we used earlier when discussing the autoregressive part.

Imagine you’re trying to predict how much coffee you’ll need tomorrow. One obvious approach is to look at how much coffee you drank yesterday or the day before. That’s exactly what the autoregressive (AR) component does—it learns from past values. But there’s another smart thing we can do, and this is where the moving average comes in.

Instead of only looking at past coffee consumption, what if you also considered the mistakes you made in your earlier predictions? For example, maybe yesterday you predicted you’d need three cups of coffee, but in reality, you only drank one. That difference between what you predicted and what actually happened is an error. The moving average component learns from these “whoopsies.”

In simple terms, the MA part looks at the errors from previous predictions and uses them to improve future forecasts. It’s like saying, “I thought I needed three cups yesterday, but I only had one—let me remember that when guessing today.” This helps the model adjust its predictions more intelligently.

One of the main reasons we need the moving average component is to smooth out random fluctuations in the data. Real-world data often has sudden bumps or noise that don’t represent long-term behavior. The MA helps prevent the model from overreacting to those random changes and making drastic decisions based on very recent events.

At the same time, the moving average is great for quick adjustments. If there’s a sudden drop or unexpected change, the MA component adapts quickly because it directly accounts for recent prediction errors. It doesn’t just rely on past values—it also learns from past mistakes.

So, in a nutshell, the moving average adds a layer of wisdom to the forecasting process. It teaches the model not only to learn from what has happened before, but also from where it went wrong. That balance makes predictions smoother, smarter, and more responsive.

# **I) Python - ARIMA**

Okay, now it’s time to move on to ARIMA. In our functions, you’re going to find SARIMAX, and we’ll be using this to build our ARIMA model. Sorry, SARIMAX, for everything—but the reason we use it is because SARIMAX can also be used to build a plain ARIMA model. And just to be complete, there’s also ARIMAX, which is ARIMA with external regressors but without seasonality. That setup is quite unique and not something you see very often in real-world time series, but it’s good to be aware of it.

As always, the first thing we do is assess our data properly. That means splitting it into training and testing sets. Here, we’ll keep the last 30 days as our test data. Everything before that becomes the training set. Once we do this split, we can quickly preview the test data to make sure everything looks correct. So far, everything looks fine.

At some point in the future, when we include variables like discount rate and coupon rate, we’ll need to deal with the fact that they’re stored as objects—probably percentages. We already saw that when checking the dataset info earlier. But that’s a future problem, and for now, we’ll ignore it.

Next, we create the ARIMA model itself. Since this is a non-seasonal ARIMA, the seasonal order will be set to (0, 0, 0, 0). That means no seasonal autoregressive terms, no seasonal differencing, and no seasonal moving average terms. For now, we keep it simple.

For the main ARIMA order, we use (1, 1, 1) for p, d, and q. The differencing term d is set to 1 because we already saw that the data is not stationary. It’s important to highlight that choosing these values is often a bit of a guessing game, especially in the beginning. Personally, I like to look at PACF values, and here I’m choosing to look back three days for the autoregressive component. That’s why we include three lags when thinking about the AR structure.

Once the model is defined, we immediately fit it using .fit() and then print the model summary. While this is running, a quick reminder that documentation is always important—the SARIMAX documentation link is already included at the top of the script.

The model fits very quickly, in about two seconds. In the summary, you’ll see the endogenous variable, which is our main time series, and the exogenous variables, which would be things like discount rate or coupon rate if we had added them. You’ll also see the order and seasonal order listed. There are many other parameters shown, but from an application perspective, most of them aren’t critical, and honestly, I’ve never really needed to use them.

There’s also the disp parameter in the fit method. If you ever see a lot of convergence warnings and your model isn’t behaving well, you can set disp=False to suppress those messages. It’s just a boolean flag. For us, though, everything looks fine, so we don’t need to worry about it.

Looking at the coefficients, we can see three autoregressive terms, corresponding to the three past days we’re using, and one moving average term. The moving average coefficient is around -0.9. I’ll explain what this means visually in a moment with a simple drawing, even though I’m terrible at drawing. The idea is just to make the intuition clearer.

Before we move to the drawing, let’s generate predictions. We use the model’s forecast method and set the number of steps equal to the length of the test set, which is 30 days. Since the test data length is already a number, we don’t need to do anything fancy here—just generate 30 predictions.

When we look at the predicted values, something interesting happens. The predictions start increasing—141, 149, 151, 52, 54, 55, 56, 57—and then they flatten out. Eventually, they stabilize at a fixed value. This happens because ARIMA is a simple model that only looks at the last three days and the average error from the previous day. Over time, it converges to a steady level and stops changing much.

Now let’s visualize the forecast and assess the model. When we compare predictions with actual values, the results are not great. To make it easier to see, we focus only on the year 2022. Even then, the error is quite large—around 24%, which translates to roughly 7 million units. That’s a significant error, and visually, the fit looks pretty bad.

If we zoom in on the forecast, we clearly see the plateau effect. Because the model only looks at the last three days, repeated forecasting causes it to settle at a constant level. This behavior is exactly why ARIMA is considered an introductory model. It’s great for learning the fundamentals, but it has clear limitations when applied to complex real-world data.

This is why we’re using ARIMA mainly as a stepping stone. It helps us understand how autoregression, differencing, and moving averages work together. In the next step, we’ll draw this out visually to really see how it behaves, and then we’ll build on it with more advanced models.

# **J) ARIMA in Action**

Okay, welcome back. Let’s now look at ARIMA in practice. I went ahead and made a few drawings to help visualize what’s actually going on under the hood. I’ll be the first to admit that I’m really bad at drawing, but I genuinely hope this still helps make the idea clearer.

In the drawing, we start with our time series, and I’ve shown four periods here. The model itself is using three previous periods, which matches the way we configured it earlier. You can see coefficients like 0.35, 0.02, and 0.12—these represent the weights applied to the previous values in the time series. Let me zoom in a bit so it’s easier to see what’s happening.

So here’s the idea. We have values from the time series at t-1, t-2, and t-3. These past values are multiplied by their respective coefficients and combined together. At the same time, we also consider the forecasting errors. I’ve drawn the actual time series in one color and the forecasting errors in another to distinguish them.

To predict the value at time t, we combine all of this information. That includes the values from the previous three periods and the forecasting error from t-1. The ARIMA prediction is essentially a mixture of past observations and past mistakes. On top of that, there is also a constant term that is always applied as part of the ARIMA equation. Overall, it’s a very straightforward structure once you see it visually.

I also added another part to the drawing to show that this process is iterative. If the current point is time t, then naturally the previous points become t-1, t-2, and t-3. This keeps rolling forward as time progresses. That’s why this is often called a rolling approach—we’re always moving forward one step at a time, using the most recent information available.

At the same time, we again bring in the forecasting error from the most recent step. So when we predict a future value, we’re always using the last three observed periods and the most recent error. This repeats continuously as the model generates forecasts further into the future.

Even though the drawing isn’t perfect, the key takeaway is that t is always influenced by three previous periods, because that’s how the model was defined: (3, 1, 1). The 1 for differencing means the model internally differences the data to make it stationary, and then it reverts that differencing when producing the final forecast.

Now, when we move fully into forecasting mode—where the model starts predicting future values based on its own previous forecasts—the amount of information available becomes limited. There are no external variables influencing the predictions, and eventually the model starts predicting based on predictions. Because of that, the forecasts converge toward a certain level and stay there.

That behavior explains why we saw the plateau earlier. The model simply doesn’t have enough new information to keep adjusting, so it stabilizes. This is completely expected for a basic ARIMA setup and is one of the reasons why it’s mainly used as an introductory model.

We’ll keep building on this idea when we move to SARIMA, where seasonality adds more structure and usefulness. For now, this visualization should help solidify how ARIMA actually works behind the scenes.

# **K) SARIMA**

Alright, let’s do this. Now we’re going to talk about SARIMA, which naturally builds on ARIMA. If you already understand how ARIMA works, then SARIMA is actually going to feel quite easy, because all we’re really doing is adding a seasonal component on top of what we already know.

The reason this matters is that most real-world data is seasonal. What we do at 10 a.m. is very different from what we do at 10 p.m. What happens in February is not the same as what happens in August. Behavior changes over time in predictable patterns, and because of that, incorporating seasonality becomes extremely important if we want better forecasting results. SARIMA takes us one level deeper and helps us capture those repeating patterns.

The overall purpose here is simple: ARIMA alone is not enough to represent the real world. Even SARIMA is not perfect, but it’s definitely a step in the right direction. When time series methods were originally developed, computational power was very limited. A lot of calculations were done by hand, or with very basic computers and calculators. Given those constraints, these models were designed to be practical and interpretable, and they still make sense today.

Seasonality is everywhere. You don’t eat ice cream all year round, and that example is often used as the poster child for why SARIMA exists. Certain behaviors repeat at regular intervals, and SARIMA is designed specifically to capture that structure.

In terms of components, we already know the lowercase p, d, q from ARIMA. SARIMA adds uppercase P, D, Q, along with m, which represents the seasonal period. One drawback of SARIMA is that it only allows for one seasonal component. In the real world, we often have multiple seasonalities—within a day, across days of the week, across months, and across years. SARIMA can’t handle all of those at once, but it still captures a major part of the structure.

These components work in a fairly straightforward way. For example, earlier we used (3, 1, 1) for the non-seasonal part—three previous days, one level of differencing, and one moving average term. For the seasonal part, we might look at what happened one week ago. In that case, we could set the seasonal autoregressive term P = 1. If we want to look at the forecasting error from one week ago, we would set the seasonal moving average term Q = 1. We can also apply differencing specifically to the seasonal component using D.

The value m represents the number of periods in a season. For example, if the seasonality is weekly with daily data, then m = 7. As we move on to more advanced models later, we’ll see ways to handle multiple seasonalities more effectively, but SARIMA is a solid foundation.

Under the hood, what SARIMA is really doing is splitting the data into seasonal and non-seasonal components. This should sound familiar from when we talked about seasonal decomposition. Once the data is separated, the AR and MA components are applied to each part appropriately.

One major reason SARIMA works better than ARIMA is that it stabilizes the data. ARIMA often struggles with regular ups and downs caused by seasonality. From ARIMA’s perspective, those fluctuations look confusing, so it tries to adapt without fully understanding what’s going on. By removing or explicitly modeling seasonality, SARIMA gives us much cleaner data, making it easier to understand how much information truly comes from the previous days or weeks.

At the same time, we also care about the errors from previous seasons. Are the errors consistently positive or negative week over week? That’s where the moving average component plays a role again. In the end, SARIMA is about balancing information from past values and past errors, both in seasonal and non-seasonal terms.

If you remember Holt-Winters, we also had trend and seasonality components there. Even just adding seasonality alone can explain a large portion of what’s happening in many time series. That’s why SARIMA often performs much better than plain ARIMA.

On the slide, you’ll see references to data with predictable fluctuations. That idea applies to almost all time series forecasting methods—many principles are shared across models.

And that’s it for the theory. Next, we’ll look at SARIMA in practice, just like we did with ARIMA, and see whether we can actually improve our results.

# **L) Python - SARIMA**

Alright, let’s do SARIMA. Here we go. At this stage, we’re simply going to build the model and start with some initial parameter choices. We’ll stick with the non-seasonal order (3, 1, 1) and then add a seasonal order. Since we’re working with daily data, a natural starting point is 7 days, and we’ll also experiment with 365 days to see how the model behaves and whether there are any noticeable differences.

Before finalizing the seasonal order, let’s take another look at the PACF. We can still see meaningful information one week and even two weeks back. Based on that, we’ll use a seasonal autoregressive order of 2. Once again, it’s important to emphasize that a lot of this is based on intuition and exploratory analysis. Another thing to keep in mind is that the lower the parameter values, the less computationally expensive the model becomes. That’s why we’ll keep the seasonal differencing at zero for now. We already have differencing happening in the non-seasonal part, so this should be sufficient as a starting point.

With that in mind, we go ahead and define the SARIMA model. Everything is organized, and we run the model. Initially, something looks odd in the output, but that turns out to be a simple mistake—we were still referencing the ARIMA model instead of the SARIMA one. Once that’s corrected and the proper SARIMA model is used, everything behaves as expected.

Now we can clearly see the components in the model output. We have the non-seasonal autoregressive terms L1, L2, and L3, along with the moving average term. On top of that, we also see the seasonal autoregressive terms, specifically L7 and L14, which correspond to one week and two weeks back. We also include the seasonal moving average error at L7. When we check statistical significance, everything appears to be significant, which is a good sign—it tells us that the model is capturing meaningful information.

The next step, as always, is to look at predictions. We generate forecasts using the SARIMA model for the same number of days as our test set. Instead of focusing on the raw prediction values, we move straight to model assessment and visualization. To make the plot easier to interpret, we again focus only on the year 2022.

Visually, we can now see some ups and downs, which indicates that the model is capturing at least a small seasonal pattern. However, the improvement isn’t as strong as we might have hoped. In fact, the error metric comes out to around 25.9, which is actually worse than what we had before. Even though the curve looks slightly better toward the end, it’s still off overall.

This highlights an important point: not every model works well on the first try. The time period chosen here is especially difficult—think Black Friday and similar events. Those spikes are hard to model without additional information. Just because the model underperforms in one segment doesn’t mean it’s useless. In fact, this is a perfect example of why we need external variables to help explain what’s going on.

One thing we can always try is changing the seasonal period to 365. We can even experiment with two years if we want. However, we need to be very careful here. The further back we go, the heavier the computational load becomes. From a computational perspective, this quickly becomes tricky.

We try running the model with a seasonal period of 365. At first, there are no immediate errors, which seems promising. But as time goes on, the model becomes extremely slow. This is expected—long seasonal periods combined with higher-order models are computationally expensive. The hope is that once we add external variables, the model will improve, even if it still doesn’t perform perfectly. At the very least, we’d expect to see improvements in our evaluation metrics.

At this point, the model is still running. After letting it run for a long time—long enough to go for lunch, cook, eat, relax, stop it, and try again—it becomes clear that the 365-day seasonal setup simply isn’t working for this dataset and parameter combination. It could be the dataset itself, the model complexity, or simply not a good day for this experiment.

A natural question here is whether using a GPU would help. Unfortunately, it wouldn’t. SARIMA is a CPU-only process, and there’s no way to accelerate it using a GPU at the moment. So the only real option is to interrupt the process and restart.

In the end, we revert back to using a seasonal period of 7, which is the standard choice for daily data and works reliably. That’s what we’ll stick with moving forward.

And that’s it for this video. If you tried this yourself and managed to get the 365-day seasonality working, I’d genuinely love to hear about it—what you changed, what worked, and what made the difference. It’s very possible that for a dataset this large and with these parameters, a yearly seasonal component is simply too demanding from a programming and computational perspective.

The key takeaway is this: the more orders and seasonal components we add, the harder the problem becomes. That’s something you should always keep in mind when working with SARIMA.

# **M) SARIMA in Action**

Okay, welcome back. Let’s recap what we were looking at last time and then build on it step by step.

Previously, we focused on the ARIMA forecast. That forecast was a combination of several elements. First, we had the integrated part, which comes from differencing the data to make it stationary. Then we had the forecasting error from time t–1, multiplied by its coefficient. On top of that, we included the autoregressive component, which uses the values from the last three periods—t–1, t–2, and t–3—each weighted by their own coefficients. All of this together makes up the ARIMA side of the model.

If we try to write this conceptually, that’s essentially what ARIMA is doing. It combines recent past values, recent errors, and the differenced structure of the data to produce a forecast.

Now, when we move from ARIMA to SARIMA, we are simply adding more structure to this same framework. The final forecast is still one single value, but it now becomes a combination of more elements. The ARIMA components remain exactly the same, but we add seasonal information on top of them.

So, in the final SARIMA forecast, we are combining four main pieces. First, we still have the ARIMA part, which includes the forecasting error at t–1. Second, we still use the time series values at t–1, t–2, and t–3. Third, we now add the seasonal autoregressive terms, which in our case come from t–7 and t–14. You can see those clearly reflected in the table, where the seasonal lags are 7 and 14. Finally, we also include the seasonal forecasting error, specifically the error from t–7.

All of these elements are combined together to produce the final forecast. It’s important to highlight that although we talk about the integrated part, differencing itself doesn’t have a coefficient. It’s simply a transformation applied during modeling and then reversed when we generate forecasts.

In addition to all of this, we also have the sigma term, which acts like an intercept or baseline. You can think of it as the starting level of the forecast, and then all the coefficients from the AR, MA, and seasonal components adjust that baseline up or down.

At this stage, we don’t yet know whether this is the best possible combination of parameters. That’s something we’ll explore later in the section through experimentation and tuning. For now, we’re using this setup to clearly understand how the model is structured.

The main takeaway from this drawing is that SARIMA is actually a very simple and logical framework. We look a little bit into the recent past, we look further back into the seasonal past, and then we combine everything together into a single forecast. That’s really all there is to it conceptually.

There’s still a lot more to learn, but this foundation is crucial. I’ll see you in the next video.

# **N) SARIMAX**

In this video, we’re going to go one level up and talk about SARIMAX. So what exactly is it? Quite simply, it’s SARIMA plus exogenous variables. That’s where the “X” comes from. Conceptually, SARIMAX is just SARIMA with the ability to include external factors that influence the time series.

Even though the idea sounds simple, it’s extremely powerful. Imagine you’re trying to predict ice cream sales. With SARIMA, you’re already considering past sales and seasonal patterns—things like weekly or yearly cycles. But what about temperature? A very hot day could suddenly cause a spike in ice cream sales that SARIMA alone would struggle to explain.

This is where SARIMAX really shines. It allows you to include exogenous variables, such as temperature, directly into the model. That’s the real “X factor”—pun intended. These variables help explain changes in the time series that don’t come purely from its own past behavior.

In the ice cream example, exogenous variables could include weather conditions, holidays, promotions, or even nearby events. SARIMAX takes these into account, giving you a more complete and realistic view of what actually drives the data.

From a modeling perspective, we’re not adding brand-new internal components like we did when moving from ARIMA to SARIMA. Instead, we’re augmenting SARIMA with these external inputs. The model analyzes how these exogenous variables have historically influenced the main time series and uses that information to improve future forecasts.

One of the biggest advantages of SARIMAX is higher accuracy, especially when external events clearly impact the data. If those outside factors matter—and often they do—SARIMAX can significantly outperform models that only rely on past values and seasonality.

Another major benefit is greater insight. Not only can we make better predictions, but we can also study how different external factors influence our data. This helps with understanding, decision-making, and even business strategy.

Of course, there are downsides as well. The biggest one is complexity. Adding exogenous variables means we need more data, and that data needs to be reliable. We also need to think carefully about data availability, because SARIMAX requires both historical exogenous data and future values of those same variables in order to make forecasts.

In short, using SARIMAX effectively comes down to careful variable selection. We don’t want to make the model unnecessarily complex. As always, if a simpler approach works well, it’s usually the better choice. The key is to experiment—add external factors thoughtfully and see whether they actually improve accuracy.

# **O) Python - SARIMAX**

Alright, welcome back. We’re done with SARIMA, and now it’s time to move on to SARIMAX. This is the next step, and it’s also the one we’re really going to explore in more depth.

If we take a look at our dataset—starting with a quick dataframe.head()—we can see that we now have two additional variables: discount rate and coupon rate. Both of these are currently stored as objects, which is not ideal for modeling. The reason is simple: these values include percentage symbols, and the model expects numerical input.

So the first thing we do is clean this up. We remove the percentage symbol from both the discount rate and the coupon rate columns and then convert them to floats. I’m being very explicit here so we can avoid confusion and get some shortcuts later. Once this transformation is done, the columns are properly converted, and everything looks good.

Next, we split the regressor data into training and testing sets. Just like before, the training regressors will go into the model, and the test regressors will be used during forecasting. This split has to align perfectly with how we split the target variable, otherwise the model won’t work correctly.

To do this cleanly, we select only the discount rate and coupon rate columns and then split them using the same logic as before. The training regressors go up to minus the test days, and the test regressors run from the last test days to the end. We double-check the dates to make sure everything lines up correctly—training ends on October 31st, and predictions start on November 1st. That confirms the split is correct.

With the regressors ready, we can now build the SARIMAX model. We pass in the training target variable along with the training regressors as exog. We keep the same configuration as before: non-seasonal order (3, 1, 1) and seasonal order (2, 0, 1, 7). The documentation reminds us that the exogenous input must be shaped as the number of observations by the number of regressors, which is exactly what we have.

We fit the model and print the summary. This takes a few seconds to run. Once it’s done, we can immediately see what’s new compared to SARIMA. The key difference is that we now have coefficients for discount rate and coupon rate. That’s exactly what we expect—these are the new external variables influencing the forecast.

With that, the modeling part is essentially complete. The next step is prediction. We generate forecasts using the SARIMAX model, specifying the number of steps equal to the test days and passing in the test regressors as exogenous inputs. Once that’s done, we move straight to model assessment and visualization.

The results show a clear improvement. The error drops to 21.7%, and the MAE is around 630, which is definitely better than what we had before. So adding regressors helped—this is a win.

That said, we’re still missing something important. We do capture weekly seasonality, but we’re missing yearly effects—things like Christmas, Thanksgiving, and Black Friday. These events have a big impact on the data, and they’re not explicitly represented in the model right now. If we were building this from scratch in a real project, we would likely add additional variables to represent those events.

The key takeaway from this section is not feature engineering. We’ll do plenty of that later. The real focus here is on understanding the frameworks. From this point onward, we’re going to spend a lot of time on cross-validation, parameter tuning, and reusing the same modeling patterns again and again.

If we truly understand these frameworks now, everything that comes later—especially more advanced models—will be much easier to grasp.

So yes, we improved the model by adding regressors, which is great. But there’s still room for improvement, and that’s exactly what we’ll continue working on next.

# **P) SARIMAX in Action**

In this final video of the section, we take everything we have built so far and bring it together into one complete picture. At this point, we already have our ARIMA and SARIMA components clearly defined, and now we focus on how they combine with external variables to form the final forecasting model.

First, we start with the ARIMA (non-seasonal) part of the model. This captures short-term patterns in the data. In this case, it includes:

A moving average (MA) component of one period

An autoregressive (AR) component of three periods

This means the model looks at recent errors and values from the last few time steps to explain current behavior.

Next, we include the seasonal ARIMA (SARIMA) part, which handles repeating seasonal patterns. Here, the model:

Uses the error from the previous seasonal period, such as one week ago

Uses seasonal autoregressive terms, for example at time steps t − 7 and t − 14

These components help the model capture weekly seasonality and recurring trends that repeat over time.

Now comes the key extension: exogenous variables. In this example, we add two external factors:

Discount

Coupon

These variables are applied at the same time step t, meaning they directly influence the forecast for that specific point in time. While only two are shown here for simplicity, in practice you can include more external variables if they are relevant and available.

When everything is combined, the final forecast is built from multiple elements:

Non-seasonal AR and MA components

Seasonal AR and MA components

Exogenous variables (discount and coupon)

A baseline intercept term (α)

Although we might initially count six major components, the true number is higher because seasonal and non-seasonal parts each contain multiple lag terms. All these elements are weighted and combined according to the model’s equation to produce the final prediction.

It’s important to note that this example represents a basic SARIMAX setup. We are deliberately not introducing feature engineering yet. Feature engineering would involve transforming variables, adding lagged versions of discounts or coupons, encoding promotions differently, or creating interaction effects. That level of complexity is intentionally avoided in this section.

The main focus here is on understanding the fundamentals. Before adding more features, it’s crucial to master:

Cross-validation for time series

Parameter tuning

Evaluating whether additional complexity actually improves accuracy

As always, simpler models are often better if they perform well. External variables should be added carefully and tested incrementally to confirm that they genuinely improve the forecast.

That wraps up this section. If you have any questions or face any issues, feel free to ask. In the next video, we’ll move on to cross-validation and parameter tuning, which are the most important skills to master at this stage.

# **Q) Cross-Validation for Time Series**

Cross-validation is a fundamental concept in time series forecasting because it adds credibility and robustness to our models. The core idea is to repeatedly test the model under different conditions, such as different periods of the year, to ensure that it performs consistently and reliably.

Instead of relying on a single train–test split, we create multiple training and testing scenarios. In time series forecasting, this is especially important because data is ordered in time, and patterns such as trends and seasonality can change throughout the year. A model might perform well in one seasonal period and poorly in another, which does not necessarily mean the model is bad overall.

To address this, we evaluate the model across different seasonal segments. By doing so, we can test whether the model generalizes well across the full range of seasonal behaviors rather than just a specific time window.

When we zoom in on cross-validation for time series, there are two main approaches.

The first approach is called rolling forecast cross-validation. In this method, after each evaluation, we add the test set to the training set before making the next prediction. Over time, the training data grows larger and larger. This approach reflects real-world forecasting scenarios, where all past data is available and should be used to improve future predictions.

The second approach is known as sliding forecast cross-validation. Here, the size of the training set remains constant. Each time we move forward, we add new test data to the training set but remove the same amount of the oldest data. As a result, the training window slides forward in time while maintaining a fixed length.

Between the two, the general preference is often for the rolling forecast approach. The reasoning is simple: if historical data is valuable enough to be used for evaluation, then it should also be valuable for training the final forecasting model. If certain data is not useful, then it probably should not be included at all, even during experimentation.

Rolling forecasts ensure that all available information contributes to both model assessment and future predictions. Sliding forecasts can be useful in cases where very old data is no longer relevant, but this should be a deliberate and well-justified choice.

To summarize, cross-validation is a simple yet powerful concept for building reliable forecasting models.

Rolling cross-validation continuously expands the training set by incorporating past test data.

Sliding cross-validation keeps the training window fixed by adding new data and discarding the oldest observations.

In the next video, we’ll see how this cross-validation strategy is applied directly to our forecasting model. Have fun, and feel free to ask if you have any questions!

# **R) Python - Cross-Validation**

Alright, let’s move on to cross-validation and see how it actually works in practice.

For this, we are going to use time series cross-validation, specifically the TimeSeriesSplit approach. I already have the module ready, and we’ll include it directly in our workflow. This method is designed specifically for time-dependent data, where maintaining the order of observations is critical.

We’ll start by clearly defining the cross-validation configuration. This keeps the code clean and avoids unnecessary noise later on. The non-seasonal order remains (3, 0, 1) and the seasonal order stays (2, 0, 1, 7). We also define the number of splits as five. This means the model will be trained and tested five separate times across different temporal segments of the data.

Next, we import TimeSeriesSplit from sklearn.model_selection. We initialize it with:

n_splits = 5, meaning five cross-validation runs

test_size = number_of_test_days

The choice of test size is very intentional. Our forecasting use case always focuses on 30-day forecasts, so our test window is consistently set to 30 days. This is a crucial difference from traditional machine learning, where people often use fixed ratios like 80/20. In forecasting, we must evaluate the model in the same horizon we plan to use it in production.

The max_train_size parameter is left as None, which means we are using a rolling forecast strategy. The training data grows with each split instead of being restricted to a fixed window.

Before running the model, we inspect how the time series splits actually look. When we print the train and test indices for each split, we see that only indices are shown. To interpret this correctly, we check the total length of the dataset, which is 1,795 observations. Since indexing starts at zero, the final index is 1,794.

Each split consistently takes the last 30 observations as the test set, then shifts backward in time for earlier splits. For example:

One split tests days 60–30

Another tests days 90–60

Another tests days 120–90
and so on. This confirms that our cross-validation setup is behaving exactly as expected.

Now we move on to actually performing the cross-validation loop.

We begin by initializing an empty list to store cross-validation scores. Inside the loop, for each train-test split:

We retrieve the corresponding indices

We isolate the correct training and testing data

We extract the exogenous regressors (discount rate and coupon rate)

We build the SARIMAX model using the defined orders

We generate forecasts for the test period

We evaluate predictions using:

RMSE (Root Mean Squared Error)

MAE (Mean Absolute Error)

MAPE (Mean Absolute Percentage Error)

Each metric is stored for later aggregation. This loop is essentially the same modeling logic we used before, but now it’s wrapped in a systematic evaluation framework.

At this point, it’s important to acknowledge the warnings that appear during model fitting. These warnings are not errors—they simply indicate that from a statistical standpoint, the model may not be perfectly specified for some splits. This does not automatically mean the model is unusable. From a business perspective, performance may still be acceptable, and the final decision should always be based on the actual error metrics.

Once the loop finishes running, we aggregate all results into a DataFrame. This allows us to inspect how the errors vary across different splits. In our case, the MAPE values range roughly between 6% and 24%, showing noticeable variability depending on the time period being forecasted.

This variability is expected in real-world time series. Ideally, we would like more stability, but performance always depends on the underlying volatility of the data.

Finally, we compute and print the average errors across all splits:

Average RMSE

Average MAE

Average MAPE (around 18%)

There is clearly room for improvement. However, that is not the purpose of this stage.

The key takeaway here is that we now understand:

How the model behaves across multiple time periods

What level of error we can expect for this specific configuration

That cross-validation provides a much more reliable assessment than a single train-test split

At this point, we know exactly where we stand with this set of variables and parameter choices. The natural next step is parameter tuning, where we systematically search for better configurations.

# **S) Parameter Tuning**

Parameter tuning is what takes a forecasting model from good to great.

It’s true that the programming involved can feel a bit challenging at first, but the good news is that once you understand the structure, you’ll be working with a reusable template. This makes the process much easier to replicate across models and projects.

Let’s start with the most important question: why do we need parameter tuning?

Modern analytics gives us an incredible amount of flexibility. We can customize models, adjust assumptions, and fine-tune how they behave. However, this flexibility also means that there is no single “default” configuration that works best for every problem. To get the highest possible accuracy, we must actively search for the optimal set of parameters for each model and each dataset.

From a process perspective, parameter tuning follows a very logical sequence.
First, we define a range of possible parameter values.
Next, we run the model using each of these combinations.
Then, we measure the model’s accuracy and store the resulting error.

At its core, this is nothing fundamentally new. It’s exactly what we’ve already been doing—but now we do it systematically, repeatedly, and in an automated way.

To make this concrete, imagine we are tuning the autoregressive component of a model. We might test lags of 1, 2, and 3. For each lag value:

We fit the model

We compute the error

We store the result

Once all combinations are evaluated, we simply select the parameter value that produced the lowest error. For example, if lag 1 performs best, then lag 1 becomes our optimal autoregressive parameter.

From an intuition standpoint, this process is very straightforward.
We explore different combinations, compare their performance, and keep the one that works best for our specific forecasting problem.

To summarize, parameter tuning is about finding the optimal configuration of a model that maximizes forecasting accuracy. It transforms model development from guesswork into a disciplined, data-driven process.

In the next step, we’ll implement this entire concept in Python, using the structure we’ve already built.

# **T) Python - Setting the Parameters**

Okay, parameter tuning. This part is going to be split into three videos. In the first video, we are going to focus purely on the configurations. The goal here is not to run anything yet, but to get everything properly set up so that parameter tuning can run smoothly later.

In video two, we will actually run the parameter tuning. This step takes a few minutes to execute, which is why we are not going to try a large number of configurations. You will already have the full structure in place, so scaling it up later will be easy. For now, we are going to work with eight configurations. You could easily run 512 configurations if you want, but eight is more than enough for us to clearly understand the flow.

Then, in video three, we will look at the results and interpret them. That is the overall purpose of this section and how it is structured.

Now let’s get started.

First, let me quickly check the imports. Everything looks good. One important thing we need here is the parameter grid. This parameter grid is what is going to automatically create all the combinations for us. I’m going to run this import, and once that’s done, we can kick things off.

Next, we define the parameter options. The parameter grid is honestly one of those pieces of code that you’ll end up using forever. We use it so often that it becomes second nature.

We start by defining the non-seasonal parameters. For p, I’m going to use values 1 and 3. Three is the value we’ve been using so far. For d, we’ll stick with 1. For q, we’ll test 0 and 1.

Then we move on to the seasonal parameters. For uppercase P, we’ll use 1 and 2. For uppercase D, we’ll use 0. And finally, for uppercase Q, we’ll use 1, just one value.

Once all of these are defined, we bring everything together by applying the parameter grid. This creates all the possible combinations based on the values we specified. At this point, you can convert the grid to a list and inspect all the combinations if you want. Personally, what I usually do is just check the length of the grid.

When we do that here, we see that the total number of combinations is eight. So we’re going to run eight different configurations. I always like to stay on the lower side at the beginning because it’s much faster. Cross-validation itself can take a couple of minutes per run. So eight configurations means we can expect something like 10 to 12 minutes overall, give or take.

Now, regarding the setup, we actually don’t need to redefine everything because we’re going to reuse the same setup we already have. The number of splits is already defined, and the test size is also defined. That said, one thing I like to do is to explicitly include the time series split here again. Yes, this is technically a repetition, but I personally prefer having everything visible in one place.

In fact, I like to include as much as possible here. That way, if you want to change something later—like increasing the number of splits—you can do it easily without hunting through the notebook.

It’s also important to understand that cross-validation is realistically an in-between step. In practice, you could skip explicit modeling and cross-validation and go straight to parameter tuning, because parameter tuning is really the last step before forecasting the future. However, since we’re doing this step by step for learning purposes, this is how I prefer to structure it.

Finally, just to have something ready for the next step, we initialize an RMSE list. We’re going to track only one KPI, and that’s more than enough to make a decision. There’s no need to overcomplicate things at this stage.

And that’s it for now. Everything is ready.

In the next video, we’re going to actually perform the parameter tuning.

# **U) Python - Parameter Tuning**

The way we kick this off is by iterating over the parameters. That’s always step one. So we start with a loop over the parameter grid. For each set of parameters in the grid, we are going to evaluate how well the model performs.

So essentially, we loop through the grid, and for each parameter combination, we initialize a few things. We create an empty list for the fold errors, and we also prepare an empty list that will later store the average error for that parameter configuration.

Once that’s set up, we move into the next loop, which is the cross-validation loop. For each train index and test index coming from the time series split, we isolate the training data and the test data. This gives us the specific slice of the time series that we are working with for that fold.

After that, we isolate the exogenous regressors. We create the training regressors and the testing regressors, making sure they align perfectly with the train and test indices. This step is crucial because the model needs the correct regressors for both fitting and forecasting.

Now comes the modeling part. We build the SARIMAX model using the current parameter combination. From the parameter grid, we extract p, d, and q, along with the seasonal parameters, where the seasonal period is set to seven. We then fit the model.

At this stage, we are not interested in the summary output. We don’t inspect coefficients or diagnostics here. That exploratory phase is already done. In parameter tuning, the focus is purely on output, output, output—we care only about the error metrics.

Once the model is fitted, we generate predictions for the test period. With those predictions, we calculate the RMSE by comparing the predicted values with the actual test values. We triple-check that the test target and predictions are correctly aligned.

That RMSE value is then appended to the list of fold errors. This process repeats for every fold in the time series split. Each fold contributes one RMSE value to the list.

After all splits are completed for that specific parameter combination, we calculate the mean RMSE across all folds. This gives us a single performance score for that parameter set. We then append this average RMSE to our results list.

To summarize the flow clearly:
We start with an empty list → loop through each parameter combination → create a fresh list for fold-level errors → loop through each train-test split → isolate data and regressors → build the model → make predictions → compute RMSE → store it → repeat until all folds are done → compute the mean RMSE → store the result.

This structure is consistent regardless of the model you use. Whether it’s ARIMA, SARIMA, or SARIMAX, the flow remains the same.

At this point, everything looks good. The logic is sound, the structure is clean, and the code is running. If this finishes within about ten seconds, then everything is good to go. Otherwise, just give it a bit more time.

Either way, once this is done, we’ll move on to the next video, where we connect this logic to the final results and analyze them.

# **V) Python - Parameter Tuning Results**

Alrighty, so it actually took just about five minutes to run, which is quite good. It was way faster than I expected, so that’s definitely positive news. Let’s now check the output and see what we got.

The way this works is that we aggregate the RMSE values together with the parameter grid and then transform everything into a DataFrame. So first, we transform the grid into a DataFrame, which makes things much easier to inspect and work with. After that, we add the RMSE results as a column.

Instead of just looking at the head of the DataFrame, it’s much nicer to view the full table. One of the great things about working in Colab is that you can turn this into an interactive table. You can click around and immediately see which configuration performs best.

Looking at the results, the best configuration turns out to be with parameters corresponding to something like 1-1-0-2-1-0. That’s actually not very far off from what we were using before. Previously, we were using 3-1-1-2-1-0, so this is a reassuring result. It shows that parameter tuning is refining the model rather than completely changing it.

What’s even better is that we clearly see an improvement. The RMSE went from around 4.5 million down to about 4.3 million. And this improvement came from just eight iterations, which is quite encouraging. It shows how effective parameter tuning can be, even with a small search space.

Next, we extract the best parameters. There are a couple of ways to do this. One option is to convert values and items into a dictionary, which works fine. However, a cleaner approach is to keep everything as a DataFrame and use .loc to filter the row with the minimum RMSE. This keeps the structure consistent and makes the result easier to reuse.

Once we have the best parameters, we can also store them externally if we want. For example, we can save them to a CSV file like best_sarimax_params.csv. This is useful if you want to reuse the results later without rerunning the tuning process.

At this point, we are in a very good position to move forward. The next step is to start thinking about predicting the future using this optimized model. Now that parameter tuning is done, the process becomes fairly straightforward.

From here on, the main focus will be on understanding and preparing the future regressors. You can already see them in the dataset. Although there are some suggested ideas—like using lagged versions of regressors—we are not going to apply those just yet, because they were not part of the modeling step so far.

That said, lagging a feature by one time step is a very powerful form of feature engineering. For example, instead of only using today’s discount rate, you could also include yesterday’s discount rate as an additional input. This is almost like applying an autoregressive concept to the exogenous variables themselves.

We’ll focus on these ideas in the next step. The next video is really about setting everything up properly so that we can use our tuned model to predict the future.

# **W) Q&A Highlight: Handling Future Data in Forecasting**

I created this video because this is probably the number one question I get: how do I deal with future data when my model needs values that I may not know yet? The reality is that many forecasting models require information about the future in order to work properly. This could include upcoming events, planned promotions, or even weather estimates. The goal of this video is to walk through each of these aspects, explain the overall process, and share my experience as a real-world forecaster from when I worked at an e-commerce company—what we actually did, what worked, and what questions we had to ask.

The first and most important question is: why do we need future data at all? The answer is simple. Models like SARIMAX, Prophet, and LSTM require exogenous variables for future dates if those same variables were used to explain the past. In other words, if you use a variable to explain historical behavior, you must also provide that variable for the future in order to make predictions. These inputs could be marketing campaigns, holidays, major events like Easter, Black Friday, or Thanksgiving, and similar calendar-driven effects.

You also have planned promotions, which are slightly different. These are not necessarily external factors, but internally driven events such as birthday sales, flash sales, or major discount campaigns. On top of that, there are clearly external variables like weather forecasts or economic indicators. The key point here is that without these future values, the model cannot produce a meaningful forecast. It would be like asking the model to predict demand without telling it the conditions that actually shape customer behavior.

So the next natural question is: how do we actually get this future data? In practice, the answer is collaboration. In real-world forecasting, you work closely with other teams and rely on their plans and forecasts. Finance teams are a great starting point because they almost always have budgets, expense plans, and revenue targets. In larger companies—especially publicly traded ones—these forecasts exist for the rest of the quarter, the semester, and often the entire year. As a forecaster, you depend heavily on this information.

Marketing teams are equally important. They know when campaigns will run, when promotions will happen, how long they will last, and what discount levels will be applied. These things do not happen randomly; they are carefully planned both operationally and financially. Marketing and finance teams are usually tightly connected, so this information is often already aligned.

Sales teams also play a key role. They understand targets, growth plans, and what actions are expected to drive sales. Other teams may also be relevant depending on the business, but the bottom line is clear: collaborate. These teams already have the data you need. For example, if marketing plans a major holiday sale, they will know the exact dates, duration, expected discount rates, advertising intensity, and more. Go to them—this information already exists.

From an operational standpoint, creating the future dataset can feel challenging, but it follows a clear process. First, identify the regressors you need, such as temperature, discount rate, or holiday flags. Next, define the future time range you want to forecast. Are you predicting seven days, 30 days, 60 days, or even 365 days? Based on this, you create a future date range.

Then you fill in values for each regressor. For fixed regressors—like holidays or campaign dates—this is straightforward. These are often binary values that indicate whether an event happens or not. Dynamic regressors are trickier. Weather forecasts, for example, require external APIs and come with uncertainty. Economic indicators behave similarly. After filling everything in, you must carefully check for consistency, because it is very easy to make mistakes when manually preparing data.

Your target time series itself, of course, does not have future values. That’s exactly what you are trying to predict.

Another important aspect is forecasting horizons, which vary widely depending on the use case. Daily or weekly forecasts are common when fast reactions are needed, such as for demand planning or short-term promotions. Monthly forecasts are often used for financial planning and budgeting. Quarterly forecasts are critical for scheduling, reporting, and even stock market expectations—companies constantly report performance quarter by quarter. Yearly forecasts support long-term strategic planning, such as capacity planning, hiring, warehouse expansion, and investment decisions.

Regardless of the horizon, the approach to preparing future data stays the same. You extend the dataset to cover the full forecast period. If you want to forecast one full year at a daily level, your future regressors dataset must contain one row per day for that entire year. The same logic applies for weekly or monthly granularity.

At this point, a very reasonable question arises: what if these regressors are important for explaining the past, but I simply don’t have them for the future? This does happen, and it can feel tricky—but there are solutions.

One solution is N-BEATS. This model can work using only the historical target series, and optionally past regressors if they are available. In fact, N-BEATS was originally designed to work without any regressors at all. It is a very powerful model, and we cover it in the course. If this setup fits your situation, I strongly encourage you to explore it.

Another option is the Temporal Fusion Transformer (TFT). You can think of it as N-BEATS on steroids. We also cover this model in the course. TFT can handle static variables, past inputs, and future inputs—but crucially, it can also work when future regressors are missing. The main difference between the two is that with N-BEATS, you cannot include future regressors at all, whereas with TFT you can choose whether to include them or not. Both scenarios are supported.

If you don’t have future regressors, I strongly recommend trying these models and going through their respective sections. The deep learning foundations are mainly covered in the LSTM section, and the theoretical explanations for N-BEATS and TFT are included where they are introduced.

To conclude, forecasting often depends on future values of exogenous variables. When you can plan those future regressors with help from other teams, models like SARIMAX or Prophet work very well. When you cannot, models like N-BEATS and TFT really shine. The guiding principle is simple: if a variable is useful to explain the past, it is probably relevant for the future—but you need a plan for how to obtain or handle it.

Know your forecasting horizon, identify the key regressors, collaborate with the right stakeholders, and build a future dataset that mirrors your training setup. Follow these steps, and you’ll be well on your way to mastering time series forecasting.

# **X) Python - Predicting The Future Set Up**

Alright, we’ve done quite a few transformations and written a fair amount of code so far. Let’s get started right away with the next logical step.

The first thing we’re going to do is isolate X and Y. This is always the foundation. Our Y is very straightforward—it’s simply the target variable from our dataframe. Our X consists of the discount rate and the coupon rate, which we’ve already prepared earlier in the process.

Next, we need to fetch the best parameters. This is the next stage in the workflow. In principle, we could recompute them here, but since we already have them available in the same session, we’ll keep things simple and reuse them. The parameter p will be set equal to the previously identified best parameters.

One important thing to keep in mind here is that if we inspect best_params_p, we’ll notice that it’s a float stored as a NumPy type. What we want to do is transform this value into an integer—so, for example, turning 1.0 into 1. As a best practice, we explicitly apply this conversion. In the past, I’ve run into issues when this step wasn’t made explicit, so it’s always better to be very clear about what’s happening. This way, we know exactly what value we’re passing forward.

Once this is done, we move on to loading the future regressors. We load them into a dataframe—let’s call it future_reg—and take a quick look at the first few rows. You’ll notice that revenue is not present, which makes sense because this is the variable we’re trying to predict. Instead, we only have future values for the discount rate and coupon rate, and they’re currently in a slightly different format, which we’ll need to handle.

You may also notice that lag variables are present. For this particular session, we’re not going to use them, even though they’re already prepared. They were created for a different section of the workflow, but we’ll simply ignore them here.

Now, if we look closely at the date column, we can see that something isn’t quite right. The format isn’t ideal. To fix this, we explicitly specify dayfirst=True and enable date parsing. Once we do that, the dates are properly interpreted and everything looks correct. At this point, we’re good to proceed.

The next step is to isolate the future discount rate and future coupon rate. These values are currently divided by 100, which comes from how the data was sourced. This kind of issue is actually very common in real-world projects, since different data sources often use different units or conventions. To standardize things, we multiply both variables by 100 so that they match the scale used in the historical data.

We assign this transformed data to a new variable—X_future. After applying the multiplication, we verify the result and confirm that the units now match the format used earlier in the modeling process. This ensures consistency between training data and future inputs, which is absolutely critical for reliable forecasting.

At this point, we’ve completed everything we need for this video. In the next one, we’ll build the model, generate predictions, and then visualize the results.

# **Y) Python - Predicting The Future**

If you take a moment to reflect on everything we’ve done so far, it’s actually quite impressive. We started from the basics and went all the way through SARIMA, ARIMA, SARIMAX, cross-validation, and step-by-step parameter tuning. It was a lot of work, but now we’ve reached the point where everything finally comes together.

Now it’s time to build our fully tuned SARIMAX model.

We start by defining the model using our prepared inputs: the target variable Y, the exogenous variables X, and the tuned parameters p and q. Once the model is specified, we fit it and then print the model summary.

At this point—fingers crossed—it should work, and realistically, there’s no reason why it shouldn’t.

Once the model is fitted, we look at the summary output. The results look really solid. You’ll see the coefficients listed, but it’s important to understand that the raw coefficient values themselves don’t mean much in isolation. Interpretation is always relative.

For example, both the discount rate and the coupon rate are expressed as percentages. When we compare their coefficients, we can see that the coupon rate has roughly twice the magnitude of the discount rate. This suggests that coupons are more effective at driving the target outcome than discounts. That’s exactly how you would interpret these coefficients in practice.

Another good example is the autoregressive terms. If you look at the coefficients associated with different lags—say lag 7 versus lag 14—you would typically expect the more recent lag to have a higher coefficient. Higher coefficients generally imply higher relevance, and that intuition aligns well with how time series data behaves.

Next, we move on to making future predictions.

We generate forecasts for the next 30 steps and pass in the future exogenous variables using X_future. At this point, we double-check the dimensions and notice something interesting: the future dataset actually contains 31 rows, while we’re predicting 30 steps.

This is a good observation. In principle, we could have aligned this perfectly by setting the test window to 31 instead of 30. In our case, we’ve been working month over month, so this small mismatch isn’t a problem, but it’s a useful reminder.

A more robust and dynamic approach is to always tie the number of prediction steps directly to the length of the test set or the future exogenous data. That way, these mismatches never happen.

Finally, we visualize the results.

We use the built-in plotting functionality to display the historical data alongside the future predictions. We include the year 2022 in the plot so we can clearly see where the forecast begins and how the model projects forward.

Once the plot appears, everything looks exactly as expected. The future predictions are smoothly aligned with the historical data, and the forecast behaves in a realistic and interpretable way.

And with that, we’re done.

This is the full end-to-end process: from raw data, to transformations, to parameter tuning, to building a tuned model, to generating and visualizing future forecasts. This is how you build a truly strong forecasting solution.

I hope you enjoyed this journey as much as I did. As always, if you have any questions, let me know—I’m here to help. Looking forward to seeing you in the next video.

# **Z) SARIMAX Pros and Cons**

First and foremost, congratulations on completing this section.

I know this was a long one, and we went through quite a lot of steps when it comes to the actual modeling implementation. That said, from my perspective, this is actually a big advantage. Thanks to the pmdarima library, we were able to move relatively quickly despite the complexity. Having ready-made functions for tasks like cross-validation and parameter tuning makes these models far more accessible, especially for beginners.

Now that we’ve built a general structure for Holt-Winters and SARIMAX, we also have a reusable mental framework. You won’t always be able to apply it exactly as we did here, but the underlying logic is solid. And once you understand that logic, adapting it to new datasets becomes much easier.

Even though SARIMAX is considered an older methodology, it can still deliver very strong results—as you’ve clearly seen. That said, it does come with some limitations. One important drawback is that SARIMAX does not always perform well on very long time series. To be fair, this is true for many forecasting models, so it’s not a deal-breaker.

Another key point is that when a forecasting model relies heavily on autoregressive terms, it usually performs best in the short term. Think of forecasts for the next few days or weeks. As you extend the horizon further into the future, the predictions tend to become less stable and less reliable.

When it comes to regressors, SARIMAX uses a simple linear regression framework. This means that if your data suffers from multicollinearity or contains strong non-linear relationships, the model may struggle to capture those effects properly.

Finally, SARIMAX does not support multiple seasonalities. You saw that while we could model weekly seasonality, it would have been ideal to also include yearly seasonality. This limitation is shared with Holt-Winters as well. However, as we move forward into more modern time series techniques, this will no longer be an issue.

All things considered, SARIMAX is still a great model. It’s easy to apply, relatively intuitive, and remains one of those must-know forecasting tools that every data scientist and analyst should have in their toolkit.

# **IX) Section 9: PART 2: MODERN TIME SERIES FORECASTING**

# **A) Modern Time Series Forecasting Overview**

Welcome to the modern era of time series forecasting.

Ever feel like you're stuck in the slow lane of the data? Not anymore. This is where you shift into overdrive and start predicting the future like a pro, ready to make your data work harder than Jeff Bezos in his garage days. Let's get started.

Ever wonder how big players like Meta and LinkedIn stay ahead of the game? They use cutting-edge tools like Prophet and Silver Kite. And guess what? So will you. Let's turn that plain vanilla approach into something as irresistible as double choc fudge with extra sprinkles.

We'll kick things off with Facebook Prophet. Now, this bad boy is a beast at handling real-world data. You'll go from zero to hero. Setting up the Python environment and building the Prophet models makes managing seasonality and holidays look easy.

And to keep things spicy, we'll dive into a case study on bike sharing. Now let's get technical. Holidays can mess your data, but don't worry. You've got dynamic holidays covered. You learn how to incorporate this into your models and keep your forecasts sharp.

Plus, you'll get the lowdown on Prophet model parameters, learning how to tweak them for peak performance. But this is just the beginning. Next, we unleash LinkedIn's Silver Kite. This tool is like Prophet on steroids. You'll see how it stacks up and when to use it.

We'll set up Silver Kite, load and prep data, and dive into its unique features. Ever wonder how to handle different types of seasonality or manage change points and lag regressors? Silver Kite's got you covered.

Seasonality is a big deal in time series forecasting. You'll master handling different types of seasonality with Silver Kite and explore how it manages change points and lagged regressors. These features will make your models rock solid.

We'll also cover fitting data using ridge regression and gradient boosting. Learn to boost your model's performance with feature sampling and custom elements. Cross-validation is key to accuracy, and you'll practice this with both Prophet and Silver Kite.

Kite parameter tuning can make or break your model. I'll guide you through setting it up and executing parameter tuning in Python, ensuring your models are spot on. You'll see the tuning results and visualize your forecasts, making it easier to impress your boss and crush your goals.

By the end of this part, you'll be crushing time series analysis like a young, balding Jeff Bezos. See you in the next video.

# **X) Section 10: (Facebook) Prophet**

# **A) Game Plan for Facebook Prophet**

Alrighty, let's get started into the heart of forecasting with Prophet and Python. And we are going to do all of this with a very cool case study which is on bike sharing. Our adventure will begin by taking a close look at this data set, and we'll build and build and build. Imagine being able to predict the bike rental demand in any weather, season, or city event. And this is where we're headed.

And you know what is the best part? Yeah, I have not told you you'll learn all of this by doing so. You'll get hands-on experience that really sticks. So then, what I'll also do is guide you through setting up the workspace. We'll sort out all the directories and libraries, and then we'll put our data scientist hat on. We'll prep the data, clean it, and start asking questions that only EDA, or exploratory data analysis, can actually answer.

With our bike rentals data, we will explore what factors affect bike rentals more. So what is it? Weather? Is it holidays? We will find that out. And speaking of holidays, you'll learn how to factor those in when the city is either bustling with activities or really quiet as a mouse. This step is really crucial because it shows you how real-world events twist and turn data trends.

Next, we'll dive into the heart of the section and also forecasting, which is the Prophet model. We'll customize models, understand what makes them tick, and really tweak them to perfection. You'll learn not just how to use the model, but to bend it to your own will, making forecasts that are really as accurate as they are insightful.

But one thing is that data is not always straightforward. So we need to tackle anomalies, rework our approach for special days, and refine our forecasts. This is where your analytical skills are sharpened. You'll learn how to spot and smooth out bumps in the data.

Because we are not just dabbling but mastering, you'll dive into feature engineering and parameter tuning. These are your tools for turning a good model into a great one. We'll push our forecast from kind of right to spot-on.

Finally, with our model tuned and ready, you'll step into the future. We'll predict trends, demands, and really the pulse of our bike-sharing rentals with confidence. And on top of predicting, we'll also visualize it. We'll share our insights in a way that people can understand, in ways that turn heads and win nods.

Throughout this journey, you'll not just learn—you'll do. And that's the most important thing: practice, practice, practice. Each step is a step you can take to go from a normal, kind of banal time series forecasting model to a really great one. And that's the goal, really.

So we have learned quite a bit in this course, and it's time to go pro. Until the next video, have fun.

# **B) Structural Time Series and Prophet**

Do you remember the specifics of time series? In very simple terms, it is data that occurs in consecutive periods—for instance, days, weeks, or months—to understand, study, and predict time series. There are loads of concepts and some algorithms that were developed. The most common framework that is currently applied, and also my favorite, is the structural time series.

Now, what does it mean? Let's visualize. Imagine that this is our time series. A structural time series is a decomposition into, at least—and “at least” is a keyword—a trend, which is the growth of the data; seasonality, which are the cycles; the exogenous regressors, which are extra things that affect our time series; and then everything else that cannot be explained, which is the error term.

If we look at the trend, it's usually a line and represents the general direction of the data. It doesn't always have to go up, but it’s not something that fluctuates every day. We also have the seasonality, which are the cyclical patterns in our data. For instance, the consumption of ice cream is cyclical since it peaks in specific periods—specifically, the warmer months of the year. When it's colder, people don’t eat ice cream as much.

Then, we have exogenous events. These are external to our time series but can affect it. Think about the weather, economic sentiment, or even sales events, which can really impact the time series. The secret to a good forecast is really determining this exogenous impact—what they are and figuring out what makes them go up and down with the time series. That being said, even with the best regressors, one thing we need to accept is that the error is part of the prediction. It’s impossible to have 100% accuracy. If you have an error of 5%, you’re doing amazing already. Trying to achieve 100% is not only impossible but can result in overfitting, meaning your model won’t perform well in the real world.

To recap, a time series is the decomposition into trend, seasonality, exogenous impacts, and the error term, which is basically what cannot be explained by the first three. This decomposition can be further split, allowing different types of seasonalities. Mathematically, we can represent it as an equation: the time series 
𝑦
𝑡
y
t
	​

 at time 
𝑡
t equals the trend at time 
𝑡
t plus the seasonality at time 
𝑡
t, plus the exogenous regressors, and finally, the error term.

After understanding structural time series, Prophet becomes very easy. But first, let’s start with some curiosities. The first one, which you may have guessed, is that Prophet was developed by Facebook, now called Meta, in 2016. The mathematics behind it are based on Stan, which is used in causal inference. I won’t go into the details of Stan—it’s complex and beyond the scope of this course—but if you’re curious, there are plenty of YouTube resources available. And, as a fun aside, there’s a famous Eminem song called Stan.

Another revolutionary feature at the time was dynamic holidays. They’re very cool, and I have a dedicated video explaining how to model events in a way that’s not complex but easy to apply. Prophet also has intuitive parameters that are easy to grasp and a built-in cross-validation system, making parameter tuning simple even for beginners in Python.

To sum it up, Prophet brings a lot to the table. Whenever I start a new time series forecasting project, I use a pre-built Prophet script. I make a few tweaks, understand my time series data, get a baseline accuracy, and then refine it further. I’m really excited to show this to you.

Mechanically, Prophet shares many similarities with structural time series. The equation is a bit longer: 
𝑦
𝑡
y
t
	​

 is the time series at time 
𝑡
t, with trend, seasonality, holidays or event impact, exogenous regressors, and the error term. Holidays and events are especially crucial because they occur at irregular intervals—Thanksgiving doesn’t happen on the same day every year, and Easter even varies in month. Prophet allows us to model these efficiently, making forecasting much easier.

We still have seasonality, trend, and regressors, as we know from previous sections. But Prophet lets us go further with practical modeling, programming, and hands-on tutorials. There’s a lot to unpack, but step by step, we’ll practice and get started. Until the next video, have fun.

# **C) CASE STUDY BRIEFING: Bike Sharing**

Before we get started, I want to have more of a formal lecture on the case study because it’s going to be a long one, and hopefully, you find it very cool. But just to make sure that you have a complete understanding, let me give this brief introduction with the help of some slides.

The dataset is all about bike sharing and bike demand. This is where we are going to focus, try to understand it, analyze it, and ultimately predict demand. The dataset was built by Professor Hadi Forney from the University of Porto, Portugal. It takes us to the streets of Washington DC in the USA and is a detailed version of the bike-sharing data collected during 2011 and 2012.

I find this dataset extremely juicy—not just because it gives us all this rental information, but because it really makes us understand what drives rental demand. We are going to focus on factors like temperature, whether it was raining or sunny, and even the day of the week. All of these factors have a strong effect on demand.

Why am I really geeking out over this dataset? It’s packed with information—everything you could imagine. This is a proper dataset. If I were working to predict demand, this would be the kind of dataset I would try to build. Its level of detail makes it perfect for forecasting with Prophet, where we’ll see how all of these variables actually affect bike rentals.

So, what is our plan for this data? We will explore forecasting demand and understanding how city events impact rentals. We’ll even do some research online if we have questions. This helps us get a “big picture” view—not just Python programming, but really understanding, exploring, and putting on our detective hat to mimic a real-life project that any company would face.

In a nutshell, we’re going one step further with this dataset. We have the data and the documentation—please have a look at it. It’s very important to know what is happening because anyone can just create a Prophet model. But moving from step A—understanding the problem—to each step of analysis, while constantly assessing, evaluating, and questioning, is what takes you from being a mediocre data analyst or data scientist to a great one.

And that’s exactly what I want to do with you. Until the next video, have fun.

# **D) Python - Directory and Libraries**

Welcome to this practice tutorial. Please navigate to the folder: first go to Modern Time Series Forecasting, then to Prophet, and click on New → More → Google Colaboratory. For this video, we are going to focus on loading the data and preparing the script.

Let me also get our main script where we have all the core components, just to speed things up. This useful code template will be here for us to use and modify to ensure efficiency. While it opens, let me give a brief overview of this section. It’s going to be long, and the goal is to go from start to finish, trying things, sometimes failing, in a way that closely mirrors a real-world project.

First, let’s get our libraries and the initial part of the code. We will organize what we need and what we don’t. Let’s give our script a name—Prophet—and this will be your template. At this stage, we don’t need everything yet. First, we’ll mount the drive and change the working directory. Follow the usual Google Drive connection steps and continue.

As a quick overview, Prophet differs from ARIMA and exponential smoothing because its library has a lot of built-in functionality. Not everything from our template will be needed or work directly. Let’s get the path: drive → My Drive → Python Time Series Forecasting → Modern Prophet. Copy the path and we’re ready.

The libraries we need are mostly numpy, pandas, matplotlib. We don’t need sklearn because Prophet already has built-in metrics like RMSE and MAPE. We’ll also use ParameterGrid for parameter tuning because I find this approach more intuitive from a programming perspective, even though there are different ways to tune parameters in Prophet.

For the data, navigate to the Prophet folder. Here we have daily bike sharing training. Once opened, you’ll see columns like date, season, year, month, holiday, weekday, and working day (a 0-1 flag indicating whether people were working). Other important columns include weather situation, temperature, A temperature (the normalized “feels like” temperature), humidity, windspeed, and demand metrics: casual, registered, and count. We will focus on count, which is the total demand.

The dataset also comes with detailed documentation. Zoom in on the dataset characteristics. For example, weather situation is categorical even though it’s coded as 1–4 (1 = clear, 2 = mist, 3 = light snow, 4 = heavy rain). Temp is normalized temperature, while A temp is normalized “feels like” temperature. Reading and understanding these descriptions is crucial to truly “become one with the data.” Otherwise, we are just coding, not solving a problem.

Now, let’s load the CSV: daily bike sharing training CSV. For Prophet, we don’t use the index column—it will function as a normal column. Once loaded, Colab might show recommended plots, but we won’t use them because our date column is a regular column and these plots aren’t suitable for time series.

Next, check the data info. Are there null values? No, we have 701 entries, all consistent. All columns are integers except for date, which is fine. At this stage, we need to figure out which columns are essential, which ones to transform, and which can be ignored—this is a core part of data preparation. We don’t need to set frequency because we haven’t set date as the index yet; frequency will be handled later.

I’ll stop here for this video. The focus was on loading our new script and getting the data. In the next video, we will dive into preparing the variables for modeling. Until then, have fun.

# **E) Python - Preparing Data**

Welcome back! In this video, we’ll make several data changes to prepare our dataset for modeling. We’ll start by renaming variables, then adjust the date format, and finally process the weather situation variable. Even though the date is not part of the index, it still needs to follow the standard format: year-month-day.

Next, let’s focus on the weather situation variable. Although it’s numeric (1, 2, 3, 4), it is actually categorical and ordinal. Ordinal variables are categorical variables with a specific order. In this case: clear → mist → light snow → heavy rain. Using 1, 2, 3, 4 directly as numbers doesn’t make sense, so we need to rework this variable into a categorical style.

First, let’s rename our target variable for Prophet. Instead of revenue, we’ll work with count, which we’ll assign as y. For the date column (ds), we’ll convert it using pandas.to_datetime() so that it’s in the correct format (yyyy-mm-dd) even though it’s not the index. The original format is month/day/year with single digits for month and day, so we specify the format accordingly in our conversion.

Next, we prepare the weather situation variable using pandas.get_dummies(). We apply it to the weather situation column and set drop_first=True to avoid redundancy. After this, the dummy variables generated are two and three (note that situation 4 did not occur in the dataset period). We then concatenate these new dummy columns back to our main dataframe on the right side.

Once the dummy variables are added, we rename them to make them more meaningful. Number 2 becomes weather_situation_2 and number 3 becomes weather_situation_3. This makes the dataset easier to work with in Python, avoiding numeric variable names.

Finally, we drop unnecessary columns to simplify the dataset. Columns like instant (observation number), season, year, month, and weekday are removed because Prophet handles seasonality automatically. We also remove casual and registered since our focus is on count. We keep holiday and working day as they may impact bike rentals, along with temp, A temp, humidity, and windspeed. Using inplace=True ensures these changes are applied directly to the dataframe.

At this point, we’ve accomplished quite a bit: we renamed variables 2 and 3, prepared the weather situation, adjusted the date format, and removed unnecessary columns. This completes the first layer of data pre-processing. Now, we’re ready to explore the dataset more deeply in the next video. Until then, have fun!

# **F) Python - Exploratory Data Analysis**

Welcome back! In this video, we’ll focus on exploratory data analysis (EDA), using our script as much as possible. Let’s start by opening our useful code template and copying the EDA section into our Prophet template.

Our first task is to handle the date column. Since ds is a regular column, a simple way to work with it is to create a temporary dataframe and set the date as the index. We’ll create a copy of our dataframe called dataframe_temp and then set ds as its index using inplace=True. Once this is done, we inspect the head of the dataframe to ensure ds is correctly set as the index.

Next, we’ll set the daily frequency for the index using dataframe_temp.index.freq = 'D'. Now, all plotting and resampling operations will be aligned correctly with daily intervals. Every reference to dataframe in our template should now be replaced with dataframe_temp.

With this prepared dataframe, we can start looking at daily demand trends. Plotting daily demand reveals a growing trend, with seasonal cycles appearing larger in the second half of the dataset. We also notice some spikes: one at the very end near zero, several in 2011, and others in March and April 2012. These outliers are important because they highlight the portion of demand that can be explained by our regressors versus the unexplained error.

Next, we inspect monthly trends by resampling the data to monthly frequency. From January to May, demand gradually increases, peaking between May and September, with October roughly at the same level. November and December show a decline. Quarterly resampling confirms this: Q1 has the lowest demand, Q2 increases, Q3 peaks, and Q4 drops again. This confirms that our data is highly seasonal, which is where Prophet’s capabilities will be valuable.

We also attempted seasonal decomposition to see the underlying trend and seasonality. However, decomposition requires at least two complete cycles (730 days), which our dataset does not fully satisfy. Weekly decomposition (period = 7) gives some insight but isn’t fully representative. Prophet handles this more elegantly with structural time series decomposition, incorporating trend, seasonality, and regressors automatically, including complex seasonality like yearly or weekly cycles.

Next, we examined autocorrelation and partial autocorrelation. The autocorrelation plot shows significant correlation for the first 50 days, indicating that recent values strongly influence future demand. The partial autocorrelation indicates that the most relevant lags are the first 6 days, suggesting that weekly seasonality is not strong in this dataset. This observation will influence how Prophet interprets the data, since it doesn’t explicitly use autoregressive components.

In summary, after EDA, we observe a huge trend, multiplicative seasonality (amplitude increases over time), spikes in Q2 and Q3, and very low demand in Q1. Recent observations carry a lot of information, while weekly seasonality is minimal. Overall, we’ve gained a much deeper understanding of our dataset and are ready to explore Prophet modeling in the next video.

Until then, have fun!

# **G) Dynamic Holidays**

Alrighty! In this section, we’re going to talk about dynamic holidays, and I really hope I haven’t overhyped this feature because it’s genuinely very useful. I find it simple to apply, and I’m excited to show you how it works.

We’ll illustrate this with an example: Valentine’s Day, a day celebrated on February 14th. Let’s imagine we’re analyzing chocolate demand around this holiday. The demand typically increases from February 11th, peaks on the 14th, and then drops slightly on the 15th. Even after the holiday, demand remains slightly higher than average because of last-minute purchases—people forgetting or making late decisions. This creates a demand curve that rises before the event, peaks on the day itself, and tapers off afterward.

The next question is: how do we model this demand curve? With traditional models like SARIMAX, you’d need to create a separate variable for each day of interest—so in this case, five variables for February 11th, 12th, 13th, 14th, and 15th. This quickly becomes cumbersome, especially when you have multiple events.

Prophet makes this process much simpler. You only need to specify the day of the event (February 14th in our example) and define a lower and upper window of impact. The lower window specifies the number of days before the event, and the upper window specifies the days after. For our example, a lower window of -3 captures February 11th–13th, while an upper window of 1 captures February 15th. Prophet automatically applies the event effect across these days.

The result is a much more beginner-friendly and intuitive approach. You can assess the impact of each day, visualize it easily, and avoid the complexity of managing multiple variables manually. Dynamic holidays in Prophet let you capture real-world events in your forecasts without excessive programming effort.

Until the next video, have fun experimenting with dynamic holidays!

# **H) Python - Holidays**

Welcome back! In this video, we’re going to focus on holidays in our Prophet model. We’ll explore how they work, what’s already included in our dataset, and what adjustments we might need to make.

First, let’s inspect our data. We can check the holiday column to see which dates are marked as holidays. Using a subset of the data where holiday == 1, we can extract only the relevant dates. However, we notice that the holidays don’t always repeat consistently, and some important holidays like Christmas are missing. Additionally, the dataset may mark some holidays as “observed,” which means that if a holiday falls on a weekend, it’s officially celebrated on the nearest weekday instead. While this is useful, we want to make sure key holidays like Easter and Christmas are explicitly included because their impact on demand is likely relevant.

To create a complete set of holidays for Prophet, we start by defining general holidays using a DataFrame. For each holiday, we specify:

holiday: a name for the holiday

ds: the date of the holiday (as a timestamp)

lower_window and upper_window: the number of days before and after the event where the holiday may have an impact

For example, we might start with lower_window = -2 and upper_window = 2 to capture two days before and after each holiday.

Next, we explicitly add Christmas, New Year’s Eve, and Easter. For Easter, we looked up the actual dates in 2011 and 2012:

2011: April 24

2012: April 8

Each of these holidays is assigned its own identifier so we can later visualize and assess their individual effects on demand.

Finally, we combine all holidays into a single DataFrame using pandas.concat. This combined holidays DataFrame now includes:

General holidays (e.g., Martin Luther King Jr. Day, Presidents Day, Memorial Day, etc.)

Christmas

New Year’s Eve

Easter

At this point, you can also customize the impact window for each holiday based on domain knowledge, stakeholder input, or observed demand patterns. For instance, if you know that Christmas impacts sales for several days beforehand, you can adjust the lower_window accordingly.

With this setup, our Prophet model can now incorporate holidays in a way that’s intuitive, flexible, and aligned with real-world events. This is a crucial part of feature engineering for time series forecasting, as holidays often drive significant spikes or drops in demand.

Now that our holidays are ready, we can move on and get more familiar with the Prophet model itself.

Until the next video—have fun exploring!

# **I) Prophet Model Parameters**

The Prophet model has a lot of parameters for you to tweak, and thus I actually really wanted to introduce you to them just before we actually apply them. This is so that you can familiarize yourself with them. What we're going to do is that we're going to start with seasonality, which can be yearly, weekly, or daily. For daily data, we include weekly and yearly seasonality, and for hourly data, we would also select the daily option.

The important thing here is that we go one step further versus SARIMA and also Holt-Winters. If you recall, with those models we were setting this “M,” the seasonality cycle, and we could have put one. But here what we're saying is that we have more than one seasonality. If you are working with daily data, you have the weekly seasonality and the yearly seasonality. This is important because, in the end, we have different seasonal cycles that need to be accounted for, and this is now possible with Prophet as well.

The seasonality can be multiplicative or additive. This is something that we have covered as a very quick recap. If the sales of ice cream increase by 50% in August, we’re talking about multiplicative seasonality. If it increases by €70 or $70 in absolute terms, then it is additive. The difference lies in the dimension: percentages for multiplicative, absolute figures for additive seasonality.

We need to include the holidays as a DataFrame. We have built it, we know what it is, and it’s just a matter of including it here. These parameters are part of the model, and they are intuitive. We have three key values that we include, which allows us to tune the model. We'll start with the default values, and then as we move on throughout this section, we'll tune these parameters.

The first parameter is the seasonality prior scale, which reflects the strength of the seasonality curve. The next is the holidays prior scale. This determines how much the holidays actually affect the seasonality curve. The seasonality and holidays are connected, but for holidays to have an impact, they need to interfere with the seasonality curve. This is exactly what this parameter accounts for.

Finally, we move on to the trend with the change point prior scale. This controls how easily the trend changes. If the trend changes too easily, we risk overfitting. If it doesn’t change at all, we risk underfitting. The goal is to find the optimal value that allows the trend line to adjust appropriately. We'll also see this visually as we build a chart showing when the trend has inflection points or changes.

Again, this can be a lot of information, but ultimately it’s all about reflecting the components of a structural time series: seasonality, trend, and holidays. We don’t have anything specific here for regressors, but we will include them in the model. This is the part where we aggregate everything, and we'll do that in the next video.

To recap, there are three key parameters we’ll need to tune because these are values that directly affect the model. Anything else is more straightforward, as the seasonality values are already there. We can also tune the seasonality mode (additive vs. multiplicative), but we will focus on that in the future. For now, let’s build our first Prophet model.

Until the next video, have fun!

# **J) Python - Prophet Model**

Welcome back. This is a very exciting video because this is really where the Prophet model starts. Let me include it here and do shift+enter. The way that I want to work with this is by first looking at the data frame. This is always important. Checking data_frame.head() with just one row is okay. This is version 1.2. Remove any NaN or missing values that you may have—they won't work with Prophet. I usually include a drop step just in case to remove any such values. We don’t really have any right now, but this is a template and it must be complete.

Next, let's import Prophet. From prophet import Prophet. Shift+enter. Here we will be building the Prophet model. The model object is usually called m. When it comes to Prophet, they often refer to it as m. We start with Prophet and can include the seasonality. For instance, we can specify yearly or weekly seasonality, which you can see on the screen. But the auto part that Prophet sets usually works very well. Of course, you should never blindly trust technology, but in this case, it works fine.

We initialize Prophet with holidays=holidays and set the seasonality_mode, which I will start as multiplicative based on our previous analysis. Then we have the seasonality_prior_scale (default is 10), the holidays_prior_scale (default 10), and the change_point_prior_scale (default 0.05). This sets up the first part of the model. We then fit the model with m.fit(data_frame).

But we are not done yet. We need to include regressors, which must be added one by one using m.add_regressor(). Holidays are already covered, so let’s add the other variables. First, working_day, then temperature variables like temp_a and temp, humidity, wind_speed, and finally the weather situation variables weather_situation_2 and weather_situation_3. You can copy these names carefully to avoid spelling mistakes.

Once all regressors are added, do control+enter. You will see output indicating the Prophet run. For example, it may mention that yearly seasonality is disabled or daily seasonality is disabled—this is something we can review later. For now, let’s keep it simple. Set yearly_seasonality=True and weekly_seasonality=True explicitly. Then run the model again, and it will be ready.

This is how you build a Prophet model with regressors. These are the default parameters that come with Prophet, and these are the ones we will need to tune over time. For now, we keep building on this foundation.

# **K) Python - Regressor Coefficients with ChatGPT**

We'll come back. In this video, we are going to focus on the coefficients, and this is a very cool feature of Prophet: regressor coefficients. Prophet is very good at dealing with non-linearity. For instance, consider our example where we have temperature, felt temperature, humidity, wind speed—there are bound to be very high correlations here. Fortunately, in the background, Prophet compensates for this multicollinearity. This is why I love using Prophet for insights, because it handles this naturally.

To get started, we import the utility: from prophet.utilities import regressor_coefficients. Then, it’s as easy as calling regressor_coefficients(m). Here we go. For instance, working_day has a positive coefficient of 0.33, which implies that the target increases by 33%. Temperature variables are normalized, so it’s not super easy to interpret directly, but both have positive coefficients, which makes sense. For bike sharing or bike demand, if the weather is warmer, people ride bikes. If it’s humid or windy, then they don’t.

The same applies to weather situation variables. Weather situation two and three correspond to “mist plus cloudy” and “light snow/light rain,” which are not nice weather, so they have negative coefficients as expected. One thing you can do is ask ChatGPT to build a function to interpret the coefficients of a Prophet model. We tried this, but initially, the output was not ideal—especially with GPT-3.5. The interpretation function produced a dictionary, but it wasn’t very helpful, and the delta trend output was confusing.

We then tried a different approach: building an agnostic function to read tables like the coefficient table so that it can work on different datasets. This function should be able to interpret coefficients correctly, taking into account whether the model is multiplicative or additive. After iterating and refining, we now have interpret_prophet_coefficients, which provides a much clearer understanding of each regressor’s impact.

For example, when we apply this function to our model m, it outputs a table with clear interpretations: for each unit increase in working_day (0 or 1), the target variable is expected to increase by 33%. For temperature, the target increases by 81%. For weather situation two, the demand decreases by 11.56%. This gives a straightforward interpretation of the model coefficients and helps us understand the impact of each regressor.

Of course, some details like multiplicative vs additive scaling can be tricky to recall, but having this function makes it much easier to interpret the results correctly. It also handles dummy variables and normalized features, providing a starting point for understanding the influence of regressors.

It took quite a bit of time to get this function working and to interpret the coefficients properly, but persistence pays off. With this, we can clearly see how each variable affects our target, and we are ready to move forward. Let’s continue in the next video. Have fun!

# **L) Python - Cross-Validation**

We'll come back. In this video, we are going to focus on cross-validation, and we are going to make it happen. In the next video, we will focus on the performance of it, but for now, this is really about the setup and making it run. So let's kick it off. Let me also start here with a new section on cross-validation.

We do need a specific function for this. So we import it using: from prophet.diagnostics import cross_validation. Let’s do that and move forward. After that, this is where we apply the cross-validation to the model. In the end, we build a DataFrame, which I’m going to call dataframe_cross_validation, using the cross_validation() function.

To use this function, we need to include several parameters. First, we specify the model. The model is a Prophet class object, so we set model = m. Next, we need the period. The period determines how often we simulate a forecast. For example, if we do a forecast today, then in 15 days we will do another, and so on. In general, using seven or fifteen days is more than enough to give a first flavor of the model.

After the period, we set the initial parameter, which determines how much data to use for training. This is important because it sets the cutoff for the initial training set. Currently, our dataset has 701 days (dataframe.shape[0]). Usually, we subtract 180 days, which gives 521 days for initial training. This is roughly six months, which is a reasonable period to train the model. If more data is available, you could go up to twelve months, but not more than one year. The idea is to ensure that the model works now, as current performance is what matters most.

Next, we specify the horizon. The horizon defines how long we are going to predict. In our example, we take the future DataFrame from December 2nd to December 31st, which is 30 days. Therefore, we set horizon = 30. Finally, we set parallel to processes. This ensures that the cross-validation runs efficiently without needing a distributed client like Dask. Processes work best for this setup.

Now that the parameters are set, we run the cross-validation. The result is stored in dataframe_CV, which is a pandas DataFrame. We can check it using .head() to ensure it ran correctly. This process does not take very long. Essentially, what we have done is taken our data and validated it at different periods. We went back 180 days in the past, and at periods of 15 days, we made a new 30-day forecast repeatedly across the dataset.

With this, the cross-validation setup is complete. In the next video, we will focus on actually evaluating and interpreting the performance of our cross-validation results. Till the next video—have fun!

# **M) Python - Performance Metrics**

Welcome back. I realized that we didn't actually look at the output, so let's do it in this video. We will look at the cross-validation (CV) output and understand what actually comes out of it. The most important thing is to know what information we get. We can start by checking dataframe_CV.head(). Here, we see five columns.

The first column is ds, which is the date related to that forecast. This is different from the cutoff, which indicates when the forecast was made. For instance, on June 19th, we might forecast for June 20th, 21st, 22nd, 23rd, and so on. Next, we have yhat, yhat_lower, and yhat_upper. These represent the prediction and the lower and upper levels of the confidence interval. Finally, we have y, which is what actually happened. This is essentially what comes out of the cross-validation process.

So what can we do with this output? Prophet provides a function for evaluating the performance metrics. We import it using: from prophet.diagnostics import performance_metrics. Then, we call performance_metrics on our cross-validation DataFrame: performance_metrics(dataframe_CV). This gives us a variety of metrics across the horizon, such as MSE, RMSE, MAP, MAPE, SMAPE, and coverage.

As usual, I like to focus on the MAPE because it provides a clear sense of relative error, and I also pay attention to RMSE because it reflects the impact of outliers. ME (mean error) can also be useful. For simplicity, we will focus on RMSE and MAPE. To do this, we first take the mean across the metrics using .mean(). Then we round the results for readability. For example, RMSE can be rounded to zero decimal places.

For MAPE, we multiply the decimal output by 100 to express it as a percentage. After calculating, I noticed that our MAPE was extremely high—about 100%, which is massive and indicates that something is off. This could be due to predictions or possibly issues in the dataset itself.

Fortunately, there is a simple way to investigate this: by plotting the metrics over time. Prophet provides the plot_cross_validation_metric function for this. We import it using: from prophet.plot import plot_cross_validation_metric. To use it, we simply pass our CV DataFrame and specify the metric, e.g., metric='mape'. Adding a semicolon at the end avoids duplicate output in Jupyter notebooks.

Looking at the plot, we see that while most points seem reasonable, there are some extreme outliers that are causing the MAPE to inflate significantly. These small dots represent periods where the forecast error is very high. This indicates that we need to investigate both our predictions and our dataset more closely to understand the cause of these anomalies.

In the next video, we will focus on exploring these issues in detail, analyzing why the MAPE is so high, and determining how to address it. Until then, have fun!

# **N) Python - Fixing 2012-10-29 with ChatGPT**

Welcome back. Let's explore this error and see what is happening. First and foremost, we need to look at the performance metrics. Diving deep into the performance metrics, we see that our MAPE is mostly okay, but on days 12, 13, and 14, it explodes to 400%, which is massive. Something is clearly wrong. Similarly, on days 27, 28, and 29, it spikes again. This indicates a significant issue that we need to investigate.

To address this, we’ll start an exploration process, and I’m calling this section “exploring the error.” In a sense, this is where we put on our detective hats. I tried to find a detective hat, but I didn’t, so I’ll use a magician’s hat instead for this video. It’s inconvenient, but it works for now.

The first step is to identify when our predictions differ drastically from the actual values. A simple approach is to compute the residuals or deviations. We can do this with dataframe_CV['deviation'] = dataframe_CV['yhat'] - dataframe_CV['y']. Once we have this, we can look at the days with the highest deviation by sorting the DataFrame based on the deviation column. Using sort_values with ascending=False allows us to see the top deviations.

Looking at the results, we see that some deviations are extremely high. Interestingly, when we check the performance metrics, the RMSE spikes, but not as dramatically as the MAPE. The RMSE shows small increases around 1.2 to 1.5, but the MAPE explodes on certain days. This indicates that the issue is not the magnitude of the error itself but how MAPE reacts to very small actual values.

To better understand this, we compute the deviation in percentage using deviation_percentage = (yhat / y - 1) * 100. After correcting a small error in referencing the DataFrame column, we see that October 29, 2012, shows a deviation of over 11,000%. This is reminiscent of the “It’s over 9000!” meme from Dragon Ball, highlighting how extreme this spike is. October 30, 2012, also shows a significant deviation.

Investigating these dates, we find that Hurricane Sandy made landfall on October 29, 2012, affecting Washington, DC, and the surrounding East Coast areas. Heavy rainfall, strong winds, flooding, and power outages caused extreme anomalies in the data. On October 30, 2012, cleanup and recovery efforts continued, which also affected the data. These unique events caused these extreme outliers in our dataset.

To handle this, the simplest approach is to replace these outlier values with the value of the previous day. This allows us to avoid skewing the seasonal patterns and regressor effects in the model. We retrieve the value of October 28, 2012, and replace the y values for October 29 and 30 with it. This correction ensures that our cross-validation and error analysis are fair and not distorted by these rare events.

After replacing the values, we return to the modeling component. Our Prophet model and regressor coefficients might now show slight differences, but the main improvement is in our performance metrics. The MAPE now appears more realistic, around 16% when multiplied by 100, which is a better representation than the previous extreme spikes. While it’s not perfect, it’s a more accurate reflection of the model’s performance.

Examining the errors again, the days with the highest deviation now correspond to more typical events, such as July or Thanksgiving, rather than extreme anomalies. This allows us to focus on feature engineering to further improve the model’s predictive power. In the next video, we will work on feature engineering and continue refining the model.

# **O) Python - Feature Engineering**

Welcome back. In this video, we are going to focus on feature engineering. Specifically, I will be looking at temperature and its lagged values. My hypothesis is that if someone is planning their next day—whether to rent a bike or do something else—the weather they see outside will influence what they plan to do tomorrow. To test this hypothesis, we first need to see if there is a correlation between the lagged weather values and what actually happened in terms of bike demand, which is our target variable, y.

To begin, we need to create lagged variables for temperature. We will create these lagged versions for 1, 3, 5, and 7 days. I think this range is sufficient because otherwise the dataset gets too crowded. For each lag, we create a new column in the dataframe using an f-string, such as temp_lag_{lag}, and assign it the shifted temperature values. For now, we focus only on temperature (temp and a_temp), though in the future, we could include wind speed or other weather variables. The key here is to understand the method, and replacing or adding other variables is straightforward.

Once we have created the lagged columns, we can inspect the dataframe with .head() to see the new lagged values. Naturally, there will be some NaN values at the beginning of each lag series. The next step is to compute the correlation between these lagged values and y to determine how strong the relationship is. To do this elegantly, we loop over variables (temp and a_temp) and lags (1, 3, 5, 7), isolate the relevant columns, and compute a correlation matrix.

From the correlation analysis, we notice that the correlation between y and each lagged value is roughly similar across lags. This indicates that including all lags may not add much new information. In fact, the information in lag 1 is nearly identical to lag 7. Therefore, for simplicity, we can choose to include only one lagged variable, such as lag 1.

To handle the NaN values that were introduced by shifting, we remove all other lagged columns except for temp_lag_1. This is done using dataframe slicing with .iloc, keeping only the relevant columns. Although this approach is not the most elegant or dynamic, it is simple and effective. Once the dataset is adjusted, we drop any remaining NaN values and prepare it for modeling.

With the new lagged variable in place, we can run our Prophet model and perform cross-validation. The resulting performance metrics show that the inclusion of temp_lag_1 slightly changes the model's RMSE from 16.021274 to 16.01266. While the improvement is minor, it is a start and demonstrates the effect of feature engineering.

Looking at the coefficients, we see an interesting result: temp_lag_1 has a negative effect of about 95%. This suggests that if the weather was favorable yesterday, bike demand tends to decrease the following day. This counterintuitive effect may indicate that high demand on one day can “borrow” demand from the next day, which is a very interesting insight.

Although the numerical improvement in RMSE is small, adding lagged features is an important step in exploring potential enhancements to the model. The next step will be parameter tuning, which we will start in the following video. Parameter tuning is a more involved process, so make sure to spare a few minutes to code along and see it run.

Overall, this video demonstrates how to create lagged variables, check correlations, and interpret their effect on bike demand. It is a foundational step in feature engineering that can inform future model improvements. Until the next video, have fun experimenting with your own lagged features!

# **P) Python - Parameter Tuning Set Up**

Welcome back. In this video, we are going to focus on parameter tuning—or better yet, we will set up the parameter grid in this video and do the actual tuning in the next one. The results will be discussed in the following video. We are breaking this down into multiple videos so that it’s easier to follow and digest.

We start by defining the parameter grid to search. We create a dictionary called param_grid. The first parameter is the change_point_prior_scale. Here, we give it a couple of values: 0.05 and 0.5. Next, we define the seasonality_prior_scale with values 10 and 20. Then, we include the holidays_prior_scale and again give it 10 and 20. You can add more values if you want, but keep in mind that the more options you include, the longer the tuning will take. In a real-world scenario, you would want to try several values to get the best results. For the sake of this demonstration, we are using only two values for simplicity.

Finally, we set the seasonality_mode, which can be either additive or multiplicative. In this example, we start with multiplicative. While entering the dictionary, we initially get an error because we mistakenly used an equal sign instead of a colon. After correcting it to use a colon, the dictionary works as expected.

Next, we generate all combinations of the parameters. We create a list called all_params and use the parameter grid we defined earlier to generate every possible combination. If you want to see the generated combinations, you can print all_params and it will display all the possible sets of parameter values.

For now, we are only preparing the parameter options. We also create a placeholder list to store the tuning results called tuning_results, which is currently empty. In the next video, we will build the loop to run the tuning using these parameter combinations.

This sets up everything needed for parameter tuning. In the next video, we will run the tuning pipeline and evaluate the results. Until then, have fun.

# **Q) Python - Parameter Tuning**

Welcome back. In this video, we are going to build the pipeline for parameter tuning. The pipeline works in the following way: we first build a model with a set of parameters, then perform cross-validation, compute the error, and store the error. This process needs to be prepared carefully so that we can systematically test different parameter combinations.

To start, we set up a loop over all parameter combinations. For each set of parameters, we first build the model. This is step one. Step two is performing cross-validation. After the cross-validation is complete, we compute and store the error. Essentially, we are taking all the building blocks we’ve already created—model creation, cross-validation, and error computation—and integrating them into a single loop. Even though it may seem like a lot of code, because the pieces are already built, it is straightforward to assemble.

We begin by copying our Prophet model code into the parameter tuning section. All we need to do is replace the specific parameters with the current combination from our loop. This is done easily by using **params in the model initialization, which unpacks the dictionary of parameters into the constructor. The regressors remain the same, and we fit the model to the target variable as before.

Next, we add the cross-validation step. The cross-validation settings, such as a 15-day period, 521 initial days, 30-day horizon, and processes for parallelization, remain unchanged. Once the cross-validation is complete, we compute the error. In this case, we focus on the RMSE (Root Mean Squared Error) as the most relevant metric. Using the performance_metrics function, we calculate the RMSE, take its mean, and store the result.

Finally, each computed error is appended to our tuning_results list. This list was initially empty and now gets populated with the RMSE for each parameter combination tested. At the end of the loop, we will have a complete record of the error for every set of parameters, which allows us to identify the best performing combination.

In summary, in this video we took all the building blocks we had prepared—model creation, cross-validation, and error measurement—and assembled them into a loop that systematically tries all parameter combinations. The logic is straightforward, even if the programming looks a bit complex. In the next video, we will analyze the results of this parameter tuning, see how long it took to run, and determine which parameters performed best.

Until then, have fun experimenting!

# **R) Python - Parameter Tuning Outcome**

Welcome back. The parameter tuning finished much faster than I expected—just two minutes and 30 seconds. That’s really quick! Now, we are going to check the outcome of the tuning.

First, we create a pandas DataFrame to store all the parameter combinations. This will help us organize the results in a tabular format. We start by including all the parameter combinations as the first step. Step two is to add the corresponding results—the RMSE values we computed for each combination—into this DataFrame. Once this is done, we can inspect the outcome interactively, which is one of the great features of Colab.

Looking at the results, even though it initially appeared that a multiplicative seasonality mode might perform best, the tuning shows that the additive seasonality actually gives the best results. There’s a significant difference: for the best multiplicative result, the RMSE was around 1.259, whereas for additive seasonality, it dropped to 0.981.

Examining the parameter values in detail, we see that the change point prior scale is consistently 0.05 for the best results, and the seasonality mode is always additive. The holidays prior scale and the seasonality prior scale have a smaller influence; their variation doesn’t drastically affect the outcome. This highlights that the seasonality mode and the change point prior scale are the parameters that matter most for improving performance.

We also briefly explored some of the recommended plots, but nothing striking stood out due to the large number of combinations. The key takeaway is clear: the additive seasonality mode combined with a change point prior scale of 0.05 gives the best performance.

Finally, we fetch the best parameter combination. To do this, we identify the index in the tuning results where the RMSE is minimal and select the corresponding parameters from all_params. We store this in a variable called best_params. After executing this step, we have the optimal parameter set ready for future predictions.

In the next video, we will use this tuned model to predict the future. While much of the process—exploring the data, building the model, performing cross-validation, and tuning parameters—seems repetitive, the challenge is to adapt it all for predictive analytics. Essentially, we will move from training and evaluating the model to actually using it to forecast future values.

# **S) Python - Predicting The Future Set Up**

Welcome back. In this video, we’re going to focus on building the script to predict the future. I’ll be using the full workflow that we’ve developed since the beginning. Essentially, we will copy all the necessary steps, check what is needed, remove what isn’t, and ensure everything works together smoothly.

This part of the process is split into four steps:

Preparing the data – which we are doing now.

Building and tuning the model – covered in the next video.

Predicting the future – the video after that.

Data visualization – the final step.

To start, I go back to the beginning where we loaded the data. I don’t need all the library imports, but I copy the data loading and preparation steps. I skip the EDA steps since we’ve already done them, but I make sure to include the holidays, as well as the feature engineering steps for lagged variables. Some parts, like correlation checks, aren’t needed for prediction and can be skipped.

Next, I set up the future regressors. I start with the training data, dataframe_train, and then load the future features (daily_bike_sharing_future). I concatenate the training and future datasets using pandas.concat. After concatenation, I reset the index with drop=True and inplace=True to ensure it’s sequential, which avoids any potential issues later. Checking the tail of the DataFrame shows that the indices are now in order.

After preparing the combined dataset, I inspect it for null values. The columns casual, registered, and count have nulls at the end, which is expected since these are the values we want to predict. I also ensure the date format is consistent and prepare the weather-related variables. At this stage, I remove unnecessary columns to reduce clutter—keeping the system simple prevents errors and makes the script more robust.

Next, I regenerate the holidays to include the complete set up until the end of 2012. After combining all relevant holidays, I create lagged variables for temperature (lag=1) and confirm the result using dataframe.head(). A single NaN at the very end is acceptable, as it won’t affect predictions.

At this point, the data is fully prepared. Everything needed for prediction is in place, and the workflow is clean and error-free. If anyone is struggling, it’s helpful to compare their script with mine to ensure all required steps are included.

In the next video, we will move on to building and tuning the model. Until then, have fun!

# **T) Python - Tuned Prophet Model**

Welcome back. In this video, we are going to build our tuned Prophet model with our training data.

First, we need to remove the names we have just created with the lagged variable. Therefore, removing the NaNs is important. Make sure that we have a clean dataset as well. We need to focus here on just having the training data — the training data.

The way that we fit our model is that we only use the training data. Therefore, we are going to build this train, and this is nothing but our DataFrame sliced up until the last 30 days. In our case, because we have been building this to predict the next 30 days, for the training set we remove the last 30 days which are for the future regressors and we just keep the rest.

So we have this part, and this is done. Then we need to work on something here: we need to include the best parameters. And it's crazy easy because we just do double asterisk **best_params. This was something that we have built already, and this is it.

Instead of fitting to the full DataFrame, we fit to the train DataFrame that we created. I do a shift-enter to run it.

Okay, we're getting an error. Let's see what we can do. The error says: "cannot perform P.O.W. with this index type date time array."

Ah, so one thing that we're missing here is a comma. Let's see if this works.

Okay, so this works. That was the issue — not an error really, just a small typo that was easy to decode. Fortunately, it was also easy to fix.

And this was actually it. Let me just check if I've done everything that I wanted. And yes, this is it.

In the next video, we are going to use the model to predict the future. This is really the goal here — to make sure that the models we built, every analytical tool we produce, has an actual value, that it has an impact. That will be covered in the next video.

# **U) Python - Forecasting**

Alrighty, let's kick it off. In this video, we are going to do some forecasting. That will be our main focus, and we will do it step by step.

First, we need to create a future DataFrame. Using our model, we generate a future DataFrame and specify the number of periods, which in this case is 30. We have established that we are predicting the next 30 days. Let’s take a look at the future DataFrame.

If you look at it here, you see that we only have the date stamps from the 2nd of January all the way until the 1st of December. Okay, I think we are missing something. Let's go back and check.

If I evaluate my training set and notice something missing, it’s because, ideally, we should have data all the way until the end of 2012. So something must be missing. Aha! I think it is because of this NaN issue. Let’s go back and run everything from when we reloaded the data and restarted the prediction process.

Now, if we check again, everything works. Looking at our DataFrame, it is working correctly until the 31st of December. The issue must have occurred when we dropped the NaNs earlier. To fix this, I am going to put this step into the train set and drop the NaNs only from the training data. This should work.

Let me run it. I build the model and make sure that everything is working. Now, let’s build the future DataFrame. It still wasn’t completely working before, but now we see that it goes until the 31st of December. Something changed along the way, but now it works.

This is part one. At the same time, I want to include future regressors. The future DataFrame should also contain the regressors. I create future_regressors by taking the original DataFrame and dropping the columns ds and y. Running this gives us the future regressors as expected.

Next, I drop any NaNs from the future regressors. This completes part one. However, our index currently runs from 1 to 730 in the DataFrame, and 0 to 729 in the future regressors, which is not ideal. To fix this, we reset the index in future_regressors with drop=True and inplace=True. This ensures the index runs correctly from 0 to 729 and the changes are stored.

Now, we concatenate the future DataFrame and the future regressors using pandas.concat. Yes, there are a lot of small steps here, but in the end, it works. I set access = 1 and run it.

Finally, we make the forecast. Using the model, we predict on the future DataFrame we have built. If we look at forecast.head(), we can see the output. This output is quite large, including not just the predicted values but also upper and lower confidence intervals for scenario forecasting. We also have the trend, additive components, and seasonal decompositions like working day effects and yearly patterns.

Now it’s about exploring the forecast and understanding what is happening. We can see the predictions and how the data behaves structurally. The key point is that we now have y_hat for the last 30 days. This is the prediction we wanted to make.

We will stop here in this video. In the next one, we will finish this Prophet tutorial with some data visualization and a proper conclusion.

# **V) Python - Prophet Data Visualization with ChatGPT**

Welcome back. In this final video of our practice tutorial, we are going to focus on data visualization. Along with that, I will also be using ChatGPT to demonstrate what we can do with it in practice. The idea here is to explore how ChatGPT can assist us in understanding and visualizing what is happening inside our Prophet model.

The overall goal is to create a mix-and-match approach, combining ChatGPT (or GenAI) with traditional programming. This combination is very relevant in real-world workflows. Of course, there are situations—especially when very new features or tools are released—where GenAI might not yet be fully up to date. However, it is improving rapidly, and it is extremely useful from a productivity standpoint. You should definitely take advantage of it.

When it comes to data visualization in particular, I really like using GenAI. Visualization code is often longer, requires more customization, and involves a lot of trial and error. In many cases, similar visualizations have already been built by others, so instead of starting from scratch, it is much easier to ask ChatGPT for help and iterate from there.

So, I open ChatGPT and start a new conversation. In the prompt, I ask something like: “Give me five ways to visualize my Prophet model in Python.” I specify Python because there is always the possibility of using R as well, but here we want to stay within Python. I also ask ChatGPT to provide the code. The plan is to try each visualization, see whether it fits our needs, keep the ones we like, and discard the ones we don’t.

I copy all the code that ChatGPT provides and start reviewing it. The first visualization is an evaluation metrics plot, but we don’t really need this anymore because we have already evaluated our metrics earlier. So we skip that one. The next visualization looks useful, so we keep it, even though we already notice there is an error in it that we will fix later.

Next, we keep the “plot the forecast” option, which is usually very useful. We also keep the “plot the components” visualization, which is always a good one. You will often hear people say that plotting components is important, and here we will actually see why that is the case. We also decide to keep a time-series visualization. Everything else that ChatGPT provided is not really needed at this point, so we remove it. All required libraries have already been imported earlier, so there is nothing else to add.

When we start running the code, we immediately get an error saying that data is not defined. This is because the correct variable name should be df. So we replace data with df everywhere it appears. We also make sure the forecast variable is correctly referenced. After fixing that and running the code again, it works.

The first plot we see is a comparison between the actual values and the forecasted values. We notice that there are certain periods—especially spikes—that the model does not predict very well. This is completely expected. Sharp spikes are always difficult for forecasting models to capture accurately. Despite that, the overall forecast looks reasonable, and we can move on.

Next, we plot the components, which is a very interesting visualization. Instead of calling the variable model, we correct it to m. Once we run it, we see the individual components of the Prophet model. First, we see the trend, which shows a clear upward movement from the beginning of the dataset to the end. This indicates long-term growth over time.

Then we look at the holiday effects, which are always interesting. We see recurring spikes that are consistent across years, representing the impact of generic holidays. Most of the time, holidays do not have a very large impact, but there are some cases where the impact is significant. One of the biggest spikes is very likely related to Christmas, and another positive spike is likely due to Easter.

After that, we examine the weekly seasonality. The values range roughly from –200 to +600, while our actual values are often in the range of 4,000 to 6,000. This tells us that weekly seasonality exists, but it is not particularly strong or impactful relative to the overall scale of the data.

We then look at the yearly seasonality, which is clearly stronger than the weekly seasonality. However, it is not perfectly smooth or well defined. There are ups and downs, suggesting that we may not have enough data to fully capture a stable yearly seasonal pattern.

Finally, we look at the regressors, and this is where the most significant impacts appear. The regressor effects range from about –2000 to +2000, which is substantial. This indicates that the regressors are driving a large part of the model’s dynamics. In fact, without these regressors, the model’s accuracy would likely be much worse. While there may be some seasonality in the regressors, the key takeaway is that this is where most of the explanatory power comes from.

We also plot another version of the forecast, which looks very similar to the first chart. The main difference is in the visualization style: we now have lines and dots. The dots represent the actual values, while the line represents the predicted values (or fitted values). This provides another useful way to visually assess model performance.

Lastly, we use a plotting function from Prophet combined with Plotly to create an interactive visualization. This type of plot may not be something we have seen before, but it is useful because Plotly allows interactive exploration of the data. Even though the information is similar to previous plots, the interactivity can be very helpful.

Overall, all these visualizations look quite good. You can definitely explore further and customize them even more, but this is a solid approach. Using ChatGPT to generate visualization ideas and code, and then refining them manually, is a very effective workflow.

# **W) Prophet Pros and Cons**

Alrighty, so Prophet is definitely one of my favorite approaches, and I really hope that by now it has become one of your favorites as well. I want to go through the pros and cons of Prophet in a way that is as unbiased and objective as possible—although, no promises. Let’s kick it off with the positives first.

The first major advantage of Prophet is that it is extremely flexible. There are loads of possibilities that you can include, and all of this complexity actually translates into flexibility. I hope you noticed how easy it is to program with Prophet. There are built-in functions for almost everything—visualization, cross-validation, forecasting, and more.

This ease of use makes Prophet especially beginner-friendly, but it is also valuable even if you are an experienced practitioner. You don’t need to write complex custom Python functions or reinvent the wheel. Instead, you can focus more on the outcomes and insights rather than spending time on low-level programming details. That shift in focus can be extremely powerful in real-world projects.

Another important positive is how Prophet handles events and holidays. You can explicitly define an impact window, with both upper and lower bounds, and this approach is both simple and intuitive. Even better, Prophet allows you to clearly see how these events affect the time series, which is very useful for interpretation and storytelling.

Last but not least on the positive side, Prophet is excellent at handling non-linearity. When working with coefficients, Prophet often operates in terms of percentages rather than fixed linear effects. Additionally—although this is not always stated explicitly—the way Prophet handles regressors helps avoid multicollinearity issues.

For example, if you have two regressors that are highly correlated, this would normally cause problems in a standard linear regression model. Prophet, however, uses a Stan-based modeling mechanism that deals with this internally. This means you don’t need to worry as much about correlated regressors, which is an important advantage to be aware of.

Now let’s move on to the negative side.

One downside is that Prophet really benefits from hyperparameter optimization. This does take additional time. While the extra effort is not excessive and is usually manageable, it is still something you need to account for. For that reason, I consider it a con, even if it’s not a major one.

The biggest downside of Prophet—for me personally—is its weakness in handling short-term dynamics. This becomes especially clear when comparing it to models like Silver Kite, which we will look at next. Prophet does not include an autoregressive component that captures short-term changes that cannot be explained by regressors.

As a result, when there is a sudden shift or abrupt change in the data, Prophet does not model it well. It takes time to adapt, which can lead to poor short-term accuracy. This limitation means that Prophet is not ideal when you care deeply about immediate or near-term forecasting performance.

Because of this, I primarily use Prophet for long-term forecasts and insight generation rather than for short-term, high-precision predictions. If short-term accuracy is critical, Prophet can be problematic.

# **XI) Section 11: Capstone Project: Prophet**

# **A) Project Introduction**

Transcript not available;

# **B) Python - Challenge Solutions Part 1**

Let’s solve this challenge together. You are provided with two files: a PDF and a CSV file. These serve as the starting point for the challenge. The first thing I do is download both of these files. Once that’s done, I head over to ChatGPT, where I want to specifically find a GPT that is designed for working with GitHub repositories.

Now, a quick note here: if you don’t have a premium subscription, unfortunately you won’t be able to follow this part exactly the same way I do. That said, you still have access to the final Python file, which I will include in the course materials. You can also use regular ChatGPT or simply rely on the provided template. The most important thing here is solving the challenge itself and sharing one possible way of doing it.

So I search for “GitHub” inside ChatGPT, and this is our first step. Immediately, the results are ordered by popularity. I click on the first option. It has a 3.8 rating, about 17 reviews, and is categorized under programming in English. Reading the description, it says it empowers the system for comprehensive repository interaction, from code contributions to read/write operations, reviews, and advanced task automation. This sounds promising.

I also check the second option, which has around 5K users and the same 3.8 rating. It provides both general and specific guidance on publicly accessible GitHub repositories and their contents. Both options seem quite similar, so I decide to simply pick the most popular one and move forward.

Once selected, I start the chat. Now, the way I approach this is by using a prompt-engineering technique called “chain of thought.” The idea is to start at a high level and then gradually go deeper, step by step. So the first thing I ask is whether it can access the Prophet GitHub repository, specifically clarifying that Prophet is a model used for time series forecasting. This helps narrow down the scope.

The response is encouraging, which is great. From there, I ask what is new in the recent Prophet releases. At the same time, I open a new Google Colab notebook. What I want to do here is check which version of Prophet is currently installed in Colab and compare it with the most recent releases to see if there are any new features that we should be using.

I run pip freeze, wait for the environment to connect, and then search for Prophet in the output. I see that the installed version is 1.1.5. I go back to ChatGPT and ask what’s new in Prophet since version 1.1.5. The response mentions newer versions and some added features, so I want to verify whether these versions actually exist.

At this point, I notice some confusion. Versions like 1.16 and 1.17 are mentioned, but when I double-check, it turns out that 1.1.5 is actually the most recent official release. This makes me wonder whether the GPT is hallucinating newer versions. So I decide to assume that we are already working with the most up-to-date version and that there’s nothing critical missing.

Now that this is settled, it’s time to actually solve the challenge. I upload both the PDF and the CSV file. I then give a clear instruction: read the challenge from the PDF, analyze the CSV file, outline the code, and include comments in the code. That should be enough to get started.

However, the response I get back tells me that while the files were uploaded, it needs more clarity about the challenge described in the PDF. So I refine the instruction further. I explicitly state that the task is to analyze the PDF, extract all the steps needed to complete the challenge, and then apply those steps to the CSV file.

This works better, but I immediately notice some issues. First, the generated code is using fbprophet, which indicates an old version of Prophet. That’s already a red flag. Second, the solution is overly simplistic: there are no regressors included, and the CSV file is largely ignored. While cross-validation is included, there is no parameter tuning.

This isn’t necessarily wrong, but it’s incomplete. One thing that really bothers me is the continued use of fbprophet. So I push back and tell the GPT that using fbprophet implies an outdated version. I ask it to inspect the Prophet repository and ensure that it is using the most recent functions. I also explicitly instruct it to analyze the CSV file and include things like holidays and regressors in the model.

At this point, I assess the GPT’s performance so far as “not amazing,” but I’m still curious to see what happens next. Then I hit another roadblock: I’m asked to sign in with GitHub, and I get an error saying my account is marked as pending and cannot authorize third-party applications.

This is interesting, but not ideal. I try signing out and signing back in. Eventually, I realize that for this specific GPT, you actually need a GitHub account in order to authorize access properly. That’s not a big issue, but it’s something worth pointing out.

Once I authorize the application, things finally start moving. The GPT begins to improve in quality. It starts outlining steps like loading and inspecting the CSV, initializing the Prophet model, and creating a future dataframe. However, I still see that holidays are effectively ignored, and assumptions are being made instead of actually reading the CSV.

At this point, we at least have a rough understanding of what needs to be done. However, the video is getting quite long. So I decide to stop here. In the next video, we will break everything down properly. We’ll take each step one by one, place it into the notebook, and carefully implement it in a much more structured, step-by-step way.

# **C) Python - Challenge Solutions Part 2**

Welcome back. In this video, we are going to kick things off by using some of the material we obtained from ChatGPT and the GitHub GPT we’ve been working with so far. The goal is to see how far this can take us and what kind of results we can realistically get from it.

The first thing we need to do is include the required libraries and load the data. For this, we need to connect to Google Drive, which is always an important step when working with Google Colaboratory. I navigate to my Drive and point it to the capstone project directory. Specifically, I go to My Drive → Python → Modern Time Series Forecasting Capstone Project, copy the path, and then use %cd to change the working directory. Once that’s done, everything is set up correctly and working as expected.

At this point, I jump between tabs and begin the actual work. The first step is to load the data. While that’s happening, I also ask ChatGPT to analyze the CSV file and adapt the code accordingly, hoping that it will actually read the data and do something meaningful with it. It starts analyzing, which is a good sign.

Next, I focus on importing the libraries and loading the dataset. I cut and paste pieces of code as needed and double-check how the data is structured. I verify column names, check whether parentheses are missing or duplicated, and quickly inspect the dataset using df.head(). This is the very first sanity check.

At the same time, I keep the challenge requirements in mind. The first major requirement is to prepare the dataframe properly. This means renaming columns and transforming the date column. Since we usually perform these operations in place, I do that immediately. After running the cell again, I confirm that the date column is now named ds.

However, I notice an issue: the code is replacing a column called demand, which doesn’t exist in our dataset. The actual column name is total individuals in shelter. So I correct this mistake, replace the correct column name, run the cell again, and confirm that this step is now properly completed.

Preparing the dataframe also involves handling holidays. Even though this wasn’t explicitly specified in the challenge, it is important. I look at how holidays are handled, particularly when Easter equals one, and how Thanksgiving and Christmas are incorporated. I extract that logic, run it, and confirm that holidays are correctly created with lower and upper windows. The values chosen are zero and one, which is acceptable for now, so I don’t change them.

The next step is initializing the Prophet model. This part is straightforward. We also see regressors being added, including holidays. However, this immediately raises a red flag. Holidays have already been added as holidays, so adding them again as regressors duplicates information. That’s a clear mistake, and I count this as a negative point for ChatGPT.

I also notice that daily and weekly seasonality are disabled, which is actually correct because we are working with weekly data. Since we don’t have daily granularity, weekly seasonality in this context doesn’t make sense. The future dataframe creation is not perfect, but it’s good enough for now.

At this point, one important realization should be forming: when things get complex or extensive, it’s not always easy to directly apply ChatGPT’s output. You need to understand what you’re trying to do and what is possible; otherwise, things can quickly go wrong. Once you have that understanding, ChatGPT becomes much faster and more useful.

Next, I decide to explicitly handle the training and test split. I ask ChatGPT to fetch the PDF and provide the code for steps two and three. Looking at our data, I check the tail of the dataframe and realize that we don’t actually have future dates available. Unlike our earlier projects, we don’t already have data extending into the future.

This means that to truly forecast forward, we would need to fetch external data, such as temperature or holiday data. While this is doable, it’s outside the scope of what we want right now. For visualization and evaluation, we still need a training and test split, which is mandatory.

ChatGPT suggests using the last 60 days as a test set, which is reasonable. I go with that. I include this logic in the Prophet model and fit it on the training dataframe. This part is important: the model must be fitted on train_df, not the full dataset.

From there, I create the future dataframe for the next 60 periods and add temperature as a regressor. This is one way of merging external regressors, and we’ll revisit this later. When I attempt to generate the forecast, I immediately run into an error.

After thinking about it, I realize the issue: the model is treating the data as weekly, but the future periods were defined in days. I fix this by switching to weekly frequency and setting the horizon to 13 weeks instead of 60 days. Once I make this correction, everything works as expected.

Now we move to training, testing, and accuracy assessment. I copy the evaluation code, calculate the forecast for the test set, compute the absolute error, and then calculate the mean absolute error (MAE). The MAE comes out to around 29, which is quite large.

Looking at the plot, it’s clear why. The blue line represents the model predictions, and the black dots represent actual observations. There is a significant gap between them, meaning the model is not performing particularly well. This tells us that the model needs improvement.

Next, I move on to step four. Even though we already have a template from earlier work, I still like this approach for two reasons. First, it feels different, so it doesn’t feel like repetitive practice. Second, templates age over time. Revisiting problems in a different way helps challenge assumptions and discover better approaches.

Step four focuses on visualization. I organize the plots step by step, separating residual plots from error metrics. I check the forecast plot, which looks similar to previous ones. Then I examine the component plots.

The components show a growing trend that stabilizes and then declines slightly. Holiday impacts are minimal—around one thousand compared to an overall level of four hundred thousand. Extra regressors also show minimal impact. Yearly seasonality exists but is still relatively small, and weekly seasonality effects from regressors are also low. Overall, it doesn’t look like there’s much happening that could significantly improve the model.

I also examine the residual plots, but I don’t find them particularly useful. The plots are messy, not very insightful, and visually unattractive. I decide to delete them. Additionally, comparing residuals during prediction versus fitting is not entirely fair, since the model is optimized for training data. So I remove those plots as well.

Finally, I move to step five: parameter tuning. I ask ChatGPT to provide code for this step. Initially, the output is not very good—it tries random adjustments without a clear structure. When I don’t like the output, I stop it and regenerate. This is one way of providing implicit feedback.

Eventually, ChatGPT starts doing what I want. It proposes tuning parameters such as changepoint_prior_scale and seasonality_prior_scale, and it uses itertools instead of sklearn’s parameter grid. I actually like this approach because it’s different from what we used before and forces us to think in new ways.

However, I notice another issue. ChatGPT attempts to split the data using the first 80% for training. This does not make sense in a time series context. We have data from 2014 to 2021, and what really matters is whether the model performs well in the most recent period. Context matters in time series forecasting. If the model works well in the first five years but poorly in the last year, that’s unacceptable.

So I ignore that logic and stick with our existing train-test split. I also notice that ChatGPT is still treating the data as daily instead of weekly, which causes additional confusion. I fix these issues manually.

Eventually, I run into another error related to frequency definitions. ChatGPT handles these errors fairly well, often fixing spelling or parameter mistakes quickly, and that’s something I genuinely appreciate. It makes experimentation much easier.

At this point, the video has gone on for quite a while. I still need to wrestle a bit more with ChatGPT to get everything exactly right. So I decide to take a break here. In the next video, I promise we’ll finish this properly and also talk more about the pros and cons of using ChatGPT in this way.

# **D) Python - Challenge Solutions Part 3**

So, the error message explains the issue we encountered earlier. If you remember, we were working with cross-validation, and the problem comes from the fact that pandas does not support time units like six weeks or thirteen weeks. That was something I personally wasn’t aware of before. Because of this limitation, we need to convert those periods into days instead. So instead of six weeks and thirteen weeks, we now use 42 days and 91 days, which is perfectly fine.

I update the period values accordingly and run the cell again using Control + Enter. Now the code is running, which is a good sign. At this point, though, we are more or less done with the main part.

What I’d like to do next is take a look at the best parameters. Let me find where that is. Okay, here it is—this is the section where we retrieve the best parameters. I copy that part and place it at the end. The thing is, I honestly don’t know how long this is going to take to run. I probably should have checked beforehand, but it doesn’t really matter. I’ll just leave it running and move on.

Before wrapping up, I want to briefly stress something that I think is extremely important: you really need to understand what a library can do and what it cannot do. As you saw, we struggled a bit along the way, and that’s completely normal. It’s part of the learning process. For something that’s relatively new, tools like this can be very helpful, and they can save you a lot of time—but they can also become an issue if you rely on them blindly.

You shouldn’t just use tools like ChatGPT without thinking. You always need to keep the documentation in mind. Throughout all my courses and in the way I build them, I continuously refer back to the documentation to understand what’s really happening. It’s not the most exciting job—it’s research—but that’s the reality. Whether you’re an analyst, a data scientist, or just someone trying to solve a real problem properly, you have to do this if you want to go deep and do a really good job.

ChatGPT still has a long way to go, especially for more niche or advanced use cases like this one. In particular, it can struggle with the most recent or up-to-date versions of libraries. Sometimes the code it generates isn’t the most elegant, sometimes it misses things, and sometimes you need to micromanage it quite heavily. You might even get outdated patterns.

For example, in our case, we saw a simple print statement with a variable. That really should have been written as an f-string, which is the cleaner and more modern way of doing it. It’s a small thing, but it shows the point.

At this stage, I’m going to stop here. I don’t think it really matters whether we inspect the best parameters right now. I’ll share the code so that you can look at it yourself, compare it with what you’ve done, and see how things differ depending on whether you used ChatGPT, another GPT model, or a predefined template.

I’d actually be very keen to hear how you handled it, what kind of results you got, and how your approach compared. Also, this type of content—where I take a challenge and try to use AI tools to fix it—I’d really love to know what you think. Do you like it? Do you not like it? Is this something you’d like to see more of?

Your feedback is genuinely the most important thing. If we want to make this course an absolute 11 out of 10, that feedback really matters. With that, we’ll stop here, and I’ll see you in the next section.
